# Master en IngenierÃ­a de Datos con Inteligencia Artificial

## InformaciÃ³n General

### DuraciÃ³n
18-24 meses (intensidad flexible segÃºn dedicaciÃ³n del estudiante)

### Modalidad
Autoaprendizaje guiado con proyectos prÃ¡cticos y evaluaciones continuas

### Objetivos Generales del Master

Al finalizar este master, el estudiante serÃ¡ capaz de:

1. DiseÃ±ar, construir y mantener pipelines de datos robustos y escalables
2. Trabajar con arquitecturas de datos modernas en entornos cloud
3. Implementar soluciones de Big Data y procesamiento distribuido
4. Integrar modelos de Machine Learning e IA en pipelines de producciÃ³n
5. Aplicar buenas prÃ¡cticas de DataOps, seguridad y gobernanza de datos
6. Liderar proyectos de ingenierÃ­a de datos en equipos multidisciplinarios

### Perfil de Ingreso

**Conocimientos previos recomendados:**
- Familiaridad bÃ¡sica con computadoras y sistemas operativos
- LÃ³gica bÃ¡sica y pensamiento analÃ­tico
- InglÃ©s tÃ©cnico (lectura de documentaciÃ³n)
- MatemÃ¡ticas de nivel secundario

**No se requiere experiencia previa en programaciÃ³n.**

### Perfil de Egreso

El egresado del master serÃ¡ un **Data Engineer** con capacidad para:

- Desarrollar soluciones de datos end-to-end
- Trabajar con las principales tecnologÃ­as del ecosistema de datos moderno
- DiseÃ±ar arquitecturas de datos escalables y mantenibles
- Implementar prÃ¡cticas de seguridad y calidad en datos
- Colaborar eficazmente con Data Scientists, Analysts y equipos de negocio
- Adaptarse a nuevas tecnologÃ­as y frameworks del ecosistema de datos
- Integrar capacidades de IA/ML en infraestructuras de datos

### MetodologÃ­a de Aprendizaje

Este master sigue una filosofÃ­a prÃ¡ctica y orientada a la industria:

1. **Aprender haciendo**: Cada concepto se refuerza con ejercicios prÃ¡cticos
2. **Proyectos incrementales**: Los proyectos crecen en complejidad progresivamente
3. **TDD (Test-Driven Development)**: Escribir tests antes del cÃ³digo cuando sea aplicable
4. **CÃ³digo limpio**: Ã‰nfasis en buenas prÃ¡cticas, arquitectura limpia y mantenibilidad
5. **Seguridad por defecto**: IntegraciÃ³n de conceptos de seguridad desde el inicio
6. **Portafolio profesional**: Cada proyecto suma al portafolio del estudiante

---

## Estructura del Programa

### MÃ“DULO 1: Fundamentos de ProgramaciÃ³n y Herramientas

**DuraciÃ³n:** 8-10 semanas  
**Nivel:** Principiante

#### Objetivos de Aprendizaje

Al completar este mÃ³dulo, el estudiante serÃ¡ capaz de:

- Escribir programas en Python con sintaxis correcta y estilo limpio
- Utilizar Git y GitHub para control de versiones
- Configurar entornos de desarrollo profesionales
- Aplicar principios de programaciÃ³n funcional
- Escribir tests unitarios bÃ¡sicos
- Manejar errores y excepciones de forma profesional

#### Temas y Subtemas

**1.1 IntroducciÃ³n a Python**
- InstalaciÃ³n y configuraciÃ³n (Python 3.11+)
- Variables, tipos de datos y operadores
- Estructuras de control (if, for, while)
- Funciones y parÃ¡metros
- Manejo de errores con try/except
- MÃ³dulos y paquetes

**1.2 Estructuras de Datos en Python**
- Listas, tuplas y sets
- Diccionarios y comprensiones
- ManipulaciÃ³n de strings
- Trabajar con archivos (lectura/escritura)

**1.3 ProgramaciÃ³n Funcional**
- Funciones de primera clase
- Lambda, map, filter, reduce
- Principios SOLID aplicados a funciones
- Evitar efectos secundarios

**1.4 Control de Versiones con Git**
- Conceptos: repositorio, commit, branch, merge
- Comandos bÃ¡sicos: clone, add, commit, push, pull
- Trabajo con ramas (branching strategy)
- GitHub: issues, pull requests, colaboraciÃ³n

**1.5 Entorno de Desarrollo**
- IDEs: VS Code / PyCharm
- Virtual environments (venv)
- GestiÃ³n de dependencias (pip, requirements.txt)
- Linters y formatters (flake8, black)

**1.6 Testing BÃ¡sico**
- IntroducciÃ³n a pytest
- Escribir tests unitarios
- Asserts y fixtures
- Coverage de tests

#### TecnologÃ­as/Herramientas
- Python 3.11+
- Git & GitHub
- VS Code / PyCharm
- pytest
- black, flake8

#### Proyectos PrÃ¡cticos

1. **Calculadora de estadÃ­sticas bÃ¡sicas**: Funciones para calcular media, mediana, desviaciÃ³n estÃ¡ndar con tests
2. **Procesador de archivos CSV**: Leer, validar y transformar archivos CSV con manejo de errores
3. **Sistema de logs**: Implementar un logger configurable para diferentes niveles

#### Recursos Recomendados

- Libro: "Python Crash Course" - Eric Matthes
- Curso: Real Python - Python Basics
- DocumentaciÃ³n: docs.python.org
- Curso: Git & GitHub para principiantes

#### Criterios de EvaluaciÃ³n

- [ ] Escribir funciones con tipado explÃ­cito
- [ ] CÃ³digo cumple con flake8 y black
- [ ] Todos los proyectos tienen tests con >80% coverage
- [ ] Uso correcto de Git (commits descriptivos, branches)
- [ ] Manejo adecuado de errores y excepciones

---

### MÃ“DULO 2: Bases de Datos y SQL

**DuraciÃ³n:** 8-10 semanas  
**Nivel:** Principiante a Intermedio

#### Objetivos de Aprendizaje

Al completar este mÃ³dulo, el estudiante serÃ¡ capaz de:

- DiseÃ±ar modelos de datos relacionales normalizados
- Escribir consultas SQL complejas con JOINs, subconsultas y CTEs
- Crear y optimizar Ã­ndices para mejorar performance
- Conectar aplicaciones Python con bases de datos
- Implementar transacciones y garantizar integridad de datos
- Trabajar con bases de datos NoSQL bÃ¡sicas

#### Temas y Subtemas

**2.1 Fundamentos de Bases de Datos**
- Conceptos: tablas, filas, columnas, claves
- Tipos de bases de datos (SQL vs NoSQL)
- ACID y transacciones
- NormalizaciÃ³n (1NF, 2NF, 3NF)

**2.2 SQL BÃ¡sico**
- SELECT, WHERE, ORDER BY, LIMIT
- Funciones agregadas (COUNT, SUM, AVG, MAX, MIN)
- GROUP BY y HAVING
- DISTINCT y operadores lÃ³gicos

**2.3 SQL Intermedio**
- JOINs (INNER, LEFT, RIGHT, FULL OUTER)
- Subconsultas (subqueries)
- CTEs (Common Table Expressions)
- UNION, INTERSECT, EXCEPT
- Window functions (ROW_NUMBER, RANK, LAG, LEAD)

**2.4 DDL y Modelado**
- CREATE, ALTER, DROP
- Tipos de datos y constraints
- Claves primarias y forÃ¡neas
- Ãndices y su impacto en performance

**2.5 Python y Bases de Datos**
- ConexiÃ³n con sqlite3
- SQLAlchemy Core
- GestiÃ³n de conexiones y pools
- ParÃ¡metros y SQL injection prevention
- Transacciones desde Python

**2.6 IntroducciÃ³n a NoSQL**
- Conceptos de MongoDB
- Documentos JSON y colecciones
- Operaciones CRUD en MongoDB
- CuÃ¡ndo usar SQL vs NoSQL

#### TecnologÃ­as/Herramientas
- PostgreSQL
- SQLite
- MongoDB
- SQLAlchemy
- DBeaver / pgAdmin
- pymongo

#### Proyectos PrÃ¡cticos

1. **Sistema de biblioteca**: DiseÃ±ar modelo relacional, crear tablas, implementar consultas complejas
2. **ETL bÃ¡sico con Python**: Extraer datos de CSV, transformar y cargar en PostgreSQL
3. **API de consultas**: Crear funciones Python que ejecuten consultas parametrizadas seguras

#### Recursos Recomendados

- Libro: "SQL for Data Analysis" - Cathy Tanimura
- Curso: Mode Analytics SQL Tutorial
- DocumentaciÃ³n: PostgreSQL Official Docs
- Plataforma: SQLZoo, LeetCode SQL

#### Criterios de EvaluaciÃ³n

- [ ] Modelos de datos normalizados correctamente
- [ ] Consultas SQL optimizadas (uso adecuado de Ã­ndices)
- [ ] PrevenciÃ³n de SQL injection en cÃ³digo Python
- [ ] Tests para funciones de base de datos
- [ ] Manejo correcto de transacciones y rollback

---

### MÃ“DULO 3: IngenierÃ­a de Datos Core (ETL/ELT y Pipelines)

**DuraciÃ³n:** 10-12 semanas  
**Nivel:** Intermedio

#### Objetivos de Aprendizaje

Al completar este mÃ³dulo, el estudiante serÃ¡ capaz de:

- DiseÃ±ar y construir pipelines ETL/ELT robustos
- Implementar validaciÃ³n y limpieza de datos
- Manejar diferentes formatos de datos (CSV, JSON, Parquet, Avro)
- Trabajar con APIs REST para extracciÃ³n de datos
- Implementar logging y monitoreo en pipelines
- Aplicar patrones de diseÃ±o para pipelines escalables

#### Temas y Subtemas

**3.1 Conceptos de ETL/ELT**
- Diferencia entre ETL y ELT
- Arquitecturas de pipelines de datos
- Batch vs Streaming processing
- Idempotencia y reprocessing

**3.2 ExtracciÃ³n de Datos**
- Lectura desde archivos (CSV, JSON, Excel)
- Consumo de APIs REST
- Web scraping Ã©tico con Beautiful Soup
- ConexiÃ³n a bases de datos mÃºltiples
- Manejo de rate limits y paginaciÃ³n

**3.3 TransformaciÃ³n de Datos con Pandas**
- DataFrames y Series
- SelecciÃ³n, filtrado y agregaciÃ³n
- Merge, join y concatenaciÃ³n
- Manejo de valores nulos
- Pivoting y reshape
- Apply, map y vectorizaciÃ³n

**3.4 Calidad de Datos**
- ValidaciÃ³n de esquemas
- DetecciÃ³n de duplicados
- Manejo de outliers
- Data profiling
- DocumentaciÃ³n de reglas de calidad

**3.5 Formatos de Datos Modernos**
- JSON y JSON Lines
- Parquet (columnar storage)
- Avro (schemas evolutivos)
- ComparaciÃ³n y casos de uso

**3.6 Carga de Datos**
- Estrategias: full load, incremental, upsert
- Bulk inserts optimizados
- Particionamiento de datos
- Compression y encoding

#### TecnologÃ­as/Herramientas
- Pandas
- Requests, httpx
- Beautiful Soup
- Parquet (pyarrow)
- Great Expectations
- Logging (logging module)

#### Proyectos PrÃ¡cticos

1. **Pipeline de noticias**: Extraer datos de API de noticias, limpiar, almacenar en PostgreSQL
2. **ETL incremental**: Pipeline que solo procesa datos nuevos (control de watermarks)
3. **Data quality framework**: Sistema de validaciÃ³n reutilizable con reportes de calidad

#### Recursos Recomendados

- Libro: "Data Pipelines Pocket Reference" - James Densmore
- Curso: DataCamp - Building Data Engineering Pipelines in Python
- DocumentaciÃ³n: Pandas User Guide
- ArtÃ­culo: Martin Fowler - Patterns for ETL

#### Criterios de EvaluaciÃ³n

- [ ] Pipelines idempotentes y reproducibles
- [ ] ValidaciÃ³n de datos implementada
- [ ] Logging detallado con niveles apropiados
- [ ] Manejo robusto de errores y reintentos
- [ ] Tests de integraciÃ³n para pipelines completos
- [ ] DocumentaciÃ³n de transformaciones aplicadas

---

### MÃ“DULO 4: Almacenamiento y Modelado de Datos

**DuraciÃ³n:** 8-10 semanas  
**Nivel:** Intermedio a Avanzado

#### Objetivos de Aprendizaje

Al completar este mÃ³dulo, el estudiante serÃ¡ capaz de:

- DiseÃ±ar Data Warehouses con esquemas dimensionales
- Implementar modelos Star Schema y Snowflake
- Trabajar con Slowly Changing Dimensions (SCD)
- Comprender Data Lakes y arquitecturas modernas (Data Lakehouse)
- Optimizar almacenamiento con particionamiento y clustering
- Aplicar tÃ©cnicas de versionado de datos

#### Temas y Subtemas

**4.1 Data Warehousing**
- Conceptos: OLTP vs OLAP
- Arquitectura de Data Warehouse
- ETL vs ELT en DWH context
- Kimball vs Inmon

**4.2 Modelado Dimensional**
- Tablas de hechos (fact tables)
- Tablas de dimensiones (dimension tables)
- Star Schema
- Snowflake Schema
- MÃ©tricas aditivas, semi-aditivas, no-aditivas

**4.3 Slowly Changing Dimensions (SCD)**
- Tipos de SCD (Type 1, 2, 3, 6)
- ImplementaciÃ³n prÃ¡ctica de SCD Type 2
- Surrogate keys vs natural keys
- Validez temporal de registros

**4.4 Data Lakes**
- Conceptos y arquitectura
- OrganizaciÃ³n de datos (raw, processed, curated)
- Formatos para Data Lakes (Parquet, Delta Lake)
- Metadata management
- Data Lake vs Data Warehouse

**4.5 Data Lakehouse**
- Convergencia de Lake y Warehouse
- Delta Lake, Apache Iceberg, Apache Hudi
- ACID transactions en Data Lakes
- Time travel y versioning

**4.6 OptimizaciÃ³n de Almacenamiento**
- Particionamiento (por fecha, regiÃ³n, etc.)
- Bucketing y clustering
- Compression techniques
- Columnar vs row-based storage
- Cost optimization

#### TecnologÃ­as/Herramientas
- PostgreSQL (con esquemas dimensionales)
- DuckDB (para analytics local)
- Delta Lake
- Parquet
- MinIO (S3-compatible storage)

#### Proyectos PrÃ¡cticos

1. **Data Warehouse de ventas**: DiseÃ±ar e implementar modelo dimensional completo con SCD
2. **Data Lake bÃ¡sico**: Organizar datos en capas (raw/bronze, processed/silver, curated/gold)
3. **ImplementaciÃ³n de Delta Lake**: Migrar datos a Delta con capacidades de time travel

#### Recursos Recomendados

- Libro: "The Data Warehouse Toolkit" - Ralph Kimball
- Curso: Udemy - Data Warehouse Fundamentals
- DocumentaciÃ³n: Delta Lake Documentation
- ArtÃ­culo: Databricks - Data Lakehouse Architecture

#### Criterios de EvaluaciÃ³n

- [ ] Modelos dimensionales correctamente diseÃ±ados
- [ ] SCD implementadas con historizaciÃ³n completa
- [ ] Estrategia de particionamiento documentada y justificada
- [ ] Performance de queries optimizada
- [ ] DocumentaciÃ³n de modelo de datos (diagramas ER)

---

### MÃ“DULO 5: Big Data y Procesamiento Distribuido

**DuraciÃ³n:** 10-12 semanas  
**Nivel:** Avanzado

#### Objetivos de Aprendizaje

Al completar este mÃ³dulo, el estudiante serÃ¡ capaz de:

- Comprender arquitecturas distribuidas y conceptos de Big Data
- Desarrollar jobs de procesamiento con Apache Spark
- Trabajar con streaming de datos usando Kafka
- Optimizar jobs distribuidos para performance
- Implementar procesamiento batch y real-time
- DiseÃ±ar arquitecturas Lambda y Kappa

#### Temas y Subtemas

**5.1 Fundamentos de Big Data**
- CaracterÃ­sticas (Volume, Velocity, Variety, Veracity)
- Arquitecturas distribuidas
- CAP Theorem
- Hadoop ecosystem overview
- CuÃ¡ndo usar Big Data technologies

**5.2 Apache Spark Core**
- RDDs, DataFrames y Datasets
- Transformations vs Actions
- Lazy evaluation
- Spark architecture (driver, executors, cluster manager)
- PySpark basics

**5.3 Spark SQL y DataFrames**
- Lectura de mÃºltiples formatos
- Transformaciones complejas
- Window functions en Spark
- Joins y optimizaciones
- Catalyst optimizer

**5.4 Spark Streaming**
- Structured Streaming
- Micro-batching vs continuous processing
- Windowing y watermarks
- Stateful operations
- Checkpointing

**5.5 Apache Kafka**
- Conceptos: topics, partitions, brokers
- Producers y consumers
- Consumer groups
- Kafka Connect
- SerializaciÃ³n (Avro, Protobuf)

**5.6 Arquitecturas de Streaming**
- Lambda Architecture
- Kappa Architecture
- Event-driven architectures
- Exactly-once semantics
- Backpressure handling

**5.7 OptimizaciÃ³n de Spark**
- Partitioning strategies
- Caching y persistence
- Broadcasting variables
- Evitar shuffles
- Memory tuning

#### TecnologÃ­as/Herramientas
- Apache Spark (PySpark)
- Apache Kafka
- Confluent Platform
- Spark UI para debugging
- Docker para clusters locales

#### Proyectos PrÃ¡cticos

1. **Procesamiento batch con Spark**: AnÃ¡lisis de logs de millones de registros
2. **Pipeline de streaming**: Kafka + Spark Streaming para procesar eventos en tiempo real
3. **Sistema de recomendaciones**: Procesamiento distribuido para generar recomendaciones

#### Recursos Recomendados

- Libro: "Learning Spark" - O'Reilly (2nd Edition)
- Libro: "Kafka: The Definitive Guide" - O'Reilly
- Curso: Databricks Academy - Apache Spark Programming
- DocumentaciÃ³n: Apache Spark Official Docs

#### Criterios de EvaluaciÃ³n

- [ ] Jobs de Spark optimizados (uso eficiente de memoria y particiones)
- [ ] Procesamiento de streaming sin pÃ©rdida de datos
- [ ] Manejo correcto de errores en sistemas distribuidos
- [ ] Monitoreo de jobs (Spark UI, mÃ©tricas)
- [ ] CÃ³digo escalable para datasets grandes
- [ ] Tests con datasets de prueba

---

### MÃ“DULO 6: Cloud Data Engineering

**DuraciÃ³n:** 10-12 semanas  
**Nivel:** Avanzado

#### Objetivos de Aprendizaje

Al completar este mÃ³dulo, el estudiante serÃ¡ capaz de:

- DiseÃ±ar arquitecturas de datos en la nube
- Utilizar servicios de almacenamiento cloud (S3, Azure Blob, GCS)
- Implementar pipelines con servicios serverless
- Trabajar con Data Warehouses cloud (Snowflake, BigQuery, Redshift)
- Gestionar infraestructura como cÃ³digo (IaC)
- Implementar seguridad y gobernanza en cloud

#### Temas y Subtemas

**6.1 Fundamentos de Cloud Computing**
- IaaS, PaaS, SaaS
- Regiones y availability zones
- Pricing models y cost optimization
- ComparaciÃ³n: AWS vs Azure vs GCP

**6.2 AWS para Data Engineering**
- S3: almacenamiento de objetos, lifecycle policies
- Glue: ETL serverless y Data Catalog
- Redshift: Data Warehouse
- Lambda: funciones serverless
- Athena: queries sobre S3
- Kinesis: streaming de datos
- IAM: gestiÃ³n de permisos y seguridad

**6.3 Azure para Data Engineering**
- Azure Blob Storage y Data Lake Gen2
- Azure Data Factory: orquestaciÃ³n de pipelines
- Synapse Analytics: Data Warehouse integrado
- Databricks en Azure
- Azure Functions

**6.4 GCP para Data Engineering**
- Google Cloud Storage
- BigQuery: Data Warehouse serverless
- Dataflow: procesamiento batch y streaming
- Dataproc: Spark managed
- Pub/Sub: messaging

**6.5 Infraestructura como CÃ³digo**
- Terraform basics
- CloudFormation (AWS)
- GestiÃ³n de estado
- MÃ³dulos reutilizables
- CI/CD para infraestructura

**6.6 Seguridad en Cloud**
- Encryption at rest y in transit
- GestiÃ³n de secretos (AWS Secrets Manager, Azure Key Vault)
- Network security (VPCs, Security Groups)
- AuditorÃ­a y compliance
- Least privilege principle

**6.7 Data Warehouses Cloud**
- Snowflake: arquitectura multi-cluster
- BigQuery: almacenamiento columnar, slots
- Redshift: distribution keys, sort keys
- ComparaciÃ³n y casos de uso

#### TecnologÃ­as/Herramientas
- AWS (S3, Glue, Redshift, Lambda)
- Azure (Data Factory, Synapse)
- GCP (BigQuery, Dataflow)
- Terraform
- Snowflake
- Docker

#### Proyectos PrÃ¡cticos

1. **Pipeline serverless en AWS**: S3 â†’ Lambda â†’ Glue â†’ Athena
2. **Data Warehouse en Snowflake**: Implementar modelo dimensional cloud-native
3. **IaC con Terraform**: Desplegar infraestructura completa de data pipeline
4. **Pipeline multi-cloud**: Integrar datos de AWS S3 a BigQuery

#### Recursos Recomendados

- Curso: AWS Certified Data Analytics Specialty
- Curso: Google Cloud Professional Data Engineer
- Libro: "Cloud Data Engineering For Dummies"
- DocumentaciÃ³n: Terraform Registry
- CertificaciÃ³n: Snowflake SnowPro Core

#### Criterios de EvaluaciÃ³n

- [ ] Arquitecturas cloud bien diseÃ±adas y documentadas
- [ ] Uso eficiente de servicios (cost-effective)
- [ ] Seguridad implementada correctamente (IAM, encryption)
- [ ] IaC versionado y reproducible
- [ ] Pipelines serverless resilientes
- [ ] Monitoreo y alertas configuradas

---

### MÃ“DULO 7: OrquestaciÃ³n y AutomatizaciÃ³n

**DuraciÃ³n:** 8-10 semanas  
**Nivel:** Intermedio a Avanzado

#### Objetivos de Aprendizaje

Al completar este mÃ³dulo, el estudiante serÃ¡ capaz de:

- Orquestar pipelines complejos con Apache Airflow
- Implementar transformaciones con dbt (data build tool)
- DiseÃ±ar DAGs eficientes y mantenibles
- Configurar monitoring y alertas
- Aplicar principios de CI/CD en pipelines de datos
- Gestionar dependencias y scheduling

#### Temas y Subtemas

**7.1 Apache Airflow**
- Arquitectura (scheduler, webserver, executor, metadata DB)
- DAGs: estructura y componentes
- Operators (Python, Bash, SQL, etc.)
- Tasks y dependencias
- XComs para pasar datos entre tasks
- Sensors y triggers
- Hooks para conectores
- TaskFlow API (decoradores)

**7.2 Airflow Avanzado**
- Dynamic DAG generation
- SubDAGs y TaskGroups
- Variables y Connections
- Branching y conditional logic
- Pools y concurrency
- Executors (Local, Celery, Kubernetes)

**7.3 dbt (data build tool)**
- Conceptos: models, sources, tests
- Transformaciones con SQL
- Jinja templating
- Materializations (table, view, incremental)
- Tests de datos (unique, not null, relationships)
- Documentation automÃ¡tica
- Macros reutilizables

**7.4 dbt Avanzado**
- Snapshots (SCD Type 2)
- Seeds (archivos CSV estÃ¡ticos)
- Packages y reutilizaciÃ³n
- Orchestration con Airflow
- dbt Cloud vs dbt Core

**7.5 CI/CD para Data Pipelines**
- Git workflows para datos
- Testing en pipelines (unit, integration)
- Pre-commit hooks
- GitHub Actions / GitLab CI
- Deployment strategies
- Environment management (dev, staging, prod)

**7.6 Monitoring y Observability**
- Logging centralizado
- MÃ©tricas de pipeline (latency, throughput)
- Alertas y notificaciones
- Data lineage tracking
- SLAs y SLOs

#### TecnologÃ­as/Herramientas
- Apache Airflow
- dbt Core
- Docker & Docker Compose
- GitHub Actions
- Prometheus & Grafana
- Great Expectations

#### Proyectos PrÃ¡cticos

1. **Pipeline orquestado completo**: Airflow orquestando extracciÃ³n, transformaciÃ³n (dbt) y carga
2. **CI/CD pipeline**: GitHub Actions ejecutando tests de dbt y validaciones
3. **Sistema de monitoreo**: Dashboard de mÃ©tricas de pipelines con alertas

#### Recursos Recomendados

- Libro: "Data Pipelines with Apache Airflow" - Manning
- Curso: Astronomer - Airflow Fundamentals
- DocumentaciÃ³n: dbt Learn
- Blog: Data Engineering Weekly
- Comunidad: dbt Slack Community

#### Criterios de EvaluaciÃ³n

- [ ] DAGs bien estructurados y documentados
- [ ] Idempotencia en todos los tasks
- [ ] Tests de dbt implementados (coverage >90%)
- [ ] CI/CD funcional con tests automÃ¡ticos
- [ ] Alertas configuradas para fallos
- [ ] Data lineage documentado
- [ ] CÃ³digo sigue mejores prÃ¡cticas de Airflow

---

### MÃ“DULO 8: IA y Machine Learning para Data Engineers

**DuraciÃ³n:** 10-12 semanas  
**Nivel:** Avanzado

#### Objetivos de Aprendizaje

Al completar este mÃ³dulo, el estudiante serÃ¡ capaz de:

- Comprender workflows de Machine Learning end-to-end
- Implementar pipelines ML en producciÃ³n (MLOps)
- Trabajar con feature stores y feature engineering
- Desplegar modelos de ML como APIs
- Integrar LLMs en aplicaciones de datos
- Implementar RAG (Retrieval Augmented Generation)
- Gestionar experimentos y versionado de modelos

#### Temas y Subtemas

**8.1 ML Fundamentals para Data Engineers**
- Diferencia entre Data Engineering y ML Engineering
- Ciclo de vida de un proyecto ML
- Training vs Inference
- Batch predictions vs real-time serving
- Conceptos bÃ¡sicos: features, labels, train/test split

**8.2 Feature Engineering**
- CreaciÃ³n de features efectivas
- Encoding categÃ³rico (one-hot, label encoding)
- Scaling y normalization
- Time-based features
- Feature stores (Feast, Tecton)
- Feature pipelines vs training pipelines

**8.3 MLOps**
- CI/CD para ML
- Versionado de datos y modelos (DVC, MLflow)
- Experiment tracking
- Model registry
- A/B testing de modelos
- Monitoring de model drift

**8.4 Despliegue de Modelos**
- SerializaciÃ³n de modelos (pickle, joblib, ONNX)
- REST APIs con FastAPI
- ContainerizaciÃ³n (Docker)
- Serving predictions (batch vs online)
- Escalabilidad de modelos en producciÃ³n

**8.5 Large Language Models (LLMs)**
- Conceptos de LLMs
- APIs: OpenAI, Anthropic, Hugging Face
- Prompt engineering
- Embeddings y semantic search
- Cost optimization en LLM usage

**8.6 RAG (Retrieval Augmented Generation)**
- Arquitectura RAG
- Vector databases (Pinecone, Weaviate, Chroma)
- Document chunking y indexing
- Embedding models
- Retrieval strategies
- ImplementaciÃ³n prÃ¡ctica de RAG

**8.7 AI en Pipelines de Datos**
- Data quality con ML (anomaly detection)
- Auto-classification de datos
- NLP para extracciÃ³n de informaciÃ³n
- LLMs para transformaciÃ³n de datos
- Automated data labeling

#### TecnologÃ­as/Herramientas
- scikit-learn (bÃ¡sico)
- MLflow
- DVC
- FastAPI
- Docker
- OpenAI API / Anthropic Claude
- LangChain
- Vector databases (Chroma, Pinecone)
- Hugging Face

#### Proyectos PrÃ¡cticos

1. **Pipeline ML end-to-end**: Desde datos raw hasta modelo en producciÃ³n con MLflow
2. **Feature Store**: Implementar feature store con Feast para reutilizaciÃ³n
3. **API de predicciones**: FastAPI + modelo ML dockerizado
4. **Sistema RAG**: Chatbot que responde preguntas sobre documentaciÃ³n usando RAG
5. **Data Quality con ML**: DetecciÃ³n automÃ¡tica de anomalÃ­as en pipelines

#### Recursos Recomendados

- Libro: "Designing Machine Learning Systems" - Chip Huyen
- Libro: "Building LLMs for Production" - O'Reilly
- Curso: Made With ML - MLOps
- DocumentaciÃ³n: LangChain Documentation
- Curso: DeepLearning.AI - LangChain for LLM Development

#### Criterios de EvaluaciÃ³n

- [ ] Pipelines ML reproducibles y versionados
- [ ] Modelos desplegados con APIs documentadas
- [ ] Monitoreo de modelos implementado
- [ ] Sistema RAG funcional con retrieval relevante
- [ ] Manejo seguro de API keys y secrets
- [ ] Tests para componentes ML
- [ ] DocumentaciÃ³n de decisiones de arquitectura

---

### MÃ“DULO 9: DataOps, Calidad y Gobernanza

**DuraciÃ³n:** 6-8 semanas  
**Nivel:** Avanzado

#### Objetivos de Aprendizaje

Al completar este mÃ³dulo, el estudiante serÃ¡ capaz de:

- Implementar prÃ¡cticas de DataOps en equipos de datos
- DiseÃ±ar frameworks de calidad de datos
- Establecer gobernanza y compliance
- Gestionar metadata y data catalogs
- Implementar data lineage completo
- Aplicar data observability

#### Temas y Subtemas

**9.1 DataOps Principles**
- DevOps vs DataOps
- Continuous integration/deployment para datos
- Collaboration en equipos de datos
- Automation y orchestration
- Monitoring y feedback loops

**9.2 Calidad de Datos**
- Dimensiones de calidad (accuracy, completeness, consistency, timeliness)
- Data profiling
- Data validation frameworks (Great Expectations)
- Schema validation
- Reconciliation y data testing
- SLAs de calidad

**9.3 Data Governance**
- Conceptos de gobernanza
- Data ownership y stewardship
- PolÃ­ticas de acceso y permisos
- GDPR, CCPA y compliance
- Data retention policies
- Privacy by design

**9.4 Metadata Management**
- Tipos de metadata (technical, business, operational)
- Data catalogs (DataHub, Amundsen, Alation)
- Discovery y searchability
- Tagging y classification
- Business glossary

**9.5 Data Lineage**
- Importancia del lineage
- Tracking de transformaciones
- Impact analysis
- Tools: OpenLineage, DataHub
- VisualizaciÃ³n de lineage

**9.6 Data Observability**
- Monitoreo proactivo de datos
- Freshness, volume, distribution checks
- Anomaly detection
- Incident management
- Tools: Monte Carlo, Great Expectations

**9.7 Seguridad Avanzada**
- Column-level security
- Row-level security
- Data masking y tokenization
- Encryption strategies
- Audit logging
- Zero trust architecture

#### TecnologÃ­as/Herramientas
- Great Expectations
- DataHub / Amundsen
- OpenLineage
- dbt (docs y tests)
- Apache Atlas
- Airflow (monitoring)

#### Proyectos PrÃ¡cticos

1. **Framework de calidad**: Sistema completo de validaciÃ³n con Great Expectations
2. **Data Catalog**: Implementar DataHub con metadata de todos los assets de datos
3. **Lineage tracking**: Integrar OpenLineage en pipelines de Airflow
4. **Data governance policy**: Documentar polÃ­ticas y implementar controles

#### Recursos Recomendados

- Libro: "Data Governance: The Definitive Guide" - O'Reilly
- Curso: Data Quality Fundamentals
- DocumentaciÃ³n: Great Expectations
- ArtÃ­culo: The Rise of Data Observability
- Whitepaper: DataHub Architecture

#### Criterios de EvaluaciÃ³n

- [ ] Framework de calidad automatizado
- [ ] Data catalog poblado y actualizado
- [ ] Lineage tracking funcional
- [ ] PolÃ­ticas de gobernanza documentadas
- [ ] Seguridad implementada (RBAC, encryption)
- [ ] Monitoreo proactivo con alertas
- [ ] Documentation completa de procesos

---

### MÃ“DULO 10: Proyecto Final y EspecializaciÃ³n

**DuraciÃ³n:** 12-16 semanas  
**Nivel:** Master

#### Objetivos de Aprendizaje

Al completar este mÃ³dulo, el estudiante serÃ¡ capaz de:

- DiseÃ±ar e implementar una plataforma de datos end-to-end
- Liderar decisiones de arquitectura de datos
- Integrar todos los conocimientos del master
- Presentar y defender decisiones tÃ©cnicas
- Construir un portafolio profesional destacado

#### DescripciÃ³n

El Proyecto Final es una plataforma de datos completa que integra todos los mÃ³dulos del master. El estudiante puede elegir entre varias especializaciones o proponer un proyecto personalizado.

#### Opciones de Proyecto

**OpciÃ³n 1: Plataforma de E-commerce Analytics**
- Pipeline de datos desde mÃºltiples fuentes (web, mÃ³vil, CRM)
- Data Warehouse dimensional
- Procesamiento real-time de eventos de navegaciÃ³n
- Sistema de recomendaciones con ML
- Dashboards y APIs de analytics
- ImplementaciÃ³n completa en cloud

**OpciÃ³n 2: Plataforma de IoT y Smart Cities**
- Ingesta de datos de sensores en tiempo real
- Procesamiento streaming con Kafka + Spark
- DetecciÃ³n de anomalÃ­as con ML
- Data Lake para almacenamiento histÃ³rico
- APIs pÃºblicas para acceso a datos
- VisualizaciÃ³n geoespacial

**OpciÃ³n 3: Plataforma de Social Media Analytics**
- ExtracciÃ³n de datos de redes sociales (APIs)
- NLP para anÃ¡lisis de sentimiento
- Data Warehouse para trending topics
- Sistema RAG para Q&A sobre contenido
- Dashboards en tiempo real
- AnÃ¡lisis predictivo de viralidad

**OpciÃ³n 4: Plataforma de Healthcare Analytics**
- Pipeline de datos de salud (compliance con HIPAA)
- Data Lake con niveles de acceso
- ML para predicciÃ³n de riesgos
- Feature store para modelos clÃ­nicos
- Data governance estricta
- AuditorÃ­a completa

**OpciÃ³n 5: Proyecto Personalizado**
- Propuesta del estudiante
- Debe integrar al menos 7 de los 9 mÃ³dulos anteriores
- AprobaciÃ³n de scope por mentor/instructor

#### Componentes Obligatorios del Proyecto

1. **Arquitectura**
   - Diagrama de arquitectura completo
   - JustificaciÃ³n de tecnologÃ­as elegidas
   - Plan de escalabilidad
   - Consideraciones de seguridad

2. **ImplementaciÃ³n**
   - CÃ³digo fuente completo en GitHub
   - IaC con Terraform
   - Docker/Kubernetes para deployment
   - CI/CD pipeline funcional

3. **Data Pipelines**
   - ETL/ELT con orquestaciÃ³n (Airflow)
   - Transformaciones con dbt
   - Tests automatizados
   - Data quality checks

4. **Almacenamiento**
   - Data Lake y/o Data Warehouse
   - Modelo dimensional (si aplica)
   - OptimizaciÃ³n de storage
   - Backup y disaster recovery

5. **ML/AI Component**
   - Al menos un modelo en producciÃ³n
   - Feature store (opcional pero recomendado)
   - Monitoring de modelo
   - API de predicciones

6. **Observability**
   - Logging centralizado
   - Monitoreo de pipelines
   - Alertas configuradas
   - Data lineage

7. **DocumentaciÃ³n**
   - README exhaustivo
   - Architecture Decision Records (ADRs)
   - DocumentaciÃ³n de APIs
   - Runbooks operacionales
   - Manual de usuario

8. **PresentaciÃ³n**
   - PresentaciÃ³n de 30-40 minutos
   - Demo funcional
   - Resultados y mÃ©tricas
   - Lecciones aprendidas
   - PrÃ³ximos pasos

#### Criterios de EvaluaciÃ³n del Proyecto Final

**Arquitectura y DiseÃ±o (20%)**
- [ ] Arquitectura bien diseÃ±ada y escalable
- [ ] Decisiones tÃ©cnicas justificadas
- [ ] Diagramas claros y completos
- [ ] Plan de contingencia y DR

**ImplementaciÃ³n TÃ©cnica (30%)**
- [ ] CÃ³digo limpio y mantenible
- [ ] Tests con >80% coverage
- [ ] CI/CD funcional
- [ ] IaC reproducible
- [ ] Seguridad implementada

**Data Engineering (25%)**
- [ ] Pipelines robustos e idempotentes
- [ ] Data quality garantizada
- [ ] OrquestaciÃ³n efectiva
- [ ] Performance optimizado
- [ ] Manejo de errores completo

**InnovaciÃ³n y ML/AI (15%)**
- [ ] IntegraciÃ³n efectiva de IA
- [ ] Modelos en producciÃ³n funcionando
- [ ] Uso innovador de tecnologÃ­as
- [ ] RAG u otras tÃ©cnicas avanzadas (bonus)

**DocumentaciÃ³n y PresentaciÃ³n (10%)**
- [ ] DocumentaciÃ³n completa y clara
- [ ] CÃ³digo auto-documentado
- [ ] PresentaciÃ³n profesional
- [ ] Demo fluida y convincente
- [ ] Capacidad de defender decisiones

#### EspecializaciÃ³n Post-Proyecto

Tras completar el proyecto, el estudiante puede profundizar en una especializaciÃ³n:

1. **Real-Time Data Engineering**: Streaming, Kafka, Flink
2. **ML Engineering**: MLOps avanzado, model optimization
3. **Cloud Architecture**: Multi-cloud, FinOps, SRE para datos
4. **Data Governance Leader**: Compliance, privacy, enterprise governance
5. **AI/LLM Engineering**: LLM fine-tuning, advanced RAG, agents

#### Recursos para el Proyecto Final

- MentorÃ­as 1:1 (si disponible)
- Comunidad de estudiantes para feedback
- RevisiÃ³n de cÃ³digo por pares
- Acceso a crÃ©ditos cloud (AWS, GCP, Azure)
- Templates de documentaciÃ³n
- Ejemplos de proyectos anteriores

---

## CertificaciÃ³n y PrÃ³ximos Pasos

### ObtenciÃ³n del Certificado

Para obtener el certificado de **Master en IngenierÃ­a de Datos con IA**, el estudiante debe:

1. Completar los 10 mÃ³dulos con evaluaciÃ³n satisfactoria
2. Aprobar el Proyecto Final con calificaciÃ³n mÃ­nima de 80/100
3. Presentar portafolio en GitHub con al menos 5 proyectos destacados
4. (Opcional) Obtener una certificaciÃ³n cloud (AWS/GCP/Azure)

### Perfil Profesional al Egresar

El egresado estarÃ¡ preparado para roles como:

- Data Engineer (Senior)
- Machine Learning Engineer
- Cloud Data Architect
- Data Platform Engineer
- MLOps Engineer
- Analytics Engineer

### Salario Esperado (Referencia 2024-2025)

- Junior Data Engineer: $50,000 - $80,000 USD/aÃ±o
- Mid-Level Data Engineer: $80,000 - $120,000 USD/aÃ±o
- Senior Data Engineer: $120,000 - $180,000+ USD/aÃ±o
- (VarÃ­a segÃºn ubicaciÃ³n, empresa y especializaciÃ³n)

### Continuar Aprendiendo

La ingenierÃ­a de datos evoluciona constantemente. Recomendaciones:

- Suscribirse a newsletters (Data Engineering Weekly, DataEngineer.io)
- Participar en comunidades (dbt Community, Airflow Slack)
- Contribuir a proyectos open source
- Mantenerse actualizado con nuevas tecnologÃ­as
- Asistir a conferencias (Data Council, Spark Summit)

---

## Soporte y Comunidad

### Recursos de Ayuda

- **DocumentaciÃ³n**: Ver carpeta `documentacion/`
- **Proyectos de referencia**: Ver `documentacion/PROYECTOS_PRACTICOS.md`
- **Recursos externos**: Ver `documentacion/RECURSOS.md`

### Comunidad

Se recomienda unirse a:

- Stack Overflow (tag: data-engineering)
- Reddit: r/dataengineering
- LinkedIn Data Engineering Groups
- Discord/Slack communities especÃ­ficas de cada herramienta

---

## Estado de ImplementaciÃ³n

### MÃ³dulos Completados

| MÃ³dulo | Nombre | Estado | Tests | Cobertura |
|--------|--------|--------|-------|-----------|
| 1 | Fundamentos de ProgramaciÃ³n | âœ… 100% | 50+ | >80% |
| 2 | Bases de Datos y SQL | âœ… 100% | 40+ | >80% |
| 3 | IngenierÃ­a de Datos Core | âœ… 100% | 60+ | >80% |
| 4 | APIs y Web Scraping | âœ… 100% | 45+ | >80% |
| 5 | Bases de Datos Avanzadas | âœ… 100% | 25+ | >80% |
| 6 | OrquestaciÃ³n (Airflow) | ðŸš§ En desarrollo | - | - |
| 7 | Cloud Data Engineering | ðŸš§ En desarrollo | - | - |
| **8** | **Data Warehousing y Analytics** | **âœ… 100%** | **280+** | **91%** |
| 9 | DataOps y Gobernanza | ðŸ“‹ Planificado | - | - |
| 10 | Proyecto Final | ðŸ“‹ Planificado | - | - |

### MÃ“DULO 8: Data Warehousing y Analytics âœ… COMPLETADO

**DuraciÃ³n:** 4-6 semanas
**Nivel:** Avanzado

**Estado:** âœ… COMPLETADO (2025-11-30)

#### Temas Implementados

1. âœ… **Dimensional Modeling** (Tema 1)
   - TeorÃ­a: Star Schema, Snowflake, SCD Types
   - Ejemplos y Ejercicios completos
   - Proyecto prÃ¡ctico: 156 tests, 93% cobertura

2. âœ… **Herramientas DWH - dbt** (Tema 2)
   - TeorÃ­a: ELT, Materializaciones, Testing
   - Pipeline dbt completo con TechMart Analytics
   - ~44 tests (genÃ©ricos + personalizados)

3. âœ… **Analytics y BI** (Tema 3)
   - TeorÃ­a: KPIs, Dashboards, Data Storytelling
   - Proyecto prÃ¡ctico: Sistema de MÃ©tricas
   - 84 tests, 92% cobertura

#### MÃ©tricas del MÃ³dulo

- **Total de tests**: 280+
- **Cobertura promedio**: 91%
- **Tiempo de estudio estimado**: 60-75 horas
- **Proyectos prÃ¡cticos**: 3/3 completos

---

## Changelog del Programa

### VersiÃ³n 1.1.0 (Noviembre 2025)
- **JAR-328**: MÃ³dulo 8 (Data Warehousing) completado y documentado
  - Tema 1: Dimensional Modeling con proyecto prÃ¡ctico
  - Tema 2: Herramientas DWH con pipeline dbt
  - Tema 3: Analytics y BI con sistema de mÃ©tricas
- 280+ tests con 91% de cobertura promedio
- README del mÃ³dulo completo y documentado

### VersiÃ³n 1.0 (Octubre 2024)
- CreaciÃ³n inicial del programa
- 10 mÃ³dulos completos
- IntegraciÃ³n de IA y LLMs
- Enfoque en arquitectura cloud moderna
- Ã‰nfasis en seguridad y gobernanza

---

**Â¡Bienvenido al Master en IngenierÃ­a de Datos con IA!**

Este es un viaje desafiante pero extremadamente gratificante. Recuerda: la constancia es mÃ¡s importante que la perfecciÃ³n. Avanza a tu ritmo, construye proyectos reales y disfruta el proceso de convertirte en un Data Engineer experto.

**Â¡Ã‰xito en tu aprendizaje! ðŸš€**
