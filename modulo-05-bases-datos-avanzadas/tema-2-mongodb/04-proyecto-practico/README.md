# Proyecto Pr√°ctico: Sistema de An√°lisis de Logs con MongoDB

Sistema para procesar, analizar y detectar anomal√≠as en logs de aplicaciones usando MongoDB y agregaciones complejas.

## üéØ Objetivos

- **Procesar logs** con estructura flexible de MongoDB (documentos)
- **Construir pipelines de agregaci√≥n** complejos ($match, $group, $project, $sort)
- **Analizar datos** para identificar servicios cr√≠ticos y anomal√≠as
- **Aplicar TDD** con >80% de cobertura de c√≥digo
- **Dominar MongoDB** a trav√©s de un caso de uso real

## üìö Conceptos Aplicados

### MongoDB
- **Documentos flexibles**: Logs con estructura variable
- **Operadores de agregaci√≥n**: $group, $match, $project, $sort, $limit
- **An√°lisis temporal**: $hour, $dayOfWeek, $dateFromString
- **Operadores condicionales**: $cond, $eq para conteos selectivos

### An√°lisis de Logs
- **Parsing**: Extracci√≥n de componentes de logs estructurados
- **Validaci√≥n**: Verificaci√≥n de campos obligatorios
- **M√©tricas**: Tasa de error, servicios cr√≠ticos
- **Detecci√≥n de anomal√≠as**: Tiempos de respuesta altos, errores concentrados

### Buenas Pr√°cticas
- **TDD (Test-Driven Development)**: 56 tests implementados antes del c√≥digo
- **Type Hints**: Tipado expl√≠cito en todas las funciones
- **Funciones puras**: Sin efectos secundarios
- **Cobertura >80%**: 99% alcanzado

## üìÅ Estructura del Proyecto

```
04-proyecto-practico/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ log_processor.py         # Procesamiento de logs (33 l√≠neas, 100% cov)
‚îÇ   ‚îú‚îÄ‚îÄ aggregation_builder.py   # Construcci√≥n de pipelines (25 l√≠neas, 96% cov)
‚îÇ   ‚îî‚îÄ‚îÄ analytics.py              # An√°lisis y m√©tricas (62 l√≠neas, 100% cov)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_log_processor.py        # 19 tests
‚îÇ   ‚îú‚îÄ‚îÄ test_aggregation_builder.py  # 18 tests
‚îÇ   ‚îî‚îÄ‚îÄ test_analytics.py            # 19 tests
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ (logs de ejemplo)
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .gitignore
```

## üöÄ Instalaci√≥n

```bash
# Activar entorno virtual
cd modulo-05-bases-datos-avanzadas/tema-2-mongodb/04-proyecto-practico

# En Windows:
..\..\..\..\.venv\Scripts\Activate.ps1

# En Linux/Mac:
source ../../../../.venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt
```

## ‚úÖ Ejecutar Tests

```bash
# Todos los tests
pytest -v

# Con cobertura
pytest --cov=src --cov-report=html --cov-report=term

# Solo un m√≥dulo
pytest tests/test_log_processor.py -v
```

**Resultados**:
- ‚úÖ **56 tests pasando** (100% success rate)
- ‚úÖ **99% cobertura** (supera objetivo ‚â•80%)
- ‚úÖ **121 statements**, solo 1 miss

## üì¶ M√≥dulos Implementados

### 1. Log Processor (`log_processor.py`)

Funciones para procesar entradas de logs.

#### `parsear_log_entry(log_string: str) -> dict[str, str]`

Parsea un string de log en documento estructurado.

```python
from src.log_processor import parsear_log_entry

# Ejemplo
log = "2024-11-12 14:30:45 | ERROR | UserService | Database connection failed"
resultado = parsear_log_entry(log)

print(resultado)
# {
#     'timestamp': '2024-11-12 14:30:45',
#     'nivel': 'ERROR',
#     'servicio': 'UserService',
#     'mensaje': 'Database connection failed'
# }
```

#### `validar_log_entry(log_entry: dict) -> None`

Valida que un log tenga todos los campos obligatorios.

```python
from src.log_processor import validar_log_entry

log_entry = {
    "timestamp": "2024-11-12 14:30:45",
    "nivel": "ERROR",
    "servicio": "UserService",
    "mensaje": "Test"
}

validar_log_entry(log_entry)  # No lanza error si es v√°lido
```

#### `normalizar_timestamp(timestamp_str: str) -> datetime`

Convierte timestamp string a objeto datetime.

```python
from src.log_processor import normalizar_timestamp

timestamp_str = "2024-11-12 14:30:45"
dt = normalizar_timestamp(timestamp_str)

print(dt)  # datetime.datetime(2024, 11, 12, 14, 30, 45)
```

#### `extraer_nivel_severidad(log_entry: dict) -> str`

Extrae y valida el nivel de severidad.

```python
from src.log_processor import extraer_nivel_severidad

log_entry = {"nivel": "ERROR"}
nivel = extraer_nivel_severidad(log_entry)

print(nivel)  # 'ERROR'
```

---

### 2. Aggregation Builder (`aggregation_builder.py`)

Funciones para construir pipelines de agregaci√≥n de MongoDB.

#### `construir_pipeline_errores(fecha_inicio: str, fecha_fin: str) -> list[dict]`

Construye pipeline para filtrar y agrupar errores por per√≠odo.

```python
from src.aggregation_builder import construir_pipeline_errores

pipeline = construir_pipeline_errores("2024-11-01", "2024-11-30")

# Uso con MongoDB
# db.logs.aggregate(pipeline)

print(pipeline[0])
# {
#     '$match': {
#         'nivel': 'ERROR',
#         'timestamp': {'$gte': '2024-11-01', '$lte': '2024-11-30'}
#     }
# }
```

#### `construir_pipeline_por_servicio() -> list[dict]`

Construye pipeline para agrupar logs por servicio con m√©tricas de error.

```python
from src.aggregation_builder import construir_pipeline_por_servicio

pipeline = construir_pipeline_por_servicio()

# Uso con MongoDB
# db.logs.aggregate(pipeline)

# Retorna:
# [
#     {'servicio': 'UserService', 'total_logs': 1500, 'porcentaje_error': 12.5},
#     {'servicio': 'PaymentService', 'total_logs': 800, 'porcentaje_error': 5.2},
#     ...
# ]
```

#### `construir_pipeline_top_usuarios(limite: int) -> list[dict]`

Construye pipeline para obtener top usuarios con m√°s actividad.

```python
from src.aggregation_builder import construir_pipeline_top_usuarios

pipeline = construir_pipeline_top_usuarios(10)

# db.logs.aggregate(pipeline)

# Retorna top 10 usuarios:
# [
#     {'usuario_id': 'user_123', 'total_acciones': 450, 'cantidad_servicios': 8},
#     ...
# ]
```

#### `construir_pipeline_metricas_tiempo() -> list[dict]`

Construye pipeline para analizar m√©tricas por per√≠odo de tiempo.

```python
from src.aggregation_builder import construir_pipeline_metricas_tiempo

pipeline = construir_pipeline_metricas_tiempo()

# db.logs.aggregate(pipeline)

# Retorna m√©tricas por hora:
# [
#     {'hora': 0, 'total_logs': 120, 'errores': 5, 'warnings': 15},
#     {'hora': 1, 'total_logs': 98, 'errores': 2, 'warnings': 8},
#     ...
# ]
```

---

### 3. Analytics (`analytics.py`)

Funciones para analizar logs y detectar problemas.

#### `calcular_tasa_error(logs: list[dict]) -> float`

Calcula la tasa de error como porcentaje del total.

```python
from src.analytics import calcular_tasa_error

logs = [
    {"nivel": "ERROR"},
    {"nivel": "ERROR"},
    {"nivel": "INFO"},
    {"nivel": "WARNING"},
    {"nivel": "INFO"},
]

tasa = calcular_tasa_error(logs)
print(tasa)  # 40.0 (2 errores de 5 logs)
```

#### `identificar_servicios_criticos(logs: list[dict], umbral_error: float) -> list[dict]`

Identifica servicios con tasa de error por encima del umbral.

```python
from src.analytics import identificar_servicios_criticos

logs = [
    {"servicio": "UserService", "nivel": "ERROR"},
    {"servicio": "UserService", "nivel": "ERROR"},
    {"servicio": "UserService", "nivel": "INFO"},
    {"servicio": "PaymentService", "nivel": "INFO"},
    {"servicio": "PaymentService", "nivel": "INFO"},
]

criticos = identificar_servicios_criticos(logs, umbral_error=50.0)

print(criticos)
# [
#     {
#         'servicio': 'UserService',
#         'total_logs': 3,
#         'total_errores': 2,
#         'tasa_error': 66.67
#     }
# ]
```

#### `generar_reporte_resumen(logs: list[dict]) -> dict`

Genera un reporte resumen con m√©tricas generales.

```python
from src.analytics import generar_reporte_resumen

logs = [
    {"nivel": "ERROR", "servicio": "Service1"},
    {"nivel": "INFO", "servicio": "Service2"},
    {"nivel": "WARNING", "servicio": "Service1"},
    {"nivel": "INFO", "servicio": "Service2"},
]

reporte = generar_reporte_resumen(logs)

print(reporte)
# {
#     'total_logs': 4,
#     'errores': 1,
#     'warnings': 1,
#     'info': 2,
#     'servicios_unicos': 2,
#     'tasa_error': 25.0
# }
```

#### `detectar_anomalias(logs: list[dict], umbral_tiempo: int) -> list[dict]`

Detecta anomal√≠as en logs (tiempos altos, errores concentrados).

```python
from src.analytics import detectar_anomalias

logs = [
    {"servicio": "Service1", "nivel": "INFO", "tiempo_respuesta": 100},
    {"servicio": "Service1", "nivel": "INFO", "tiempo_respuesta": 120},
    {"servicio": "Service1", "nivel": "WARNING", "tiempo_respuesta": 5000},
]

anomalias = detectar_anomalias(logs, umbral_tiempo=1000)

print(len(anomalias))  # 1
print(anomalias[0]["tiempo_respuesta"])  # 5000
```

---

## üéì Ejemplos de Uso Completo

### Ejemplo 1: An√°lisis completo de logs de un servicio

```python
from src.log_processor import parsear_log_entry
from src.analytics import calcular_tasa_error, identificar_servicios_criticos

# Logs en formato string
logs_raw = [
    "2024-11-12 14:30:45 | ERROR | UserService | Connection timeout",
    "2024-11-12 14:31:12 | ERROR | UserService | Query failed",
    "2024-11-12 14:32:05 | INFO | UserService | Request processed",
    "2024-11-12 14:33:20 | WARNING | PaymentService | Slow response",
    "2024-11-12 14:34:15 | INFO | PaymentService | Payment completed",
]

# Parsear logs
logs = [parsear_log_entry(log) for log in logs_raw]

# Calcular tasa de error
tasa = calcular_tasa_error(logs)
print(f"Tasa de error: {tasa}%")  # 40.0%

# Identificar servicios cr√≠ticos
criticos = identificar_servicios_criticos(logs, umbral_error=50.0)

for servicio in criticos:
    print(f"\n‚ö†Ô∏è Servicio Cr√≠tico: {servicio['servicio']}")
    print(f"   Total logs: {servicio['total_logs']}")
    print(f"   Errores: {servicio['total_errores']}")
    print(f"   Tasa error: {servicio['tasa_error']}%")
```

### Ejemplo 2: Construir pipeline para an√°lisis en MongoDB

```python
from src.aggregation_builder import (
    construir_pipeline_errores,
    construir_pipeline_por_servicio
)
from pymongo import MongoClient

# Conectar a MongoDB
client = MongoClient("mongodb://localhost:27017/")
db = client["logs_db"]

# Pipeline para errores del √∫ltimo mes
pipeline_errores = construir_pipeline_errores("2024-11-01", "2024-11-30")
errores = list(db.logs.aggregate(pipeline_errores))

print("Servicios con m√°s errores:")
for item in errores[:5]:
    print(f"- {item['_id']}: {item['total_errores']} errores")

# Pipeline para an√°lisis por servicio
pipeline_servicios = construir_pipeline_por_servicio()
servicios = list(db.logs.aggregate(pipeline_servicios))

print("\nEstad√≠sticas por servicio:")
for servicio in servicios:
    print(f"\n{servicio['servicio']}:")
    print(f"  Total logs: {servicio['total_logs']}")
    print(f"  % Error: {servicio['porcentaje_error']:.2f}%")
```

### Ejemplo 3: Detecci√≥n de anomal√≠as en tiempo real

```python
from src.analytics import detectar_anomalias, generar_reporte_resumen

# Simular logs en tiempo real
logs_tiempo_real = [
    {"servicio": "API", "nivel": "INFO", "tiempo_respuesta": 120},
    {"servicio": "API", "nivel": "INFO", "tiempo_respuesta": 135},
    {"servicio": "API", "nivel": "WARNING", "tiempo_respuesta": 4500},  # Anomal√≠a
    {"servicio": "Database", "nivel": "INFO", "tiempo_respuesta": 50},
    {"servicio": "Database", "nivel": "ERROR", "tiempo_respuesta": 8000},  # Anomal√≠a
]

# Detectar anomal√≠as
anomalias = detectar_anomalias(logs_tiempo_real, umbral_tiempo=1000)

if anomalias:
    print(f"‚ö†Ô∏è Se detectaron {len(anomalias)} anomal√≠as:")
    for anomalia in anomalias:
        print(f"\n  Servicio: {anomalia['servicio']}")
        print(f"  Nivel: {anomalia['nivel']}")
        print(f"  Tiempo: {anomalia['tiempo_respuesta']}ms")

# Generar reporte
reporte = generar_reporte_resumen(logs_tiempo_real)
print(f"\nReporte General:")
print(f"  Total logs: {reporte['total_logs']}")
print(f"  Tasa error: {reporte['tasa_error']}%")
print(f"  Servicios: {reporte['servicios_unicos']}")
```

---

## üß™ Cobertura de Tests

```
Name                         Stmts   Miss  Cover
------------------------------------------------
src/__init__.py                  1      0   100%
src/aggregation_builder.py     25      1    96%
src/analytics.py                62      0   100%
src/log_processor.py            33      0   100%
------------------------------------------------
TOTAL                          121      1    99%
```

**Detalle por m√≥dulo**:

| M√≥dulo | Tests | Cobertura | Estado |
|--------|-------|-----------|--------|
| `log_processor.py` | 19 | 100% | ‚úÖ |
| `aggregation_builder.py` | 18 | 96% | ‚úÖ |
| `analytics.py` | 19 | 100% | ‚úÖ |
| **TOTAL** | **56** | **99%** | ‚úÖ **SUPERADO** |

---

## üîß Tecnolog√≠as Utilizadas

- **Python 3.13+**: Lenguaje principal
- **pymongo**: Driver oficial de MongoDB para Python
- **pytest**: Framework de testing
- **pytest-cov**: Cobertura de c√≥digo
- **faker**: Generaci√≥n de datos de prueba
- **black**: Formateo autom√°tico
- **flake8**: Linting
- **mypy**: Type checking

---

## üìä Arquitectura

### Dise√±o Funcional

El proyecto sigue un dise√±o **funcional puro**:
- **Sin clases** (solo funciones)
- **Funciones peque√±as** (<50 l√≠neas)
- **Sin efectos secundarios**: Funciones predecibles
- **Composabilidad**: Funciones que se combinan f√°cilmente

### Flujo de Datos

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Logs en string  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  log_processor.py   ‚îÇ  ‚Üê Parsea y valida
‚îÇ  - parsear_log      ‚îÇ
‚îÇ  - validar_log      ‚îÇ
‚îÇ  - normalizar_fecha ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  aggregation_builder   ‚îÇ  ‚Üê Construye pipelines
‚îÇ  - pipeline_errores    ‚îÇ
‚îÇ  - pipeline_servicios  ‚îÇ
‚îÇ  - pipeline_usuarios   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   analytics.py   ‚îÇ  ‚Üê Analiza resultados
‚îÇ  - tasa_error    ‚îÇ
‚îÇ  - servicios     ‚îÇ
‚îÇ  - anomal√≠as     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Insights +      ‚îÇ
‚îÇ  Reportes        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üêõ Troubleshooting

### Error: "ModuleNotFoundError: No module named 'pymongo'"
**Soluci√≥n**: Instalar dependencias
```bash
pip install -r requirements.txt
```

### Error: Tests fallan con "ImportError"
**Soluci√≥n**: Ejecutar desde el directorio del proyecto
```bash
cd modulo-05-bases-datos-avanzadas/tema-2-mongodb/04-proyecto-practico
pytest -v
```

### Error: "ValueError: Formato de log inv√°lido"
**Soluci√≥n**: Verificar formato del log
```python
# Formato correcto:
log = "YYYY-MM-DD HH:MM:SS | NIVEL | SERVICIO | MENSAJE"

# Ejemplo:
log = "2024-11-12 14:30:45 | ERROR | UserService | Connection failed"
```

---

## üìö Recursos Adicionales

- [PyMongo Documentation](https://pymongo.readthedocs.io/)
- [MongoDB Aggregation Framework](https://www.mongodb.com/docs/manual/aggregation/)
- [MongoDB University](https://learn.mongodb.com/)
- [Aggregation Pipeline Builder](https://www.mongodb.com/docs/compass/current/aggregation-pipeline-builder/)

---

## üéØ Pr√≥ximos Pasos

1. Agregar **conexi√≥n real** a MongoDB
2. Implementar **streaming** de logs en tiempo real
3. Crear **dashboard** con visualizaciones
4. Agregar **alertas** autom√°ticas por Slack/Email
5. Implementar **Machine Learning** para detecci√≥n de anomal√≠as avanzada
6. Exportar reportes a **PDF/Excel**

---

**Proyecto completado** ‚úÖ
**Tests**: 56/56 pasando (100%)
**Cobertura**: 99% (supera objetivo ‚â•80%)
**Calidad**: TDD con funciones puras

**¬°√âxito en tu aprendizaje de MongoDB!** üöÄ
---

## üß≠ Navegaci√≥n

‚¨ÖÔ∏è **Anterior**: [03 Ejercicios](../03-EJERCICIOS.md) | ‚û°Ô∏è **Siguiente**: [Modelado de Datos - 01 Teoria](../../tema-3-modelado-datos/01-TEORIA.md)
