# Tema 2: Extracci√≥n de Datos - Fuentes y T√©cnicas

## Introducci√≥n: ¬øPor qu√© importa la extracci√≥n de datos?

Imagina que eres un chef que necesita ingredientes para cocinar. Los ingredientes pueden estar en el supermercado (archivos CSV), en una tienda especializada que necesita pedido especial (APIs), o en el mercado local donde tienes que seleccionar producto por producto (web scraping). Cada fuente requiere una t√©cnica diferente para obtener lo que necesitas.

En Data Engineering, la **extracci√≥n de datos** es el primer paso cr√≠tico de cualquier pipeline ETL/ELT. Sin datos, no hay transformaci√≥n ni an√°lisis posible. Pero los datos viven en lugares muy diferentes: archivos locales, APIs de terceros, p√°ginas web, bases de datos, etc.

**¬øPor qu√© es un desaf√≠o?**
- Cada fuente tiene su propio formato y protocolo
- Los datos pueden estar "sucios" o mal estructurados
- Las fuentes pueden ser inestables o lentas
- Necesitas manejar errores y reintentos
- Debes respetar l√≠mites de uso (rate limits)
- La seguridad y √©tica son fundamentales

En este tema aprender√°s a extraer datos de las **3 fuentes m√°s comunes** en Data Engineering:
1. **Archivos** (CSV, JSON, Excel)
2. **APIs REST** (requests, autenticaci√≥n, paginaci√≥n)
3. **Web Scraping** (Beautiful Soup, scraping √©tico)

---

## Parte 1: Extracci√≥n desde Archivos

### 1.1 Archivos CSV (Comma-Separated Values)

#### ¬øQu√© es un CSV?

Un CSV es un formato de texto plano donde cada l√≠nea representa una fila de datos y las columnas est√°n separadas por un car√°cter delimitador (normalmente una coma).

**Analog√≠a**: Es como una hoja de Excel guardada en formato de texto simple. Si abres un CSV con el Bloc de notas, ver√≠as algo as√≠:

```
nombre,edad,ciudad
Ana,28,Madrid
Carlos,35,Barcelona
```

#### ¬øPor qu√© CSV es popular en Data Engineering?

- ‚úÖ **Universal**: Funciona en cualquier sistema operativo
- ‚úÖ **Ligero**: Ocupa poco espacio
- ‚úÖ **F√°cil de leer**: Puedes abrirlo con cualquier editor de texto
- ‚úÖ **Compatible**: Excel, Python, R, SQL... todos lo entienden

#### Desaf√≠os comunes con CSV

**1. Encoding (Codificaci√≥n de caracteres)**

El encoding determina c√≥mo se guardan los caracteres especiales (tildes, e√±es, emojis). Los encodings m√°s comunes son:

- **UTF-8**: El est√°ndar moderno, soporta todos los idiomas y emojis
- **Latin-1 (ISO-8859-1)**: Com√∫n en sistemas antiguos europeos
- **UTF-8-BOM**: UTF-8 con una marca especial al inicio del archivo

**Analog√≠a**: Es como si un libro estuviera escrito en espa√±ol pero con instrucciones en chino sobre c√≥mo leerlo. Si no sabes las instrucciones correctas, las tildes y e√±es se ven mal.

**Ejemplo de problema**:
```python
# Si intentas leer un CSV Latin-1 como UTF-8:
# "Jos√©" se ver√≠a como "Jos√©" o dar√≠a error
```

**Soluci√≥n**:
```python
import csv

# Especificar el encoding correcto
with open('datos.csv', 'r', encoding='latin-1') as archivo:
    lector = csv.reader(archivo)
    for fila in lector:
        print(fila)
```

**2. Delimitadores**

Aunque se llaman "Comma-Separated", no siempre usan comas. Pueden usar:
- `,` (coma) - Lo m√°s com√∫n en pa√≠ses anglosajones
- `;` (punto y coma) - Com√∫n en Europa (donde la coma es decimal)
- `\t` (tabulador) - Archivos TSV
- `|` (pipe) - Bases de datos

**3. Headers (Cabeceras)**

Algunos CSV tienen la primera fila con nombres de columnas, otros no:

```
# CON header
nombre,edad,ciudad
Ana,28,Madrid

# SIN header (solo datos)
Ana,28,Madrid
Carlos,35,Barcelona
```

#### Buenas pr√°cticas para CSV

‚úÖ **Detectar encoding autom√°ticamente**:
```python
import chardet

with open('datos.csv', 'rb') as archivo:
    resultado = chardet.detect(archivo.read())
    encoding = resultado['encoding']
    print(f"Encoding detectado: {encoding}")
```

‚úÖ **Usar pandas para lectura robusta**:
```python
import pandas as pd

# Pandas maneja muchos casos autom√°ticamente
df = pd.read_csv(
    'datos.csv',
    encoding='utf-8',
    delimiter=';',
    header=0,  # Fila 0 es el header
    na_values=['', 'NULL', 'N/A']  # Valores que considerar como nulos
)
```

‚úÖ **Validar estructura antes de procesar**:
```python
# Verificar que tiene las columnas esperadas
columnas_esperadas = ['nombre', 'edad', 'ciudad']
if not all(col in df.columns for col in columnas_esperadas):
    raise ValueError("El CSV no tiene la estructura esperada")
```

---

### 1.2 Archivos JSON (JavaScript Object Notation)

#### ¬øQu√© es JSON?

JSON es un formato de texto que representa datos estructurados usando pares clave-valor. Es el formato m√°s usado en APIs web.

**Analog√≠a**: JSON es como un diccionario donde cada cosa tiene una etiqueta. Es m√°s flexible que CSV porque puede tener estructuras anidadas (como cajas dentro de cajas).

**Ejemplo simple**:
```json
{
  "nombre": "Ana",
  "edad": 28,
  "ciudad": "Madrid"
}
```

**Ejemplo anidado**:
```json
{
  "nombre": "Ana",
  "edad": 28,
  "direccion": {
    "calle": "Gran V√≠a 1",
    "ciudad": "Madrid",
    "codigo_postal": "28013"
  },
  "hobbies": ["lectura", "nataci√≥n", "programaci√≥n"]
}
```

#### Tipos de JSON

**1. JSON plano (flat)**
Cada registro es un objeto JSON completo:
```json
{"id": 1, "nombre": "Ana", "edad": 28}
{"id": 2, "nombre": "Carlos", "edad": 35}
```

**2. JSON nested (anidado)**
Contiene objetos dentro de objetos:
```json
{
  "usuario": {
    "id": 1,
    "perfil": {
      "nombre": "Ana",
      "edad": 28
    }
  }
}
```

**3. JSON Lines (JSONL)**
Cada l√≠nea es un JSON v√°lido (muy com√∫n en logs y big data):
```jsonl
{"timestamp": "2025-01-01 10:00:00", "evento": "login", "usuario": "ana"}
{"timestamp": "2025-01-01 10:05:23", "evento": "compra", "usuario": "carlos"}
```

#### Lectura de JSON en Python

**JSON simple**:
```python
import json

with open('datos.json', 'r', encoding='utf-8') as archivo:
    datos = json.load(archivo)

# datos es ahora un diccionario de Python
print(datos['nombre'])  # "Ana"
```

**JSON Lines**:
```python
import json

registros = []
with open('datos.jsonl', 'r', encoding='utf-8') as archivo:
    for linea in archivo:
        registro = json.loads(linea)
        registros.append(registro)
```

**JSON nested con pandas**:
```python
import pandas as pd

# pandas puede aplanar estructuras anidadas
df = pd.read_json('datos.json')

# O normalizar JSON anidado
from pandas import json_normalize
with open('datos_nested.json') as f:
    datos = json.load(f)
df = json_normalize(datos)
```

#### Ventajas de JSON sobre CSV

- ‚úÖ Soporta estructuras anidadas
- ‚úÖ Tipos de datos expl√≠citos (n√∫meros, strings, booleanos, null)
- ‚úÖ No necesita especificar delimitadores
- ‚úÖ Est√°ndar en APIs REST

#### Desaf√≠os con JSON

- ‚ö†Ô∏è M√°s pesado que CSV (m√°s caracteres)
- ‚ö†Ô∏è Estructuras muy anidadas son dif√≠ciles de procesar
- ‚ö†Ô∏è No todos los JSON son v√°lidos (problemas de sintaxis)

---

### 1.3 Archivos Excel (.xlsx, .xls)

#### ¬øQu√© es Excel?

Excel es un formato binario (no texto plano) que puede contener m√∫ltiples hojas (sheets), f√≥rmulas, estilos y gr√°ficos.

**Analog√≠a**: Es como un cuaderno con m√∫ltiples p√°ginas, donde cada p√°gina puede tener tablas, notas y c√°lculos.

#### ¬øPor qu√© Excel en Data Engineering?

Aunque preferimos CSV o JSON, Excel es muy com√∫n porque:
- Los usuarios de negocio lo usan constantemente
- Puede contener m√∫ltiples tablas relacionadas en diferentes hojas
- A veces incluye metadatos √∫tiles

#### Lectura de Excel con Python

```python
import pandas as pd

# Leer la primera hoja
df = pd.read_excel('ventas.xlsx')

# Leer una hoja espec√≠fica
df = pd.read_excel('ventas.xlsx', sheet_name='Enero')

# Leer todas las hojas
todas_las_hojas = pd.read_excel('ventas.xlsx', sheet_name=None)
for nombre_hoja, df in todas_las_hojas.items():
    print(f"Hoja: {nombre_hoja}, Filas: {len(df)}")
```

#### Desaf√≠os con Excel

- ‚ö†Ô∏è Archivos grandes son lentos de leer
- ‚ö†Ô∏è Puede contener f√≥rmulas en lugar de valores
- ‚ö†Ô∏è Formatos de fecha pueden ser problem√°ticos
- ‚ö†Ô∏è Necesita librer√≠a adicional (openpyxl)

#### Buenas pr√°cticas

‚úÖ **Especificar qu√© leer**:
```python
df = pd.read_excel(
    'ventas.xlsx',
    sheet_name='Enero',
    header=0,
    usecols='A:E',  # Solo columnas A a E
    skiprows=2,     # Saltar las 2 primeras filas
    nrows=100       # Leer solo 100 filas
)
```

‚úÖ **Convertir a CSV para procesamiento masivo**:
```python
# Excel es lento, CSV es r√°pido
df = pd.read_excel('ventas.xlsx')
df.to_csv('ventas.csv', index=False)
# Ahora trabaja con el CSV
```

---

## Parte 2: Extracci√≥n desde APIs REST

### 2.1 ¬øQu√© es una API REST?

Una **API** (Application Programming Interface) es un puente que permite que dos programas se comuniquen. **REST** es un estilo de dise√±o de APIs que usa el protocolo HTTP (el mismo que usan los navegadores web).

**Analog√≠a**: Una API REST es como un restaurante:
- T√∫ eres el cliente (tu programa)
- El men√∫ son los endpoints disponibles
- El camarero es la API que lleva tu pedido a la cocina
- La cocina es el servidor que prepara los datos
- El plato que te trae es la respuesta JSON

#### Conceptos fundamentales

**1. Endpoint (URL)**
Es la direcci√≥n donde haces la petici√≥n:
```
https://api.ejemplo.com/v1/usuarios
```

**2. M√©todos HTTP**
- **GET**: Obtener datos (el m√°s usado en extracci√≥n)
- **POST**: Enviar datos nuevos
- **PUT**: Actualizar datos existentes
- **DELETE**: Eliminar datos

**3. Headers (Cabeceras)**
Informaci√≥n adicional de la petici√≥n:
```python
headers = {
    'Authorization': 'Bearer tu_token_aqui',
    'Content-Type': 'application/json',
    'User-Agent': 'MiApp/1.0'
}
```

**4. Query Parameters**
Par√°metros en la URL para filtrar/personalizar:
```
https://api.ejemplo.com/v1/usuarios?edad=25&ciudad=Madrid
```

**5. Response (Respuesta)**
Los datos que devuelve la API, normalmente en JSON:
```json
{
  "status": "success",
  "data": [
    {"id": 1, "nombre": "Ana"},
    {"id": 2, "nombre": "Carlos"}
  ]
}
```

### 2.2 Hacer peticiones con requests

**Instalaci√≥n**:
```bash
pip install requests
```

**GET simple**:
```python
import requests

response = requests.get('https://api.ejemplo.com/v1/usuarios')

# Verificar que fue exitosa (c√≥digo 200)
if response.status_code == 200:
    datos = response.json()  # Convertir JSON a diccionario Python
    print(datos)
else:
    print(f"Error: {response.status_code}")
```

**GET con par√°metros**:
```python
params = {
    'edad': 25,
    'ciudad': 'Madrid',
    'limit': 100
}

response = requests.get(
    'https://api.ejemplo.com/v1/usuarios',
    params=params
)
```

**GET con autenticaci√≥n**:
```python
headers = {
    'Authorization': 'Bearer tu_token_secreto',
    'User-Agent': 'MiApp/1.0'
}

response = requests.get(
    'https://api.ejemplo.com/v1/usuarios',
    headers=headers
)
```

### 2.3 Autenticaci√≥n en APIs

Las APIs necesitan saber qui√©n eres para darte acceso. Los m√©todos m√°s comunes:

**1. API Key (Clave de API)**
```python
headers = {'X-API-Key': 'tu_clave_aqui'}
response = requests.get(url, headers=headers)
```

**2. Bearer Token (Token de acceso)**
```python
headers = {'Authorization': 'Bearer tu_token_aqui'}
response = requests.get(url, headers=headers)
```

**3. Basic Auth (Usuario y contrase√±a)**
```python
from requests.auth import HTTPBasicAuth

response = requests.get(
    url,
    auth=HTTPBasicAuth('usuario', 'contrase√±a')
)
```

**Regla de oro**: ¬°NUNCA hardcodees las credenciales en el c√≥digo! Usa variables de entorno:

```python
import os

API_KEY = os.getenv('MI_API_KEY')
if not API_KEY:
    raise ValueError("Falta la variable de entorno MI_API_KEY")

headers = {'X-API-Key': API_KEY}
```

### 2.4 Rate Limiting (L√≠mites de uso)

Las APIs limitan cu√°ntas peticiones puedes hacer en un periodo de tiempo para evitar sobrecarga.

**Ejemplo t√≠pico**: "100 peticiones por minuto"

Si superas el l√≠mite, recibes un error 429 (Too Many Requests).

**C√≥mo manejarlo**:

```python
import time
import requests

def hacer_peticion_con_rate_limit(url, peticiones_por_segundo=2):
    """Hace peticiones respetando rate limit"""
    tiempo_entre_peticiones = 1.0 / peticiones_por_segundo

    response = requests.get(url)
    time.sleep(tiempo_entre_peticiones)  # Esperar antes de la siguiente

    return response
```

**Leer el rate limit de la respuesta**:
```python
response = requests.get(url)

# Muchas APIs incluyen headers informativos
limite_restante = response.headers.get('X-RateLimit-Remaining')
tiempo_reset = response.headers.get('X-RateLimit-Reset')

print(f"Peticiones restantes: {limite_restante}")
```

### 2.5 Paginaci√≥n

Las APIs no devuelven todos los datos de golpe (imagina millones de registros). Usan **paginaci√≥n**.

**Tipos de paginaci√≥n**:

**1. Offset/Limit (lo m√°s com√∫n)**
```python
# P√°gina 1: registros 0-99
response = requests.get('https://api.ejemplo.com/v1/usuarios?offset=0&limit=100')

# P√°gina 2: registros 100-199
response = requests.get('https://api.ejemplo.com/v1/usuarios?offset=100&limit=100')
```

**2. Cursor-based**
```python
# Primera p√°gina
response = requests.get('https://api.ejemplo.com/v1/usuarios?limit=100')
datos = response.json()
cursor_siguiente = datos['next_cursor']

# Segunda p√°gina
response = requests.get(f'https://api.ejemplo.com/v1/usuarios?cursor={cursor_siguiente}&limit=100')
```

**3. Page/Per Page**
```python
response = requests.get('https://api.ejemplo.com/v1/usuarios?page=1&per_page=100')
```

**Patr√≥n completo de paginaci√≥n**:
```python
def extraer_todos_los_datos(url_base):
    """Extrae todos los datos paginados de una API"""
    todos_los_datos = []
    offset = 0
    limit = 100

    while True:
        response = requests.get(
            f"{url_base}?offset={offset}&limit={limit}"
        )
        datos = response.json()

        if not datos or len(datos) == 0:
            break  # No hay m√°s datos

        todos_los_datos.extend(datos)
        offset += limit

        time.sleep(0.5)  # Rate limiting

    return todos_los_datos
```

### 2.6 Manejo de errores y reintentos

Las APIs pueden fallar temporalmente. Necesitas reintentos inteligentes.

**C√≥digos HTTP comunes**:
- `200`: OK (√©xito)
- `400`: Bad Request (tu petici√≥n est√° mal)
- `401`: Unauthorized (falta autenticaci√≥n)
- `403`: Forbidden (no tienes permiso)
- `404`: Not Found (el endpoint no existe)
- `429`: Too Many Requests (rate limit excedido)
- `500`: Internal Server Error (error del servidor)
- `503`: Service Unavailable (servicio ca√≠do temporalmente)

**Estrategia de reintentos**:

```python
import requests
import time

def hacer_peticion_con_reintentos(url, max_reintentos=3):
    """Reintenta peticiones con backoff exponencial"""

    for intento in range(max_reintentos):
        try:
            response = requests.get(url, timeout=30)

            # Si es exitoso, devolver
            if response.status_code == 200:
                return response.json()

            # Si es error del cliente (400-499), no reintentar
            if 400 <= response.status_code < 500:
                raise ValueError(f"Error del cliente: {response.status_code}")

            # Si es error del servidor (500-599), reintentar
            print(f"Intento {intento + 1} fall√≥: {response.status_code}")

        except requests.exceptions.Timeout:
            print(f"Timeout en intento {intento + 1}")

        except requests.exceptions.ConnectionError:
            print(f"Error de conexi√≥n en intento {intento + 1}")

        # Backoff exponencial: 1s, 2s, 4s, 8s...
        tiempo_espera = 2 ** intento
        print(f"Esperando {tiempo_espera}s antes del siguiente intento")
        time.sleep(tiempo_espera)

    raise Exception(f"Fall√≥ despu√©s de {max_reintentos} intentos")
```

---

## Parte 3: Web Scraping √âtico

### 3.1 ¬øQu√© es Web Scraping?

**Web scraping** es extraer datos de p√°ginas web que no ofrecen una API. Es como copiar informaci√≥n de un libro p√°gina por p√°gina.

**Analog√≠a**: Imagina que necesitas informaci√≥n de un cat√°logo en papel. El scraping es como leer cada p√°gina, tomar notas y organizarlas en una base de datos.

**¬øCu√°ndo usar scraping?**
- ‚úÖ No existe una API disponible
- ‚úÖ La API est√° limitada o incompleta
- ‚úÖ Necesitas datos p√∫blicos de sitios web

### 3.2 √âtica y legalidad del scraping

**‚ö†Ô∏è IMPORTANTE**: No todo scraping es legal o √©tico.

**Reglas de oro**:

1. **Respeta robots.txt**
   - Es un archivo que indica qu√© se puede y no se puede scrapear
   - Ubicaci√≥n: `https://ejemplo.com/robots.txt`

2. **Identifica tu User-Agent**
   - No te hagas pasar por un navegador normal
   - Incluye tu email o web para contacto

3. **Rate limiting**
   - No hagas peticiones tan r√°pido que sobrecargues el servidor
   - 1-2 segundos entre peticiones es razonable

4. **T√©rminos de servicio**
   - Lee los t√©rminos del sitio web
   - Algunos proh√≠ben expl√≠citamente el scraping

5. **Datos personales**
   - No scrapes datos personales sensibles
   - Cumple con GDPR y leyes de privacidad

**Verificar robots.txt**:
```python
import requests

def verificar_robots_txt(url_base):
    """Verifica si el scraping est√° permitido"""
    robots_url = f"{url_base}/robots.txt"
    response = requests.get(robots_url)

    if response.status_code == 200:
        print("Contenido de robots.txt:")
        print(response.text)
    else:
        print("No hay robots.txt (scraping probablemente permitido)")
```

### 3.3 Beautiful Soup Basics

**Beautiful Soup** es una librer√≠a Python para parsear HTML y extraer datos.

**Instalaci√≥n**:
```bash
pip install beautifulsoup4 requests
```

**Flujo b√°sico**:
1. Hacer petici√≥n HTTP para obtener HTML
2. Parsear HTML con Beautiful Soup
3. Buscar elementos espec√≠ficos (tablas, divs, etc.)
4. Extraer texto o atributos
5. Guardar datos estructurados

**Ejemplo simple**:
```python
import requests
from bs4 import BeautifulSoup

# 1. Obtener HTML
url = 'https://ejemplo.com/productos'
headers = {
    'User-Agent': 'MiScraper/1.0 (contacto@ejemplo.com)'
}
response = requests.get(url, headers=headers)

# 2. Parsear HTML
soup = BeautifulSoup(response.content, 'html.parser')

# 3. Buscar elementos
productos = soup.find_all('div', class_='producto')

# 4. Extraer datos
for producto in productos:
    nombre = producto.find('h2', class_='nombre').text.strip()
    precio = producto.find('span', class_='precio').text.strip()
    print(f"{nombre}: {precio}")
```

**Selectores comunes**:
```python
# Por etiqueta
soup.find('h1')
soup.find_all('p')

# Por clase CSS
soup.find('div', class_='contenido')

# Por ID
soup.find('div', id='principal')

# Por atributo
soup.find('a', href='https://ejemplo.com')

# Selector CSS
soup.select('div.producto > h2')
```

**Extraer texto y atributos**:
```python
elemento = soup.find('a', class_='link')

# Texto
texto = elemento.text.strip()

# Atributo
url = elemento['href']
titulo = elemento.get('title', 'Sin t√≠tulo')
```

### 3.4 Scraping de tablas HTML

Las tablas son la estructura m√°s com√∫n para datos estructurados en HTML.

```python
import pandas as pd
from bs4 import BeautifulSoup
import requests

url = 'https://ejemplo.com/tabla'
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

# Buscar tabla
tabla = soup.find('table', class_='datos')

# Opci√≥n 1: Manual
filas = tabla.find_all('tr')
datos = []
for fila in filas[1:]:  # Saltar header
    celdas = fila.find_all('td')
    datos.append([celda.text.strip() for celda in celdas])

# Opci√≥n 2: Con pandas (m√°s f√°cil)
df = pd.read_html(str(tabla))[0]
print(df)
```

### 3.5 Manejo de contenido din√°mico (JavaScript)

Algunos sitios usan JavaScript para cargar contenido. Beautiful Soup solo ve el HTML inicial.

**Soluciones**:

1. **Buscar API oculta** (recomendado)
   - Usa las DevTools del navegador (F12)
   - Pesta√±a Network ‚Üí XHR/Fetch
   - Busca peticiones JSON que carguen los datos
   - Usa esa API directamente

2. **Selenium** (√∫ltimo recurso)
   - Controla un navegador real
   - Lento y pesado
   ```python
   from selenium import webdriver

   driver = webdriver.Chrome()
   driver.get('https://ejemplo.com')
   time.sleep(3)  # Esperar carga de JS
   html = driver.page_source
   soup = BeautifulSoup(html, 'html.parser')
   ```

---

## Parte 4: Logging y Monitoreo de Extracci√≥n

### 4.1 ¬øPor qu√© logging en extracci√≥n?

Cuando extraes datos de m√∫ltiples fuentes, necesitas:
- Saber si la extracci√≥n fue exitosa
- Registrar errores para debug
- Medir tiempos de ejecuci√≥n
- Auditar qu√© datos se extrajeron

**Ejemplo de log completo**:
```python
import logging
from datetime import datetime

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/extraccion.log'),
        logging.StreamHandler()  # Tambi√©n en consola
    ]
)

logger = logging.getLogger(__name__)

def extraer_datos_api(url):
    """Extrae datos con logging completo"""
    inicio = datetime.now()
    logger.info(f"Iniciando extracci√≥n desde {url}")

    try:
        response = requests.get(url, timeout=30)
        datos = response.json()

        duracion = (datetime.now() - inicio).total_seconds()
        logger.info(f"Extracci√≥n exitosa: {len(datos)} registros en {duracion}s")

        return datos

    except Exception as e:
        logger.error(f"Error en extracci√≥n: {str(e)}")
        raise
```

### 4.2 M√©tricas importantes

**Registra siempre**:
- Timestamp de inicio y fin
- Fuente de datos (URL, archivo, etc.)
- Cantidad de registros extra√≠dos
- Tiempo de ejecuci√≥n
- Errores y excepciones
- Reintentos realizados

---

## Parte 5: Errores Comunes y C√≥mo Evitarlos

### Error 1: No validar encoding en CSV

**Problema**: Intentas leer un CSV con UTF-8 pero est√° en Latin-1.

**S√≠ntoma**: UnicodeDecodeError o caracteres raros (ÔøΩ).

**Soluci√≥n**:
```python
import chardet

# Detectar encoding autom√°ticamente
with open('datos.csv', 'rb') as f:
    resultado = chardet.detect(f.read())
    encoding = resultado['encoding']

# Usar el encoding detectado
df = pd.read_csv('datos.csv', encoding=encoding)
```

### Error 2: No manejar rate limits en APIs

**Problema**: Haces 1000 peticiones por segundo, la API te bloquea.

**S√≠ntoma**: Error 429 (Too Many Requests).

**Soluci√≥n**:
```python
import time

for i in range(100):
    response = requests.get(url)
    time.sleep(1)  # 1 segundo entre peticiones
```

### Error 3: No identificarte en web scraping

**Problema**: Usas el User-Agent por defecto de requests.

**S√≠ntoma**: El sitio te bloquea o devuelve contenido diferente.

**Soluci√≥n**:
```python
headers = {
    'User-Agent': 'MiScraper/1.0 (contacto@ejemplo.com)'
}
response = requests.get(url, headers=headers)
```

### Error 4: No manejar reintentos en APIs inestables

**Problema**: La API falla temporalmente y tu script explota.

**S√≠ntoma**: ConnectionError o Timeout sin reintentos.

**Soluci√≥n**: Usar la funci√≥n `hacer_peticion_con_reintentos()` mostrada anteriormente.

### Error 5: Hardcodear credenciales

**Problema**: Guardas API keys en el c√≥digo:
```python
API_KEY = "sk_live_1234567890abcdef"  # ‚ùå NUNCA HAGAS ESTO
```

**S√≠ntoma**: Credenciales expuestas en GitHub, riesgo de seguridad.

**Soluci√≥n**:
```python
import os
API_KEY = os.getenv('MI_API_KEY')  # ‚úÖ Usar variables de entorno
```

---

## Checklist de Aprendizaje

Al terminar este tema, deber√≠as poder:

- [ ] Leer archivos CSV con diferentes encodings y delimitadores
- [ ] Procesar JSON simple, nested y JSON Lines
- [ ] Leer archivos Excel con m√∫ltiples hojas
- [ ] Hacer peticiones GET a APIs REST
- [ ] Autenticarte en APIs con API keys y tokens
- [ ] Implementar paginaci√≥n para obtener todos los datos
- [ ] Manejar rate limits y reintentos
- [ ] Verificar robots.txt antes de hacer scraping
- [ ] Extraer datos de HTML con Beautiful Soup
- [ ] Scrapear tablas HTML
- [ ] Implementar logging en tus scripts de extracci√≥n
- [ ] Manejar errores de encoding, timeouts y conexi√≥n

---

## Recursos Adicionales

### Librer√≠as Python
- **requests**: https://requests.readthedocs.io/
- **pandas**: https://pandas.pydata.org/docs/
- **beautifulsoup4**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **chardet**: https://chardet.readthedocs.io/

### APIs p√∫blicas para practicar
- **JSONPlaceholder**: https://jsonplaceholder.typicode.com/ (API de prueba)
- **OpenWeatherMap**: https://openweathermap.org/api (clima)
- **REST Countries**: https://restcountries.com/ (datos de pa√≠ses)

### Scraping √©tico
- **robots.txt checker**: https://en.ryte.com/free-tools/robots-txt/
- **T√©rminos de servicio**: Lee siempre antes de scrapear

### Lecturas recomendadas
- "Web Scraping with Python" - Ryan Mitchell
- Documentaci√≥n oficial de requests y Beautiful Soup

---

**¬°Felicidades!** Has completado la teor√≠a de extracci√≥n de datos. Ahora est√°s listo para ver ejemplos pr√°cticos y hacer ejercicios.

**Pr√≥ximo paso**: `02-EJEMPLOS.md` donde ver√°s 5 casos reales de extracci√≥n de datos.
---

## üß≠ Navegaci√≥n

‚¨ÖÔ∏è **Anterior**: [Conceptos ETL - Proyecto Pr√°ctico](../tema-1-conceptos-etl/04-proyecto-practico/README.md) | ‚û°Ô∏è **Siguiente**: [02 Ejemplos](02-EJEMPLOS.md)
