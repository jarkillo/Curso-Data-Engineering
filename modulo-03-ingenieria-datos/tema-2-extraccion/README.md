# Tema 2: Extracci√≥n de Datos

**M√≥dulo**: 3 - Ingenier√≠a de Datos Core
**Duraci√≥n estimada**: 1-2 semanas
**Nivel**: Intermedio
**Prerrequisitos**: Tema 1 (Conceptos ETL)

---

## üéØ Objetivos de Aprendizaje

Al completar este tema, ser√°s capaz de:

1. **Extraer datos de archivos** (CSV, JSON, Excel) manejando encodings y estructuras complejas
2. **Consumir APIs REST** con autenticaci√≥n, paginaci√≥n y manejo robusto de errores
3. **Realizar web scraping √©tico** respetando robots.txt y rate limits
4. **Implementar reintentos autom√°ticos** con backoff exponencial
5. **Consolidar datos de m√∫ltiples fuentes** en pipelines robustos
6. **Aplicar logging y monitoreo** en procesos de extracci√≥n

---

## üìö Contenido del Tema

### üìñ Material Te√≥rico

#### [01-TEORIA.md](01-TEORIA.md) (~6,500 palabras, 45-60 min)
**Contenido**:
- **Parte 1: Archivos**
  - CSV (encoding, delimitadores, headers)
  - JSON (plano, nested, JSON Lines)
  - Excel (m√∫ltiples sheets)
- **Parte 2: APIs REST**
  - Conceptos fundamentales (endpoints, m√©todos HTTP, headers)
  - Autenticaci√≥n (API keys, Bearer tokens, Basic Auth)
  - Rate limiting y paginaci√≥n
  - Manejo de errores y reintentos
- **Parte 3: Web Scraping**
  - √âtica del scraping (robots.txt, User-Agent)
  - Beautiful Soup basics
  - Extracci√≥n de tablas HTML
  - Contenido din√°mico vs est√°tico
- **Parte 4: Logging y Monitoreo**
- **Parte 5: 5 Errores Comunes y C√≥mo Evitarlos**

**Caracter√≠sticas**:
- ‚úÖ Explicaciones desde cero (sin asumir conocimientos previos)
- ‚úÖ Analog√≠as del mundo real para conceptos t√©cnicos
- ‚úÖ Buenas pr√°cticas de Data Engineering
- ‚úÖ √ânfasis en seguridad y √©tica

---

#### [02-EJEMPLOS.md](02-EJEMPLOS.md) (5 ejemplos, 60-90 min)
**Ejemplos trabajados paso a paso**:

1. **CSV con Encoding Problem√°tico** (‚≠ê B√°sico)
   - Detectar encoding autom√°ticamente con `chardet`
   - Leer archivos Latin-1, UTF-8-BOM
   - Funci√≥n reutilizable

2. **JSON Nested con M√∫ltiples Niveles** (‚≠ê‚≠ê Intermedio)
   - Aplanar estructuras complejas con `json_normalize()`
   - Extraer listas anidadas en tablas separadas
   - Calcular campos derivados

3. **API Paginada con Reintentos Autom√°ticos** (‚≠ê‚≠ê Intermedio)
   - Implementar reintentos con backoff exponencial
   - Paginaci√≥n offset-based completa
   - Rate limiting y logging

4. **Scraping B√°sico con Beautiful Soup** (‚≠ê B√°sico)
   - Parsear HTML y extraer elementos
   - Scraping de tablas
   - Limpiar y estructurar datos

5. **Extracci√≥n Multi-Fuente** (‚≠ê‚≠ê‚≠ê Avanzado)
   - Consolidar CSV + API + Web scraping
   - Merge de m√∫ltiples DataFrames
   - Calcular KPIs y generar reportes

**Todas las soluciones son ejecutables y testeadas**.

---

#### [03-EJERCICIOS.md](03-EJERCICIOS.md) (15 ejercicios, 4-8 horas)
**Ejercicios graduados con soluciones completas**:

**B√°sicos (1-5)** - ‚≠ê
- Detecci√≥n autom√°tica de encoding
- Lectura de JSON Lines
- M√∫ltiples hojas de Excel
- Limpieza de CSV con valores nulos
- Validaci√≥n de estructura

**Intermedios (6-10)** - ‚≠ê‚≠ê
- API con autenticaci√≥n Bearer
- Paginaci√≥n offset-based
- Manejo de rate limit 429
- Scraping de tablas HTML
- Verificar robots.txt

**Avanzados (11-15)** - ‚≠ê‚≠ê‚≠ê
- Pipeline multi-fuente completo
- Extracci√≥n con logging robusto
- API oculta vs scraping
- Sistema de cach√©
- Pipeline orquestado end-to-end

**Incluye**: Tabla de autoevaluaci√≥n y criterios de √©xito

---

### üíª Proyecto Pr√°ctico (4-6 d√≠as)

#### [04-proyecto-practico/](04-proyecto-practico/) - Sistema de Extracci√≥n Multi-Fuente
**Contexto**: Construir un sistema profesional de extracci√≥n de datos para an√°lisis de mercado.

**Arquitectura** (TDD estricto):
```
src/
‚îú‚îÄ‚îÄ extractor_archivos.py    (6 funciones - CSV, JSON, Excel)
‚îú‚îÄ‚îÄ extractor_apis.py         (5 funciones - REST, auth, paginaci√≥n)
‚îú‚îÄ‚îÄ extractor_web.py          (4 funciones - Scraping √©tico)
‚îú‚îÄ‚îÄ gestor_extracciones.py    (4 funciones - Orquestaci√≥n, logging)
‚îî‚îÄ‚îÄ validadores.py            (5 funciones - Validaci√≥n de datos)

tests/
‚îú‚îÄ‚îÄ test_extractor_archivos.py    (~15 tests)
‚îú‚îÄ‚îÄ test_extractor_apis.py        (~15 tests)
‚îú‚îÄ‚îÄ test_extractor_web.py         (~12 tests)
‚îú‚îÄ‚îÄ test_gestor_extracciones.py   (~12 tests)
‚îî‚îÄ‚îÄ test_validadores.py           (~15 tests)
```

**Caracter√≠sticas**:
- ‚úÖ 70+ tests unitarios (TDD)
- ‚úÖ >85% cobertura de c√≥digo
- ‚úÖ Logging completo
- ‚úÖ Manejo robusto de errores
- ‚úÖ Datos de ejemplo incluidos
- ‚úÖ Configuraci√≥n completa (pytest, requirements)

---

## üó∫Ô∏è Ruta de Aprendizaje Recomendada

### Opci√≥n A: Secuencial Completo (Recomendado)
**Duraci√≥n**: 1-2 semanas

```
D√≠a 1-2: Leer 01-TEORIA.md + 02-EJEMPLOS.md
         Ejecutar todos los ejemplos

D√≠a 3:   Hacer ejercicios 1-5 (b√°sicos)
         Revisar soluciones

D√≠a 4:   Hacer ejercicios 6-10 (intermedios)
         Revisar soluciones

D√≠a 5:   Hacer ejercicios 11-15 (avanzados)
         Revisar soluciones

D√≠a 6-10: Proyecto pr√°ctico TDD
          Implementar los 5 m√≥dulos
          Escribir tests (>85% cobertura)
```

### Opci√≥n B: Solo Teor√≠a y Ejemplos (R√°pido)
**Duraci√≥n**: 2-3 d√≠as

```
D√≠a 1: Leer 01-TEORIA.md (45-60 min)
       Ejecutar 5 ejemplos de 02-EJEMPLOS.md (2-3 horas)

D√≠a 2: Hacer ejercicios seleccionados: 1, 3, 6, 9, 11, 15 (4 horas)

D√≠a 3: Revisar c√≥digo del proyecto pr√°ctico (2 horas)
```

### Opci√≥n C: Solo Proyecto (Experiencia Previa)
**Duraci√≥n**: 4-6 d√≠as

Si ya tienes experiencia con extracci√≥n de datos:
```
D√≠a 1: Leer 01-TEORIA.md (enfoque en buenas pr√°cticas)
       Revisar 02-EJEMPLOS.md como referencia

D√≠a 2-6: Implementar proyecto pr√°ctico completo con TDD
```

---

## üõ†Ô∏è Requisitos T√©cnicos

### Librer√≠as Necesarias
```bash
pip install requests beautifulsoup4 pandas openpyxl chardet pytest pytest-cov
```

### Versiones Recomendadas
- Python: 3.11+
- requests: >=2.32.4
- beautifulsoup4: >=4.12.0
- pandas: >=2.1.0
- openpyxl: >=3.1.0
- chardet: >=5.2.0

### Herramientas Opcionales
- Postman/Insomnia (para probar APIs)
- DB Browser for SQLite (para inspeccionar datos)
- Jupyter Notebook (para experimentar)

---

## üìä Evaluaci√≥n y Calidad

### Revisi√≥n Pedag√≥gica
- **Calificaci√≥n**: 9.5/10 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **Progresi√≥n**: Sin saltos conceptuales
- **Claridad**: Analog√≠as efectivas
- **Aplicabilidad**: Casos reales de Data Engineering
- Ver [REVISION_PEDAGOGICA.md](REVISION_PEDAGOGICA.md) para detalles

### Criterios de √âxito

Has completado exitosamente este tema si:
- ‚úÖ Puedes extraer datos de CSV con cualquier encoding
- ‚úÖ Puedes consumir APIs REST con paginaci√≥n y reintentos
- ‚úÖ Puedes hacer scraping respetando √©tica y rate limits
- ‚úÖ Puedes consolidar datos de m√∫ltiples fuentes
- ‚úÖ Implementas logging en tus extracciones
- ‚úÖ Manejas errores de forma robusta

**Validaci√≥n pr√°ctica**: Completa el ejercicio 15 (Pipeline completo) exitosamente.

---

## üîó Recursos Adicionales

### Documentaci√≥n Oficial
- [requests](https://requests.readthedocs.io/) - HTTP library
- [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - Web scraping
- [pandas](https://pandas.pydata.org/docs/) - Data manipulation
- [chardet](https://chardet.readthedocs.io/) - Encoding detection

### APIs P√∫blicas para Practicar
- [JSONPlaceholder](https://jsonplaceholder.typicode.com/) - API de prueba gratuita
- [OpenWeatherMap](https://openweathermap.org/api) - API de clima
- [REST Countries](https://restcountries.com/) - Datos de pa√≠ses

### √âtica de Web Scraping
- [robots.txt checker](https://en.ryte.com/free-tools/robots-txt/)
- [Web Scraping Best Practices](https://www.scrapehero.com/web-scraping-best-practices/)

### Lecturas Complementarias
- "Web Scraping with Python" - Ryan Mitchell
- "RESTful Web APIs" - Leonard Richardson

---

## üí° Tips para el √âxito

### Durante el Estudio
1. **Ejecuta todo el c√≥digo**: No solo leas, prueba cada ejemplo
2. **Modifica los ejemplos**: Cambia par√°metros, a√±ade funcionalidad
3. **Usa debugging**: Pon breakpoints, inspecciona variables
4. **Toma notas**: Documenta patrones que encuentres √∫tiles

### Durante los Ejercicios
1. **Lee el contexto**: Entender el problema es clave
2. **Intenta primero**: Dedica 15-30 min antes de ver la soluci√≥n
3. **Compara soluciones**: Aprende de las diferencias con tu c√≥digo
4. **Refactoriza**: Mejora tu c√≥digo despu√©s de ver la soluci√≥n

### Durante el Proyecto
1. **TDD estricto**: Test primero, c√≥digo despu√©s
2. **Commits peque√±os**: Commitea cada funci√≥n que completes
3. **Refactoriza**: Limpia c√≥digo despu√©s de que los tests pasen
4. **Documenta**: Escribe docstrings para todas las funciones

---

## üö® Troubleshooting

### Problema: UnicodeDecodeError al leer CSV
**Soluci√≥n**: Usa `chardet` para detectar el encoding autom√°ticamente
```python
import chardet
with open('file.csv', 'rb') as f:
    encoding = chardet.detect(f.read())['encoding']
df = pd.read_csv('file.csv', encoding=encoding)
```

### Problema: Error 429 (Too Many Requests) en API
**Soluci√≥n**: Implementa rate limiting con `time.sleep()`
```python
import time
response = requests.get(url)
time.sleep(1)  # 1 segundo entre peticiones
```

### Problema: Beautiful Soup no encuentra elementos
**Soluci√≥n**: Verifica el HTML real (puede ser diferente al esperado)
```python
print(soup.prettify())  # Ver estructura HTML
```

### Problema: Tests fallan en proyecto
**Soluci√≥n**: Verifica que est√°s en el directorio correcto
```bash
cd 04-proyecto-practico
pytest -v
```

---

## üìû Soporte

### ¬øDudas sobre conceptos?
- Revisa la secci√≥n correspondiente en 01-TEORIA.md
- Busca en "Errores Comunes" (Parte 5)
- Consulta las soluciones de ejercicios similares

### ¬øProblemas t√©cnicos?
- Verifica versiones de librer√≠as
- Consulta la secci√≥n Troubleshooting arriba
- Revisa logs de error completos

### ¬øNecesitas m√°s pr√°ctica?
- Repite ejercicios modificando el contexto
- Busca APIs p√∫blicas adicionales para practicar
- Implementa variaciones del proyecto pr√°ctico

---

## ‚úÖ Checklist de Completitud

- [ ] Le√≠ 01-TEORIA.md completamente
- [ ] Ejecut√© los 5 ejemplos de 02-EJEMPLOS.md
- [ ] Complet√© ejercicios b√°sicos (1-5)
- [ ] Complet√© ejercicios intermedios (6-10)
- [ ] Complet√© ejercicios avanzados (11-15)
- [ ] Implement√© proyecto pr√°ctico con TDD
- [ ] Tests del proyecto >85% cobertura
- [ ] Todos los tests pasan
- [ ] C√≥digo sin errores de linting
- [ ] Document√© funciones principales

**Si completaste 9+ items**: ¬°Felicidades! Dominas extracci√≥n de datos üéâ

---

## üîú Pr√≥ximos Pasos

Despu√©s de completar este tema:

1. **Tema 3: Transformaci√≥n con Pandas** (modulo-03-ingenieria-datos/tema-3-transformacion/)
   - DataFrames avanzados
   - Operaciones: filter, map, apply, groupby
   - Merge, join, concat
   - Limpieza y preparaci√≥n de datos

2. **Tema 4: Calidad de Datos** (modulo-03-ingenieria-datos/tema-4-calidad/)
   - Validaci√≥n de esquemas
   - Detecci√≥n de duplicados y outliers
   - Data profiling
   - Frameworks de calidad

3. **Proyecto Integrador del M√≥dulo 3**
   - Pipeline ETL completo end-to-end
   - Integra extracci√≥n, transformaci√≥n y calidad
   - Arquitectura Bronze/Silver/Gold

---

**¬°√âxito en tu aprendizaje de extracci√≥n de datos!** üöÄüìä

---

**√öltima actualizaci√≥n**: 2025-10-30
**Versi√≥n**: 1.0.0
**Issue Linear**: [JAR-265](https://linear.app/jarko/issue/JAR-265)
**Calificaci√≥n Pedag√≥gica**: 9.5/10 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
