# Tema 1: Conceptos de ETL/ELT - Fundamentos de Pipelines de Datos

**Tiempo de lectura**: 30-45 minutos
**Nivel**: Intermedio
**Prerrequisitos**: MÃ³dulo 1 (Python bÃ¡sico) y MÃ³dulo 2 (SQL bÃ¡sico)

---

## IntroducciÃ³n: Â¿Por quÃ© importan los Pipelines de Datos?

Imagina que trabajas en una empresa de e-commerce como Amazon. Cada segundo:
- Miles de usuarios navegan por productos
- Cientos realizan compras
- Los vendedores actualizan precios e inventarios
- Los sistemas de recomendaciÃ³n generan sugerencias

Â¿CÃ³mo conviertes todo ese caos de datos en **informaciÃ³n Ãºtil** para tomar decisiones? La respuesta: **pipelines de datos**.

Un **pipeline de datos** es como una **lÃ­nea de producciÃ³n en una fÃ¡brica**:
- **Entrada (materias primas)**: Datos crudos de diferentes fuentes
- **Proceso (transformaciÃ³n)**: Limpieza, validaciÃ³n, combinaciÃ³n
- **Salida (producto final)**: Datos listos para anÃ¡lisis o aplicaciones

En este tema aprenderÃ¡s los conceptos fundamentales que todo Data Engineer debe dominar para construir pipelines robustos y escalables.

---

## ğŸ¯ Objetivos de Aprendizaje

Al completar este tema, serÃ¡s capaz de:

1. Explicar quÃ© es un pipeline de datos y por quÃ© es fundamental
2. Diferenciar entre ETL y ELT y cuÃ¡ndo usar cada uno
3. Comprender arquitecturas de pipelines (batch vs streaming)
4. DiseÃ±ar pipelines idempotentes que se puedan reprocesar
5. Identificar componentes clave de un pipeline de producciÃ³n

---

## 1. Â¿QuÃ© es un Pipeline de Datos?

### DefiniciÃ³n Simple

Un **pipeline de datos** es una serie de pasos automatizados que mueven y transforman datos desde una o mÃ¡s fuentes hasta un destino final.

### AnalogÃ­a: El Ciclo del Agua

Piensa en el ciclo del agua:
1. **EvaporaciÃ³n (Extract)**: El agua se extrae de ocÃ©anos, rÃ­os, lagos
2. **FormaciÃ³n de nubes (Transform)**: El vapor se condensa y se purifica
3. **PrecipitaciÃ³n (Load)**: El agua cae limpia y lista para consumir

Un pipeline de datos funciona igual:
1. **Extract**: Extraer datos de mÃºltiples fuentes
2. **Transform**: Limpiar, validar, combinar, enriquecer
3. **Load**: Cargar en el destino (base de datos, data warehouse, data lake)

### En el Mundo Real

**Ejemplo**: Pipeline de anÃ¡lisis de ventas
```
1. Extract: Leer datos de ventas del sistema de punto de venta (POS)
2. Transform:
   - Convertir monedas a una sola (USD)
   - Calcular totales por producto
   - Identificar productos mÃ¡s vendidos
3. Load: Guardar resultados en PostgreSQL para dashboards
```

### Por QuÃ© es Importante en Data Engineering

Como Data Engineer, tu trabajo NO es solo escribir cÃ³digo que funcione una vez. Tu trabajo es:
- âœ… Automatizar procesos que se ejecutan **miles de veces**
- âœ… Manejar **millones de registros** sin fallar
- âœ… Garantizar que los datos lleguen **a tiempo y correctos**
- âœ… Recuperarse de **errores sin intervenciÃ³n manual**

Un pipeline bien diseÃ±ado es la diferencia entre:
- âŒ Trabajar 12 horas corrigiendo errores manualmente
- âœ… Tomar cafÃ© mientras el sistema se repara solo

---

## 2. ETL vs ELT: La Gran Diferencia

### ETL (Extract, Transform, Load)

**DefiniciÃ³n**: Transformar los datos **antes** de cargarlos en el destino.

**AnalogÃ­a**: Restaurante con cocina visible

Imagina un restaurante donde:
1. **Extract**: El chef compra ingredientes crudos del mercado
2. **Transform**: Cocina, mezcla, sazona en la cocina (antes de servir)
3. **Load**: Sirve el plato **ya preparado** al cliente

**Flujo**:
```
Fuente â†’ Extract â†’ Transform â†’ Load â†’ Destino
         (datos)   (proceso)   (ya listo)
```

**Ejemplo Real**:
```
1. Extract: Leer CSV de ventas (5GB)
2. Transform:
   - Filtrar solo ventas del Ãºltimo mes
   - Calcular total_con_impuesto
   - Agrupar por producto
3. Load: Insertar 100,000 filas resumidas en PostgreSQL
```

**Ventajas de ETL**:
- âœ… Cargamos **menos datos** (solo lo necesario)
- âœ… El destino recibe datos **ya procesados**
- âœ… Ãštil cuando el destino tiene **capacidad limitada**

**Desventajas de ETL**:
- âŒ Necesitas **servidor potente para transformar**
- âŒ Si quieres cambiar la transformaciÃ³n, debes **reprocesar desde cero**
- âŒ MÃ¡s lento si tienes muchas transformaciones complejas

---

### ELT (Extract, Load, Transform)

**DefiniciÃ³n**: Cargar los datos **primero** y transformar **despuÃ©s** en el destino.

**AnalogÃ­a**: Buffet de comida

Imagina un buffet donde:
1. **Extract**: Traer ingredientes del mercado
2. **Load**: Poner TODO en la mesa (sin cocinar)
3. **Transform**: El cliente elige y mezcla lo que quiere

**Flujo**:
```
Fuente â†’ Extract â†’ Load â†’ Transform â†’ Resultado
         (datos)   (todo)   (en destino)
```

**Ejemplo Real**:
```
1. Extract: Leer CSV de ventas (5GB)
2. Load: Insertar **todas** las 10 millones de filas en Snowflake
3. Transform: Ejecutar SQL dentro de Snowflake para calcular reportes
```

**Ventajas de ELT**:
- âœ… Aprovechas la **potencia del data warehouse** (Snowflake, BigQuery)
- âœ… Puedes **re-transformar sin extraer de nuevo**
- âœ… MÃ¡s **flexible**: cambias la transformaciÃ³n sin tocar la extracciÃ³n
- âœ… **MÃ¡s rÃ¡pido** con grandes volÃºmenes (el DWH tiene GPUs, clusters)

**Desventajas de ELT**:
- âŒ Cargas **todos los datos** (incluso los que no usarÃ¡s)
- âŒ Necesitas un destino **potente** (Snowflake, BigQuery, Redshift)
- âŒ Costos de almacenamiento pueden ser **altos**

---

### Â¿CuÃ¡ndo Usar ETL vs ELT?

| SituaciÃ³n                                      | RecomendaciÃ³n | Por QuÃ©                                |
| ---------------------------------------------- | ------------- | -------------------------------------- |
| Tienes **Snowflake/BigQuery/Redshift**         | **ELT**       | Aprovecha su poder de cÃ³mputo          |
| Destino es **PostgreSQL/MySQL pequeÃ±o**        | **ETL**       | No lo sobrecargues con datos crudos    |
| Volumen < 1GB diario                           | **ETL**       | No necesitas infraestructura compleja  |
| Volumen > 100GB diario                         | **ELT**       | ETL serÃ­a muy lento                    |
| Necesitas **flexibilidad** para re-transformar | **ELT**       | No dependes de re-extraer              |
| Tienes **servidor potente** para transformar   | **ETL**       | Usa lo que tienes                      |
| Datos altamente **sensibles** (GDPR)           | **ETL**       | Filtra datos sensibles antes de cargar |

---

### Ejemplo Comparativo

**Escenario**: Analizar ventas de una tienda online

**Con ETL**:
```python
# 1. Extract
ventas = leer_csv("ventas_2025.csv")  # 5GB, 10M filas

# 2. Transform (antes de cargar)
ventas_filtradas = ventas[ventas['fecha'] >= '2025-10-01']  # Solo octubre
ventas_con_impuesto = calcular_impuesto(ventas_filtradas)
resumen = agrupar_por_producto(ventas_con_impuesto)

# 3. Load (solo 100K filas resumidas)
guardar_en_postgres(resumen)  # 100K filas, 50MB
```

**Con ELT**:
```python
# 1. Extract
ventas = leer_csv("ventas_2025.csv")  # 5GB, 10M filas

# 2. Load (TODO)
guardar_en_snowflake(ventas)  # 10M filas, 5GB

# 3. Transform (dentro de Snowflake con SQL)
ejecutar_sql("""
    SELECT
        producto,
        SUM(precio * 1.21) AS total_con_impuesto
    FROM ventas
    WHERE fecha >= '2025-10-01'
    GROUP BY producto
""")
```

**Resultado**: Mismo dato final, pero el proceso es diferente.

---

## 3. Arquitecturas de Pipelines: Batch vs Streaming

### Batch Processing (Por Lotes)

**DefiniciÃ³n**: Procesar datos en **bloques** o **lotes** a intervalos regulares.

**AnalogÃ­a**: AutobÃºs pÃºblico

Un autobÃºs:
- Espera a que **varios pasajeros** suban
- Parte cada **30 minutos** (intervalo fijo)
- Lleva a **todos juntos** al destino

**Ejemplo**:
```python
# Pipeline que se ejecuta cada 24 horas
cada_dia_a_las_3am():
    ventas_del_dia = extraer_ventas()
    ventas_limpias = transformar(ventas_del_dia)
    cargar_en_warehouse(ventas_limpias)
```

**Casos de Uso**:
- ğŸ“Š Reportes diarios de ventas
- ğŸ“§ EnvÃ­o de newsletters semanales
- ğŸ’° CÃ¡lculo de nÃ³mina mensual
- ğŸ“ˆ AnÃ¡lisis de datos histÃ³ricos

**Ventajas**:
- âœ… MÃ¡s **simple** de implementar
- âœ… Usa recursos de forma **eficiente** (procesa mucho de golpe)
- âœ… MÃ¡s **econÃ³mico** (no necesitas infraestructura 24/7)

**Desventajas**:
- âŒ **Latencia alta**: Los datos llegan con retraso
- âŒ No sirve para **decisiones en tiempo real**

---

### Streaming Processing (En Tiempo Real)

**DefiniciÃ³n**: Procesar datos **uno por uno** o en **micro-lotes** a medida que llegan.

**AnalogÃ­a**: Taxi

Un taxi:
- Sale **inmediatamente** cuando llega un pasajero
- No espera a nadie mÃ¡s
- Entrega **al instante**

**Ejemplo**:
```python
# Pipeline que procesa cada evento al instante
while True:
    evento = esperar_evento()  # Click en la web, compra, etc.
    evento_procesado = transformar(evento)
    guardar_en_tiempo_real(evento_procesado)
```

**Casos de Uso**:
- ğŸš¨ DetecciÃ³n de fraude en transacciones bancarias
- ğŸ“ Tracking de ubicaciÃ³n en tiempo real (Uber, Glovo)
- ğŸ’¬ Chat en vivo y notificaciones
- ğŸ“º Streaming de video (Netflix, YouTube)
- ğŸ® Videojuegos online

**Ventajas**:
- âœ… **Latencia baja**: Datos disponibles en segundos
- âœ… Permite **decisiones inmediatas**
- âœ… Mejor experiencia de usuario

**Desventajas**:
- âŒ MÃ¡s **complejo** de implementar (Kafka, Spark Streaming)
- âŒ MÃ¡s **costoso** (infraestructura debe estar siempre activa)
- âŒ MÃ¡s difÃ­cil de **debuggear**

---

### Â¿CuÃ¡ndo Usar Batch vs Streaming?

| Pregunta                                              | Batch | Streaming |
| ----------------------------------------------------- | ----- | --------- |
| Â¿Los datos deben estar disponibles en **< 1 minuto**? | âŒ     | âœ…         |
| Â¿Puedes esperar **horas o dÃ­as**?                     | âœ…     | âŒ         |
| Â¿Tienes presupuesto **limitado**?                     | âœ…     | âŒ         |
| Â¿Necesitas procesar **grandes volÃºmenes** (TB)?       | âœ…     | âš ï¸         |
| Â¿Es crÃ­tico para el negocio (fraude, seguridad)?      | âŒ     | âœ…         |

**Regla general**:
- **Batch**: Reportes, anÃ¡lisis histÃ³ricos, dashboards que se actualizan cada X horas
- **Streaming**: DetecciÃ³n de fraude, notificaciones, tracking en vivo

---

### Arquitectura HÃ­brida (Lambda Architecture)

En la prÃ¡ctica, muchas empresas usan **ambos**:

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Fuente     â”‚
                        â”‚  de Datos    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                       â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Batch Layer   â”‚    â”‚ Streaming Layer â”‚
            â”‚  (histÃ³rico)   â”‚    â”‚  (tiempo real)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                        â”‚   Serving    â”‚
                        â”‚    Layer     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ejemplo**:
- **Batch**: Procesa ventas histÃ³ricas cada noche (Ãºltimos 5 aÃ±os)
- **Streaming**: Procesa ventas de hoy en tiempo real (Ãºltimas 24 horas)
- **Serving**: Combina ambos para dar vista completa

---

## 4. Idempotencia: La Clave de Pipelines Robustos

### Â¿QuÃ© es la Idempotencia?

**DefiniciÃ³n**: Una operaciÃ³n es **idempotente** si ejecutarla **mÃºltiples veces** produce el **mismo resultado** que ejecutarla **una vez**.

**AnalogÃ­a**: Interruptor de luz

Cuando enciendes la luz:
- Primera vez: âœ… Luz encendida
- Segunda vez: âœ… Luz sigue encendida (no se rompe)
- Tercera vez: âœ… Luz sigue encendida

No importa cuÃ¡ntas veces presiones el interruptor estando encendido, el resultado es el mismo: **luz encendida**.

### En Pipelines de Datos

**Pipeline NO Idempotente** (âŒ MAL):
```python
def procesar_ventas(fecha):
    ventas = extraer_ventas(fecha)
    for venta in ventas:
        # âŒ Inserta SIEMPRE (duplica si vuelves a ejecutar)
        insertar_en_db(venta)
```

**Problema**: Si ejecutas este pipeline 2 veces, tendrÃ¡s **datos duplicados**.

```sql
-- Primera ejecuciÃ³n: 100 filas
-- Segunda ejecuciÃ³n: 200 filas (100 duplicadas!)
SELECT COUNT(*) FROM ventas;  -- 200 âŒ
```

**Pipeline Idempotente** (âœ… BIEN):
```python
def procesar_ventas(fecha):
    ventas = extraer_ventas(fecha)

    # âœ… Borra datos de esa fecha primero
    eliminar_ventas_de_fecha(fecha)

    # âœ… Inserta datos frescos
    for venta in ventas:
        insertar_en_db(venta)
```

**Resultado**: Si ejecutas este pipeline 2 veces, tendrÃ¡s **100 filas** (sin duplicados).

```sql
-- Primera ejecuciÃ³n: 100 filas
-- Segunda ejecuciÃ³n: 100 filas (mismo resultado!)
SELECT COUNT(*) FROM ventas;  -- 100 âœ…
```

---

### Estrategias para Lograr Idempotencia

#### 1. **DELETE + INSERT** (Recomendado para batch pequeÃ±o)

```python
# 1. Borra datos del perÃ­odo
DELETE FROM ventas WHERE fecha = '2025-10-23'

# 2. Inserta datos frescos
INSERT INTO ventas VALUES (...)
```

**Ventajas**: Simple y efectivo
**Desventajas**: No eficiente con grandes volÃºmenes

#### 2. **TRUNCATE + INSERT** (Para tablas temporales)

```python
# 1. VacÃ­a la tabla completamente
TRUNCATE TABLE ventas_temp

# 2. Inserta datos
INSERT INTO ventas_temp VALUES (...)
```

**Ventajas**: Muy rÃ¡pido
**Desventajas**: Pierdes TODO (solo para tablas staging)

#### 3. **MERGE / UPSERT** (Recomendado para producciÃ³n)

```python
# Si existe: actualiza. Si no existe: inserta
MERGE INTO ventas AS target
USING ventas_nuevas AS source
ON target.id = source.id
WHEN MATCHED THEN
    UPDATE SET precio = source.precio
WHEN NOT MATCHED THEN
    INSERT VALUES (source.id, source.precio)
```

**Ventajas**: Maneja inserciones y actualizaciones
**Desventajas**: MÃ¡s complejo de implementar

#### 4. **Archivos con Timestamp** (Para data lakes)

```python
# Nombra archivos con timestamp Ãºnico
archivo = f"ventas_{fecha}_{timestamp}.parquet"

# Si reprocesas, creas un archivo nuevo
# Luego puedes deduplicar leyendo todos
```

**Ventajas**: Historial completo de ejecuciones
**Desventajas**: Necesitas lÃ³gica de deduplicaciÃ³n al leer

---

### Por QuÃ© es CrÃ­tica la Idempotencia

Imagina este escenario:

```
3:00 AM: Pipeline de ventas se ejecuta
3:30 AM: Falla por error de red (50% procesado)
3:35 AM: Sistema de alertas notifica al ingeniero
3:40 AM: Ingeniero re-ejecuta el pipeline

Â¿QuÃ© pasa?
```

**Sin idempotencia** (âŒ):
- Primera ejecuciÃ³n: 50% de datos insertados
- Re-ejecuciÃ³n: 50% duplicados + 50% nuevos
- Resultado: **Datos inconsistentes, reportes incorrectos**
- Consecuencia: El CEO toma decisiones con datos **ERRÃ“NEOS**

**Con idempotencia** (âœ…):
- Primera ejecuciÃ³n: 50% de datos insertados (falla)
- Re-ejecuciÃ³n: Borra TODO y procesa 100% de nuevo
- Resultado: **Datos correctos y consistentes**
- Consecuencia: El CEO puede confiar en los reportes

**Regla de Oro**: Siempre diseÃ±a tus pipelines para que puedan **reprocesarse** sin causar problemas.

---

## 5. Componentes de un Pipeline de ProducciÃ³n

Un pipeline profesional NO es solo "extract, transform, load". Incluye:

### 1. **ExtracciÃ³n (Extract)**
- Leer datos de mÃºltiples fuentes
- Manejar diferentes formatos (CSV, JSON, API, DB)
- **Retry logic**: Reintentar si falla la conexiÃ³n
- **ValidaciÃ³n inicial**: Â¿Los datos tienen el formato esperado?

### 2. **TransformaciÃ³n (Transform)**
- Limpiar datos (nulos, duplicados, outliers)
- Calcular nuevas columnas
- Combinar datos de diferentes fuentes
- **ValidaciÃ³n**: Â¿Los datos transformados son correctos?

### 3. **Carga (Load)**
- Guardar en destino (DB, data warehouse, data lake)
- **Bulk inserts**: Cargar miles de filas de golpe (no una por una)
- **Idempotencia**: Asegurar que se puede reprocesar

### 4. **Logging (Registro)**
```python
logger.info("Iniciando pipeline de ventas")
logger.info(f"ExtraÃ­das {len(ventas)} ventas")
logger.warning(f"Encontrados {nulos} valores nulos")
logger.error("Error al conectar a la DB")
```

**Por quÃ© es crucial**: Sin logs, cuando algo falla, no sabes **quÃ©, dÃ³nde ni por quÃ©**.

### 5. **Monitoreo (Monitoring)**
- **MÃ©tricas**: Â¿CuÃ¡ntas filas procesadas? Â¿CuÃ¡nto tiempo tomÃ³?
- **Alertas**: Notificar si el pipeline falla o tarda mucho
- **Dashboards**: Visualizar el estado del pipeline en tiempo real

### 6. **Manejo de Errores (Error Handling)**
```python
try:
    extraer_datos()
except ConnectionError:
    # Reintentar 3 veces con exponential backoff
    reintentar()
except ValueError:
    # Guardar datos problemÃ¡ticos en "dead letter queue"
    guardar_error()
```

### 7. **Tests (Testing)**
- **Unit tests**: Â¿La funciÃ³n de transformaciÃ³n funciona bien?
- **Integration tests**: Â¿El pipeline completo funciona?
- **Data quality tests**: Â¿Los datos cargados son correctos?

---

## 6. Reprocessing: Cuando Algo Sale Mal

**Reprocessing** es **volver a ejecutar** un pipeline para un perÃ­odo de tiempo pasado.

### Â¿CuÃ¡ndo necesitas Reprocessing?

1. **Bug en la transformaciÃ³n** descubierto 2 semanas despuÃ©s
2. **Fuente de datos incorrecta**: Te dieron datos errÃ³neos y ahora tienen los correctos
3. **Cambio de negocio**: Ahora quieres calcular un nuevo campo en datos histÃ³ricos
4. **Fallo parcial**: El pipeline procesÃ³ 50% y fallÃ³

### Ejemplo

```python
# 1. Pipeline original (1 octubre)
procesar_ventas('2025-10-01')  # ProcesÃ³ mal (bug en transformaciÃ³n)

# 2. Descubres el bug (15 octubre)
# Los datos de octubre estÃ¡n INCORRECTOS

# 3. Corriges el bug

# 4. Reprocessing: Vuelves a procesar todo octubre
for dia in range(1, 16):
    procesar_ventas(f'2025-10-{dia:02d}')
```

### CÃ³mo DiseÃ±ar para Reprocessing

1. **Idempotencia**: Asegura que reprocesar no cause duplicados
2. **Particionamiento**: Procesa por fecha para poder reprocesar perÃ­odos especÃ­ficos
3. **Versionado**: Guarda diferentes versiones de los datos

```python
# Mal diseÃ±o (âŒ): Todo en una tabla sin fecha
guardar_en_db(ventas)

# Buen diseÃ±o (âœ…): Particionado por fecha
guardar_en_db(ventas, particion='2025-10-23')
```

---

## 7. Errores Comunes al DiseÃ±ar Pipelines

### Error 1: No Hacer el Pipeline Idempotente

**Problema**: Ejecutar 2 veces = datos duplicados

**SoluciÃ³n**: Usa DELETE + INSERT o MERGE/UPSERT

---

### Error 2: No Loggear Suficiente

**Problema**: Cuando falla, no sabes quÃ© pasÃ³

**SoluciÃ³n**:
```python
logger.info("Iniciando pipeline")
logger.info(f"ExtraÃ­das {n} filas")
logger.warning(f"{nulos} valores nulos encontrados")
logger.error("Error al conectar a la DB: {error}")
```

---

### Error 3: Procesar Todo en una TransacciÃ³n Grande

**Problema**: Si algo falla al 90%, pierdes todo el trabajo

**SoluciÃ³n**: Usa **micro-batches** (procesa de 1000 en 1000)

```python
# Mal (âŒ): Todo o nada
with transaccion():
    for venta in todas_las_ventas:  # 10 millones
        insertar(venta)

# Bien (âœ…): En bloques
for lote in dividir_en_lotes(ventas, 1000):
    with transaccion():
        for venta in lote:
            insertar(venta)
```

---

### Error 4: No Manejar Casos Borde

**Casos borde comunes**:
- Â¿QuÃ© pasa si el archivo estÃ¡ vacÃ­o?
- Â¿QuÃ© pasa si no hay datos para hoy?
- Â¿QuÃ© pasa si la columna esperada no existe?

**SoluciÃ³n**: Valida **siempre** antes de procesar

```python
if archivo_vacio():
    logger.warning("Archivo vacÃ­o, saltando procesamiento")
    return

if not columna_existe('precio'):
    raise ValueError("Columna 'precio' no encontrada")
```

---

### Error 5: No Testear el Pipeline

**Problema**: Lo pruebas con 10 filas, falla con 10 millones

**SoluciÃ³n**:
- Test con datos reales (no inventados)
- Test con volÃºmenes grandes
- Test con datos sucios (nulos, duplicados, outliers)

---

## Checklist de Aprendizaje

Antes de continuar al siguiente tema, verifica que puedes:

- [ ] Explicar quÃ© es un pipeline de datos con tus propias palabras
- [ ] Diferenciar entre ETL y ELT
- [ ] Decidir cuÃ¡ndo usar ETL vs ELT segÃºn el contexto
- [ ] Explicar la diferencia entre Batch y Streaming
- [ ] Definir quÃ© es idempotencia y por quÃ© es importante
- [ ] DiseÃ±ar un pipeline idempotente (DELETE + INSERT o MERGE)
- [ ] Listar los componentes de un pipeline de producciÃ³n
- [ ] Explicar quÃ© es reprocessing y cuÃ¡ndo se necesita
- [ ] Identificar errores comunes al diseÃ±ar pipelines

---

## Resumen del Tema

1. **Pipeline de datos**: Serie de pasos automatizados que mueven y transforman datos
2. **ETL**: Transform **antes** de cargar (Ãºtil con destinos pequeÃ±os)
3. **ELT**: Transform **despuÃ©s** de cargar (Ãºtil con data warehouses potentes)
4. **Batch**: Procesar por lotes cada X tiempo (simple, econÃ³mico, latencia alta)
5. **Streaming**: Procesar en tiempo real (complejo, costoso, latencia baja)
6. **Idempotencia**: Ejecutar N veces = mismo resultado que 1 vez (crÃ­tico para robustez)
7. **Reprocessing**: Volver a ejecutar un pipeline para corregir datos histÃ³ricos
8. **Componentes clave**: Extract, Transform, Load, Logging, Monitoring, Error Handling, Tests

---

## PrÃ³ximos Pasos

Ahora que entiendes los conceptos fundamentales, en los siguientes temas aprenderÃ¡s a **implementarlos**:

- **Tema 2**: ExtracciÃ³n de datos (archivos, APIs, web scraping)
- **Tema 3**: TransformaciÃ³n con Pandas
- **Tema 4**: ValidaciÃ³n de calidad de datos
- **Tema 5**: Formatos modernos (Parquet, Avro)
- **Tema 6**: Estrategias de carga y pipelines completos

---

## Recursos Adicionales

### Libros
- "Data Pipelines Pocket Reference" - James Densmore
- "Designing Data-Intensive Applications" - Martin Kleppmann

### ArtÃ­culos
- [ETL vs ELT: The Difference Is In The How](https://www.integrate.io/blog/etl-vs-elt/)
- [Idempotent Data Pipelines](https://medium.com/airbnb-engineering/idempotent-data-pipelines-ef9c4e7f5c6d)

### Herramientas
- Apache Airflow (orquestaciÃ³n de pipelines)
- dbt (transformaciones con SQL)
- Great Expectations (calidad de datos)

---

**Â¡Felicidades!** Has completado la teorÃ­a del Tema 1. Ahora continÃºa con **02-EJEMPLOS.md** para ver pipelines en acciÃ³n, y luego practica con **03-EJERCICIOS.md**.

**Ãšltima actualizaciÃ³n**: 2025-10-23
