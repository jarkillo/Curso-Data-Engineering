# Ejemplos Pr√°cticos: Conceptos de ETL/ELT

**Tiempo de lectura**: 45-60 minutos
**Nivel**: Intermedio
**Prerrequisitos**: Haber le√≠do `01-TEORIA.md`

---

## Introducci√≥n

En este archivo encontrar√°s 5 ejemplos trabajados paso a paso que ilustran los conceptos de pipelines de datos. Cada ejemplo incluye:
- Contexto empresarial realista
- Datos de ejemplo
- C√≥digo Python completo
- Explicaci√≥n detallada de cada paso
- Interpretaci√≥n de resultados

**Recomendaci√≥n**: Ejecuta el c√≥digo en tu entorno para experimentar.

---

## Datos de Ejemplo

Para todos los ejemplos, usaremos una empresa ficticia:

**TechStore**: E-commerce de productos electr√≥nicos con 5 sucursales.

Archivos de datos (debes crear estos archivos para ejecutar los ejemplos):

### `ventas_2025.csv`
```csv
fecha,sucursal,producto,cantidad,precio_unitario
2025-10-01,Madrid,Laptop Dell,2,899.99
2025-10-01,Barcelona,Mouse Logitech,5,29.99
2025-10-02,Madrid,Teclado Mec√°nico,3,149.99
2025-10-02,Valencia,Monitor Samsung,1,399.99
2025-10-03,Barcelona,Laptop HP,1,799.99
2025-10-03,Sevilla,Auriculares Sony,4,79.99
2025-10-04,Madrid,SSD 1TB,2,119.99
2025-10-04,Barcelona,Webcam Logitech,3,89.99
2025-10-05,Valencia,Laptop Dell,1,899.99
2025-10-05,Sevilla,Mouse Wireless,6,19.99
```

---

## Ejemplo 1: Pipeline ETL B√°sico (CSV ‚Üí Transform ‚Üí SQLite)

### Nivel: B√°sico ‚≠ê

### Contexto

TechStore necesita un pipeline que:
1. Lea ventas diarias desde CSV
2. Calcule el total de cada venta (cantidad √ó precio)
3. Guarde en una base de datos SQLite

Este es un pipeline **ETL**: transforma antes de cargar.

---

### Paso 1: Extract (Leer CSV)

```python
import pandas as pd
from pathlib import Path

def extraer_ventas(archivo: str) -> pd.DataFrame:
    """
    Extrae datos de ventas desde un archivo CSV.

    Args:
        archivo: Ruta al archivo CSV

    Returns:
        DataFrame con las ventas
    """
    print(f"üìÇ Leyendo archivo: {archivo}")

    # Validar que el archivo existe
    if not Path(archivo).exists():
        raise FileNotFoundError(f"Archivo {archivo} no encontrado")

    # Leer CSV
    ventas = pd.read_csv(archivo)

    print(f"‚úÖ Extra√≠das {len(ventas)} ventas")
    return ventas


# Ejecutar extracci√≥n
ventas = extraer_ventas("ventas_2025.csv")
print(ventas.head())
```

**Output**:
```
üìÇ Leyendo archivo: ventas_2025.csv
‚úÖ Extra√≠das 10 ventas
        fecha   sucursal            producto  cantidad  precio_unitario
0  2025-10-01     Madrid        Laptop Dell         2           899.99
1  2025-10-01  Barcelona    Mouse Logitech         5            29.99
2  2025-10-02     Madrid  Teclado Mec√°nico         3           149.99
```

---

### Paso 2: Transform (Calcular Total)

```python
def transformar_ventas(ventas: pd.DataFrame) -> pd.DataFrame:
    """
    Transforma los datos calculando el total de cada venta.

    Args:
        ventas: DataFrame con ventas crudas

    Returns:
        DataFrame con columna 'total' a√±adida
    """
    print("üîÑ Transformando datos...")

    # Calcular total = cantidad √ó precio_unitario
    ventas['total'] = ventas['cantidad'] * ventas['precio_unitario']

    # Redondear a 2 decimales
    ventas['total'] = ventas['total'].round(2)

    print(f"‚úÖ Transformaci√≥n completada: {len(ventas)} filas")
    return ventas


# Ejecutar transformaci√≥n
ventas_transformadas = transformar_ventas(ventas)
print(ventas_transformadas.head())
```

**Output**:
```
üîÑ Transformando datos...
‚úÖ Transformaci√≥n completada: 10 filas
        fecha   sucursal            producto  cantidad  precio_unitario    total
0  2025-10-01     Madrid        Laptop Dell         2           899.99  1799.98
1  2025-10-01  Barcelona    Mouse Logitech         5            29.99   149.95
2  2025-10-02     Madrid  Teclado Mec√°nico         3           149.99   449.97
```

---

### Paso 3: Load (Guardar en SQLite)

```python
import sqlite3
from typing import Optional

def cargar_en_db(ventas: pd.DataFrame, db_path: str = "ventas.db") -> None:
    """
    Carga las ventas en una base de datos SQLite.

    Args:
        ventas: DataFrame con ventas transformadas
        db_path: Ruta a la base de datos SQLite
    """
    print(f"üíæ Cargando datos en {db_path}...")

    # Conectar a SQLite (crea la DB si no existe)
    conn = sqlite3.connect(db_path)

    try:
        # Cargar DataFrame en tabla 'ventas'
        # if_exists='replace' ‚Üí Reemplaza la tabla (idempotente)
        ventas.to_sql('ventas', conn, if_exists='replace', index=False)

        print(f"‚úÖ Cargadas {len(ventas)} filas en la tabla 'ventas'")
    finally:
        conn.close()


# Ejecutar carga
cargar_en_db(ventas_transformadas)

# Verificar que se carg√≥ correctamente
conn = sqlite3.connect("ventas.db")
resultado = pd.read_sql("SELECT * FROM ventas LIMIT 3", conn)
conn.close()

print("\nüìä Primeras 3 filas en la base de datos:")
print(resultado)
```

**Output**:
```
üíæ Cargando datos en ventas.db...
‚úÖ Cargadas 10 filas en la tabla 'ventas'

üìä Primeras 3 filas en la base de datos:
        fecha   sucursal            producto  cantidad  precio_unitario    total
0  2025-10-01     Madrid        Laptop Dell         2           899.99  1799.98
1  2025-10-01  Barcelona    Mouse Logitech         5            29.99   149.95
2  2025-10-02     Madrid  Teclado Mec√°nico         3           149.99   449.97
```

---

### Pipeline Completo (ETL)

```python
def pipeline_etl_basico(archivo_csv: str, db_path: str = "ventas.db") -> None:
    """
    Pipeline ETL completo: Extract ‚Üí Transform ‚Üí Load.

    Args:
        archivo_csv: Archivo CSV con ventas
        db_path: Base de datos de destino
    """
    print("üöÄ Iniciando Pipeline ETL...")

    # 1. Extract
    ventas = extraer_ventas(archivo_csv)

    # 2. Transform
    ventas_transformadas = transformar_ventas(ventas)

    # 3. Load
    cargar_en_db(ventas_transformadas, db_path)

    print("‚úÖ Pipeline ETL completado exitosamente!")


# Ejecutar pipeline completo
pipeline_etl_basico("ventas_2025.csv")
```

---

### Interpretaci√≥n de Resultados

**¬øQu√© logramos?**
- ‚úÖ Automatizamos el proceso de cargar ventas diarias
- ‚úÖ Calculamos el total de cada venta (ahorramos tiempo vs hacerlo manual)
- ‚úÖ Centralizamos los datos en una base de datos (f√°cil de consultar)

**¬øEs idempotente?**
- ‚úÖ S√≠, porque usamos `if_exists='replace'`
- Si ejecutamos el pipeline 2 veces, obtenemos el mismo resultado
- No hay duplicados

**Decisiones de negocio basadas en esto**:
```python
# An√°lisis r√°pido con SQL
conn = sqlite3.connect("ventas.db")
analisis = pd.read_sql("""
    SELECT sucursal, SUM(total) AS total_ventas
    FROM ventas
    GROUP BY sucursal
    ORDER BY total_ventas DESC
""", conn)
conn.close()

print(analisis)
```

**Output**:
```
    sucursal  total_ventas
0     Madrid       2369.94
1  Barcelona       1039.93
2   Valencia        1299.98
3    Sevilla        439.95
```

**Decisi√≥n**: Madrid es la sucursal m√°s rentable. Podemos:
- Replicar estrategias de Madrid en otras sucursales
- Asignar m√°s inventario a Madrid

---

## Ejemplo 2: Pipeline ELT (Load ‚Üí Transform en SQL)

### Nivel: B√°sico ‚≠ê

### Contexto

Ahora TechStore quiere usar **ELT**: cargar los datos **primero** (sin transformar) y hacer el c√°lculo de totales **en la base de datos** con SQL.

---

### Paso 1: Extract (Igual que antes)

```python
ventas = extraer_ventas("ventas_2025.csv")
```

---

### Paso 2: Load (Cargar SIN transformar)

```python
def cargar_datos_crudos(ventas: pd.DataFrame, db_path: str = "ventas.db") -> None:
    """
    Carga datos CRUDOS sin transformar (ELT).

    Args:
        ventas: DataFrame con ventas sin transformar
        db_path: Ruta a la base de datos
    """
    print(f"üíæ Cargando datos CRUDOS en {db_path}...")

    conn = sqlite3.connect(db_path)
    try:
        # Cargar datos SIN la columna 'total'
        ventas.to_sql('ventas_crudas', conn, if_exists='replace', index=False)
        print(f"‚úÖ Cargadas {len(ventas)} filas en 'ventas_crudas'")
    finally:
        conn.close()


# Cargar datos crudos
cargar_datos_crudos(ventas)
```

---

### Paso 3: Transform (Dentro de la base de datos)

```python
def transformar_en_db(db_path: str = "ventas.db") -> pd.DataFrame:
    """
    Transforma datos DENTRO de la base de datos usando SQL.

    Args:
        db_path: Ruta a la base de datos

    Returns:
        DataFrame con datos transformados
    """
    print("üîÑ Transformando datos dentro de la base de datos...")

    conn = sqlite3.connect(db_path)
    try:
        # Crear vista con el total calculado
        conn.execute("""
            CREATE TABLE IF NOT EXISTS ventas_procesadas AS
            SELECT
                fecha,
                sucursal,
                producto,
                cantidad,
                precio_unitario,
                ROUND(cantidad * precio_unitario, 2) AS total
            FROM ventas_crudas
        """)

        # Leer resultado
        resultado = pd.read_sql("SELECT * FROM ventas_procesadas", conn)

        print(f"‚úÖ Transformaci√≥n completada: {len(resultado)} filas")
        return resultado
    finally:
        conn.close()


# Transformar en la base de datos
ventas_procesadas = transformar_en_db()
print(ventas_procesadas.head())
```

---

### Pipeline Completo (ELT)

```python
def pipeline_elt(archivo_csv: str, db_path: str = "ventas.db") -> None:
    """
    Pipeline ELT completo: Extract ‚Üí Load ‚Üí Transform.

    Args:
        archivo_csv: Archivo CSV con ventas
        db_path: Base de datos de destino
    """
    print("üöÄ Iniciando Pipeline ELT...")

    # 1. Extract
    ventas = extraer_ventas(archivo_csv)

    # 2. Load (datos crudos)
    cargar_datos_crudos(ventas, db_path)

    # 3. Transform (dentro de la DB)
    ventas_procesadas = transformar_en_db(db_path)

    print("‚úÖ Pipeline ELT completado exitosamente!")
    return ventas_procesadas


# Ejecutar pipeline ELT
pipeline_elt("ventas_2025.csv")
```

---

### Interpretaci√≥n de Resultados

**¬øQu√© diferencia hay con ETL?**

| Aspecto            | ETL (Ejemplo 1)                                     | ELT (Ejemplo 2)                      |
| ------------------ | --------------------------------------------------- | ------------------------------------ |
| **Transformaci√≥n** | En Python (Pandas)                                  | En SQL (dentro de la DB)             |
| **Datos cargados** | Ya transformados                                    | Crudos                               |
| **Flexibilidad**   | Si quieres cambiar transformaci√≥n, debes reprocesar | Puedes re-transformar sin re-extraer |
| **Velocidad**      | M√°s lenta con grandes vol√∫menes                     | M√°s r√°pida (DB optimizada para SQL)  |

**¬øCu√°ndo usar ELT?**
- ‚úÖ Cuando tienes una base de datos potente (PostgreSQL, Snowflake)
- ‚úÖ Cuando quieres flexibilidad para re-transformar
- ‚úÖ Cuando trabajas con grandes vol√∫menes

---

## Ejemplo 3: Pipeline Batch Diario (Programado)

### Nivel: Intermedio ‚≠ê‚≠ê

### Contexto

TechStore necesita un pipeline que se ejecute **autom√°ticamente cada d√≠a** a las 3:00 AM para procesar las ventas del d√≠a anterior.

---

### Paso 1: Pipeline con Fecha Parametrizada

```python
from datetime import datetime, timedelta

def pipeline_batch_diario(fecha: str, db_path: str = "ventas.db") -> None:
    """
    Pipeline batch que procesa ventas de una fecha espec√≠fica.

    Args:
        fecha: Fecha a procesar (formato: YYYY-MM-DD)
        db_path: Base de datos de destino
    """
    print(f"üöÄ Iniciando Pipeline Batch para fecha: {fecha}")

    # 1. Extract: Leer TODAS las ventas
    ventas = extraer_ventas("ventas_2025.csv")

    # 2. Filter: Quedarnos solo con las ventas de la fecha
    ventas_fecha = ventas[ventas['fecha'] == fecha].copy()

    if len(ventas_fecha) == 0:
        print(f"‚ö†Ô∏è No hay ventas para la fecha {fecha}")
        return

    print(f"üìä Encontradas {len(ventas_fecha)} ventas para {fecha}")

    # 3. Transform: Calcular total
    ventas_fecha['total'] = (ventas_fecha['cantidad'] *
                              ventas_fecha['precio_unitario']).round(2)

    # 4. Load: Cargar en la DB (idempotente)
    conn = sqlite3.connect(db_path)
    try:
        # Borrar ventas de esa fecha primero (idempotencia)
        conn.execute("DELETE FROM ventas WHERE fecha = ?", (fecha,))

        # Insertar ventas nuevas
        ventas_fecha.to_sql('ventas', conn, if_exists='append', index=False)

        print(f"‚úÖ Cargadas {len(ventas_fecha)} ventas para {fecha}")
    finally:
        conn.close()

    print(f"‚úÖ Pipeline Batch completado para {fecha}")


# Ejecutar para diferentes fechas
pipeline_batch_diario("2025-10-01")
pipeline_batch_diario("2025-10-02")
pipeline_batch_diario("2025-10-03")
```

**Output**:
```
üöÄ Iniciando Pipeline Batch para fecha: 2025-10-01
üìä Encontradas 2 ventas para 2025-10-01
‚úÖ Cargadas 2 ventas para 2025-10-01
‚úÖ Pipeline Batch completado para 2025-10-01

üöÄ Iniciando Pipeline Batch para fecha: 2025-10-02
üìä Encontradas 2 ventas para 2025-10-02
‚úÖ Cargadas 2 ventas para 2025-10-02
‚úÖ Pipeline Batch completado para 2025-10-02
...
```

---

### Paso 2: Programar Ejecuci√≥n Autom√°tica (Simulaci√≥n)

```python
def pipeline_automatico():
    """
    Simula un pipeline que se ejecuta cada d√≠a.
    En producci√≥n, esto se har√≠a con cron o Apache Airflow.
    """
    # Obtener la fecha de ayer
    ayer = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

    print(f"‚è∞ Ejecutando pipeline programado para {ayer}")

    try:
        pipeline_batch_diario(ayer)
    except Exception as e:
        print(f"‚ùå Error en el pipeline: {e}")
        # En producci√≥n, aqu√≠ enviar√≠as una alerta (email, Slack, etc.)


# Ejecutar pipeline autom√°tico
pipeline_automatico()
```

---

### Interpretaci√≥n de Resultados

**¬øQu√© logramos?**
- ‚úÖ Pipeline parametrizado por fecha (flexible)
- ‚úÖ Idempotente (DELETE antes de INSERT)
- ‚úÖ Manejo de casos borde (¬øqu√© pasa si no hay ventas?)
- ‚úÖ Listo para programarse con cron o Airflow

**Ventajas del Batch Processing**:
- Simple de implementar
- Usa recursos eficientemente (procesa todo de golpe)
- Perfecto para reportes diarios

**Limitaciones**:
- Latencia alta (datos disponibles solo una vez al d√≠a)
- No sirve para decisiones en tiempo real

---

## Ejemplo 4: Pipeline con Reprocessing (Corregir Datos Hist√≥ricos)

### Nivel: Intermedio ‚≠ê‚≠ê

### Contexto

TechStore descubre que el pipeline del 1 al 5 de octubre ten√≠a un **bug**: no calculaba correctamente el IVA (21%). Necesitan **reprocesar** esos d√≠as.

---

### Paso 1: Pipeline con Bug (Incorrecto)

```python
def pipeline_con_bug(fecha: str) -> None:
    """
    Pipeline INCORRECTO que no calcula el IVA.
    """
    ventas = extraer_ventas("ventas_2025.csv")
    ventas_fecha = ventas[ventas['fecha'] == fecha].copy()

    # ‚ùå Bug: No suma el IVA
    ventas_fecha['total'] = (ventas_fecha['cantidad'] *
                              ventas_fecha['precio_unitario']).round(2)

    conn = sqlite3.connect("ventas.db")
    try:
        conn.execute("DELETE FROM ventas WHERE fecha = ?", (fecha,))
        ventas_fecha.to_sql('ventas', conn, if_exists='append', index=False)
    finally:
        conn.close()


# Simular que procesamos con bug
for dia in range(1, 6):
    fecha = f"2025-10-{dia:02d}"
    pipeline_con_bug(fecha)
    print(f"‚ùå Procesado (con bug) {fecha}")
```

---

### Paso 2: Corregir el Bug

```python
def pipeline_corregido(fecha: str) -> None:
    """
    Pipeline CORREGIDO que calcula el IVA correctamente.
    """
    ventas = extraer_ventas("ventas_2025.csv")
    ventas_fecha = ventas[ventas['fecha'] == fecha].copy()

    # ‚úÖ Correcci√≥n: Sumar 21% de IVA
    ventas_fecha['total_sin_iva'] = (ventas_fecha['cantidad'] *
                                      ventas_fecha['precio_unitario']).round(2)
    ventas_fecha['total'] = (ventas_fecha['total_sin_iva'] * 1.21).round(2)

    conn = sqlite3.connect("ventas.db")
    try:
        # Idempotencia: Borra y re-inserta
        conn.execute("DELETE FROM ventas WHERE fecha = ?", (fecha,))
        ventas_fecha.to_sql('ventas', conn, if_exists='append', index=False)
    finally:
        conn.close()

    print(f"‚úÖ Reprocesado (corregido) {fecha}")
```

---

### Paso 3: Reprocessing Masivo

```python
def reprocessing_fechas(fecha_inicio: str, fecha_fin: str) -> None:
    """
    Reprocesa un rango de fechas con el pipeline corregido.

    Args:
        fecha_inicio: Fecha inicial (YYYY-MM-DD)
        fecha_fin: Fecha final (YYYY-MM-DD)
    """
    print(f"üîÑ Iniciando reprocessing de {fecha_inicio} a {fecha_fin}...")

    inicio = datetime.strptime(fecha_inicio, "%Y-%m-%d")
    fin = datetime.strptime(fecha_fin, "%Y-%m-%d")

    fecha_actual = inicio
    while fecha_actual <= fin:
        fecha_str = fecha_actual.strftime("%Y-%m-%d")
        pipeline_corregido(fecha_str)
        fecha_actual += timedelta(days=1)

    print("‚úÖ Reprocessing completado!")


# Reprocesar del 1 al 5 de octubre
reprocessing_fechas("2025-10-01", "2025-10-05")
```

**Output**:
```
üîÑ Iniciando reprocessing de 2025-10-01 a 2025-10-05...
‚úÖ Reprocesado (corregido) 2025-10-01
‚úÖ Reprocesado (corregido) 2025-10-02
‚úÖ Reprocesado (corregido) 2025-10-03
‚úÖ Reprocesado (corregido) 2025-10-04
‚úÖ Reprocesado (corregido) 2025-10-05
‚úÖ Reprocessing completado!
```

---

### Verificar Correcci√≥n

```python
# Comparar datos antes y despu√©s del reprocessing
conn = sqlite3.connect("ventas.db")

# Consulta antes (con bug): total promedio ~200‚Ç¨
# Consulta despu√©s (corregido): total promedio ~242‚Ç¨ (20% m√°s por el IVA)
resultado = pd.read_sql("""
    SELECT
        fecha,
        COUNT(*) AS num_ventas,
        ROUND(AVG(total), 2) AS total_promedio
    FROM ventas
    WHERE fecha BETWEEN '2025-10-01' AND '2025-10-05'
    GROUP BY fecha
    ORDER BY fecha
""", conn)
conn.close()

print(resultado)
```

**Output**:
```
        fecha  num_ventas  total_promedio
0  2025-10-01           2          1180.93
1  2025-10-02           2           539.95
2  2025-10-03           2           532.48
3  2025-10-04           2           254.28
4  2025-10-05           2           559.48
```

---

### Interpretaci√≥n de Resultados

**¬øPor qu√© fue posible el reprocessing?**
- ‚úÖ **Idempotencia**: El pipeline borra y re-inserta (DELETE + INSERT)
- ‚úÖ **Particionamiento por fecha**: Podemos reprocesar solo d√≠as espec√≠ficos
- ‚úÖ **Fuente de datos conservada**: El CSV original sigue disponible

**Lecciones aprendidas**:
1. **Siempre dise√±a pipelines idempotentes** ‚Üí Podr√°s reprocesar sin problemas
2. **Particiona por fecha** ‚Üí Reprocessing m√°s eficiente
3. **Guarda logs detallados** ‚Üí Podr√°s identificar cu√°ndo comenz√≥ el bug
4. **Testea con datos reales** ‚Üí Descubre bugs antes de producci√≥n

---

## Ejemplo 5: Pipeline con Logging y Manejo de Errores

### Nivel: Avanzado ‚≠ê‚≠ê‚≠ê

### Contexto

TechStore necesita un pipeline **productivo** con:
- Logging detallado
- Manejo de errores
- Reintentos autom√°ticos
- M√©tricas de ejecuci√≥n

---

### Paso 1: Configurar Logging

```python
import logging
import time
from typing import Dict, Any

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```

---

### Paso 2: Pipeline con Logging y M√©tricas

```python
def pipeline_produccion(fecha: str, db_path: str = "ventas.db") -> Dict[str, Any]:
    """
    Pipeline de producci√≥n con logging, m√©tricas y manejo de errores.

    Args:
        fecha: Fecha a procesar
        db_path: Base de datos de destino

    Returns:
        Dict con m√©tricas de ejecuci√≥n
    """
    inicio = time.time()
    metricas = {
        'fecha': fecha,
        'filas_extraidas': 0,
        'filas_procesadas': 0,
        'filas_cargadas': 0,
        'tiempo_segundos': 0,
        'estado': 'INICIADO'
    }

    try:
        logger.info(f"üöÄ Iniciando pipeline para fecha: {fecha}")

        # 1. Extract
        logger.info("üìÇ Extrayendo datos...")
        ventas = extraer_ventas("ventas_2025.csv")
        metricas['filas_extraidas'] = len(ventas)
        logger.info(f"‚úÖ Extra√≠das {len(ventas)} filas")

        # 2. Filter
        logger.info(f"üîç Filtrando ventas de {fecha}...")
        ventas_fecha = ventas[ventas['fecha'] == fecha].copy()

        if len(ventas_fecha) == 0:
            logger.warning(f"‚ö†Ô∏è No hay ventas para {fecha}")
            metricas['estado'] = 'SIN_DATOS'
            return metricas

        logger.info(f"‚úÖ Encontradas {len(ventas_fecha)} ventas")

        # 3. Transform
        logger.info("üîÑ Transformando datos...")
        ventas_fecha['total'] = (ventas_fecha['cantidad'] *
                                  ventas_fecha['precio_unitario'] * 1.21).round(2)
        metricas['filas_procesadas'] = len(ventas_fecha)
        logger.info(f"‚úÖ Transformadas {len(ventas_fecha)} filas")

        # 4. Load
        logger.info(f"üíæ Cargando datos en {db_path}...")
        conn = sqlite3.connect(db_path)
        try:
            # Idempotencia
            filas_borradas = conn.execute(
                "DELETE FROM ventas WHERE fecha = ?", (fecha,)
            ).rowcount

            if filas_borradas > 0:
                logger.info(f"üóëÔ∏è Borradas {filas_borradas} filas existentes")

            # Cargar nuevas filas
            ventas_fecha.to_sql('ventas', conn, if_exists='append', index=False)
            metricas['filas_cargadas'] = len(ventas_fecha)
            logger.info(f"‚úÖ Cargadas {len(ventas_fecha)} filas")
        finally:
            conn.close()

        # M√©tricas finales
        metricas['tiempo_segundos'] = round(time.time() - inicio, 2)
        metricas['estado'] = 'EXITOSO'
        logger.info(f"‚úÖ Pipeline completado en {metricas['tiempo_segundos']}s")

        return metricas

    except FileNotFoundError as e:
        logger.error(f"‚ùå Archivo no encontrado: {e}")
        metricas['estado'] = 'ERROR_ARCHIVO'
        raise

    except Exception as e:
        logger.error(f"‚ùå Error inesperado: {e}", exc_info=True)
        metricas['estado'] = 'ERROR'
        raise
    finally:
        # Siempre registrar m√©tricas finales
        logger.info(f"üìä M√©tricas: {metricas}")


# Ejecutar pipeline de producci√≥n
metricas = pipeline_produccion("2025-10-01")
print(metricas)
```

**Output (en consola y en pipeline.log)**:
```
2025-10-23 10:30:15 - __main__ - INFO - üöÄ Iniciando pipeline para fecha: 2025-10-01
2025-10-23 10:30:15 - __main__ - INFO - üìÇ Extrayendo datos...
2025-10-23 10:30:15 - __main__ - INFO - ‚úÖ Extra√≠das 10 filas
2025-10-23 10:30:15 - __main__ - INFO - üîç Filtrando ventas de 2025-10-01...
2025-10-23 10:30:15 - __main__ - INFO - ‚úÖ Encontradas 2 ventas
2025-10-23 10:30:15 - __main__ - INFO - üîÑ Transformando datos...
2025-10-23 10:30:15 - __main__ - INFO - ‚úÖ Transformadas 2 filas
2025-10-23 10:30:15 - __main__ - INFO - üíæ Cargando datos en ventas.db...
2025-10-23 10:30:15 - __main__ - INFO - ‚úÖ Cargadas 2 filas
2025-10-23 10:30:15 - __main__ - INFO - ‚úÖ Pipeline completado en 0.12s
2025-10-23 10:30:15 - __main__ - INFO - üìä M√©tricas: {...}

{'fecha': '2025-10-01', 'filas_extraidas': 10, 'filas_procesadas': 2,
 'filas_cargadas': 2, 'tiempo_segundos': 0.12, 'estado': 'EXITOSO'}
```

---

### Paso 3: Manejo de Errores con Reintentos

```python
import time
from typing import Callable

def retry_con_exponential_backoff(
    func: Callable,
    max_intentos: int = 3,
    *args,
    **kwargs
) -> Any:
    """
    Ejecuta una funci√≥n con reintentos y exponential backoff.

    Args:
        func: Funci√≥n a ejecutar
        max_intentos: N√∫mero m√°ximo de reintentos
        *args: Argumentos para la funci√≥n
        **kwargs: Argumentos con nombre para la funci√≥n

    Returns:
        Resultado de la funci√≥n si tiene √©xito

    Raises:
        La √∫ltima excepci√≥n si todos los intentos fallan
    """
    for intento in range(1, max_intentos + 1):
        try:
            logger.info(f"üîÑ Intento {intento}/{max_intentos}")
            resultado = func(*args, **kwargs)
            logger.info(f"‚úÖ √âxito en intento {intento}")
            return resultado

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Intento {intento} fall√≥: {e}")

            if intento == max_intentos:
                logger.error(f"‚ùå Todos los intentos fallaron")
                raise

            # Exponential backoff: 2^intento segundos
            espera = 2 ** intento
            logger.info(f"‚è≥ Esperando {espera}s antes de reintentar...")
            time.sleep(espera)


# Ejemplo de uso
try:
    metricas = retry_con_exponential_backoff(
        pipeline_produccion,
        max_intentos=3,
        fecha="2025-10-01"
    )
    print(f"‚úÖ Pipeline exitoso: {metricas}")
except Exception as e:
    print(f"‚ùå Pipeline fall√≥ despu√©s de todos los reintentos: {e}")
```

---

### Interpretaci√≥n de Resultados

**¬øQu√© logramos con este pipeline de producci√≥n?**

1. **Logging detallado**:
   - Cada paso est√° registrado
   - F√°cil de debuggear cuando algo falla
   - Auditor√≠a completa de ejecuciones

2. **M√©tricas**:
   - Cu√°ntas filas se procesaron
   - Cu√°nto tiempo tom√≥
   - Estado final (EXITOSO, ERROR, SIN_DATOS)
   - √ötil para monitoreo y alertas

3. **Manejo de errores**:
   - Diferentes tipos de errores (archivo no encontrado, error de DB, etc.)
   - Logs de errores con stack trace completo
   - Reintentos autom√°ticos con exponential backoff

4. **Idempotencia garantizada**:
   - DELETE + INSERT
   - Logs indican si se borraron filas existentes

**Este es un pipeline listo para producci√≥n** ‚úÖ

---

## Resumen de los Ejemplos

| Ejemplo | Concepto     | Nivel | Aprendizaje Clave                                     |
| ------- | ------------ | ----- | ----------------------------------------------------- |
| 1       | ETL B√°sico   | ‚≠ê     | Extract ‚Üí Transform ‚Üí Load (transformaci√≥n en Python) |
| 2       | ELT          | ‚≠ê     | Extract ‚Üí Load ‚Üí Transform (transformaci√≥n en SQL)    |
| 3       | Batch Diario | ‚≠ê‚≠ê    | Pipelines parametrizados por fecha, idempotencia      |
| 4       | Reprocessing | ‚≠ê‚≠ê    | Corregir bugs reprocesando datos hist√≥ricos           |
| 5       | Producci√≥n   | ‚≠ê‚≠ê‚≠ê   | Logging, m√©tricas, reintentos, manejo de errores      |

---

## Pr√≥ximos Pasos

Ahora que has visto pipelines en acci√≥n, es tu turno de practicar:

1. **Ejecuta todos los ejemplos** en tu entorno local
2. **Modifica el c√≥digo**: Prueba diferentes transformaciones
3. **Contin√∫a con `03-EJERCICIOS.md`**: 15 ejercicios para dominar los conceptos
4. **Proyecto pr√°ctico**: Implementa tu propio pipeline con TDD

---

**¬°Felicidades!** Has completado los ejemplos del Tema 1. Ahora tienes las bases para dise√±ar pipelines robustos y escalables.

**√öltima actualizaci√≥n**: 2025-10-23
---

## üß≠ Navegaci√≥n

‚¨ÖÔ∏è **Anterior**: [01 Teoria](01-TEORIA.md) | ‚û°Ô∏è **Siguiente**: [03 Ejercicios](03-EJERCICIOS.md)
