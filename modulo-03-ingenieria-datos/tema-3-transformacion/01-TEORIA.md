# Tema 3: Transformaci√≥n de Datos con Pandas

**Duraci√≥n estimada**: 1-2 semanas
**Nivel**: Intermedio
**Prerrequisitos**: Python b√°sico, conceptos de ETL (Tema 1), extracci√≥n de datos (Tema 2)

---

## üéØ Objetivos de Aprendizaje

Al completar este tema, ser√°s capaz de:

1. **Manipular DataFrames y Series** con Pandas de forma eficiente
2. **Aplicar operaciones de transformaci√≥n** (filter, map, apply, lambda)
3. **Realizar agregaciones complejas** con GroupBy
4. **Combinar m√∫ltiples datasets** usando merge, join y concat
5. **Manejar valores nulos** y datos inconsistentes
6. **Optimizar el rendimiento** de transformaciones en grandes vol√∫menes de datos
7. **Construir pipelines de transformaci√≥n** limpios y mantenibles

---

## üìö Tabla de Contenidos

1. [Introducci√≥n a Pandas para Data Engineering](#1-introducci√≥n-a-pandas-para-data-engineering)
2. [DataFrames y Series](#2-dataframes-y-series)
3. [Lectura y Escritura de Archivos](#3-lectura-y-escritura-de-archivos)
4. [Operaciones de Transformaci√≥n](#4-operaciones-de-transformaci√≥n)
5. [GroupBy y Agregaciones](#5-groupby-y-agregaciones)
6. [Combinaci√≥n de Datasets](#6-combinaci√≥n-de-datasets)
7. [Manejo de Valores Nulos](#7-manejo-de-valores-nulos)
8. [Pivoting y Reshape](#8-pivoting-y-reshape)
9. [Optimizaci√≥n de Performance](#9-optimizaci√≥n-de-performance)
10. [Errores Comunes](#10-errores-comunes)
11. [Buenas Pr√°cticas](#11-buenas-pr√°cticas)

---

## 1. Introducci√≥n a Pandas para Data Engineering

### ¬øQu√© es Pandas?

**Pandas** es la librer√≠a de Python m√°s utilizada para manipulaci√≥n y an√°lisis de datos estructurados. En el contexto de Data Engineering, Pandas es la herramienta central para la **fase de Transformaci√≥n** en pipelines ETL/ELT.

### ¬øPor qu√© Pandas en Data Engineering?

**Ventajas:**
- **Expresividad**: Operaciones complejas en pocas l√≠neas de c√≥digo
- **Velocidad**: Implementado en C, muy r√°pido para operaciones vectorizadas
- **Integraci√≥n**: Compatible con m√∫ltiples formatos (CSV, JSON, Parquet, SQL)
- **Memoria eficiente**: Optimiza el uso de memoria con tipos de datos espec√≠ficos
- **Comunidad**: Amplia documentaci√≥n y ecosistema de herramientas

**Limitaciones:**
- **Memoria**: Todo en RAM (no ideal para datasets de +10GB)
- **Single-core**: No paraleliza autom√°ticamente (aunque existen alternativas como Dask)
- **Mutabilidad**: Puede generar copias no deseadas si no se usa correctamente

### Cu√°ndo usar Pandas vs SQL vs PySpark

| Herramienta | Cu√°ndo usar                                    | Tama√±o de datos          |
| ----------- | ---------------------------------------------- | ------------------------ |
| **Pandas**  | Transformaciones complejas, prototipos r√°pidos | < 5-10 GB                |
| **SQL**     | Agregaciones simples, joins, filtros           | Cualquier tama√±o (en DB) |
| **PySpark** | Big Data, procesamiento distribuido            | > 10 GB                  |

**Regla pr√°ctica**: Si cabe en memoria, usa Pandas. Si no, usa PySpark o divide en chunks.

---

## 2. DataFrames y Series

### Series: Estructuras Unidimensionales

Una **Series** es un array unidimensional con etiquetas (√≠ndices).

```python
import pandas as pd

# Crear una Series desde una lista
ventas = pd.Series([1500, 2300, 1800, 2100],
                   index=['Lunes', 'Martes', 'Mi√©rcoles', 'Jueves'])

print(ventas)
# Lunes        1500
# Martes       2300
# Mi√©rcoles    1800
# Jueves       2100
```

**Caracter√≠sticas clave:**
- Tiene un **√≠ndice** (labels) y **valores** (data)
- Todos los valores son del mismo tipo (homog√©neo)
- Se comporta como un diccionario + array de NumPy

### DataFrames: Estructuras Bidimensionales

Un **DataFrame** es una tabla bidimensional con filas y columnas etiquetadas.

```python
# Crear DataFrame desde un diccionario
datos = {
    'producto': ['Laptop', 'Mouse', 'Teclado', 'Monitor'],
    'precio': [850.0, 25.0, 75.0, 320.0],
    'stock': [15, 120, 45, 30],
    'categoria': ['Computadoras', 'Accesorios', 'Accesorios', 'Computadoras']
}

df = pd.DataFrame(datos)
print(df)
```

**Estructura de un DataFrame:**
```
      producto  precio  stock      categoria
0       Laptop   850.0     15  Computadoras
1        Mouse    25.0    120   Accesorios
2      Teclado    75.0     45   Accesorios
3      Monitor   320.0     30  Computadoras
```

**Componentes:**
- **Index**: Etiquetas de filas (0, 1, 2, 3 por defecto)
- **Columns**: Nombres de columnas ('producto', 'precio', etc.)
- **Values**: Matriz de datos (puede tener tipos mixtos por columna)

### Acceso a Datos

```python
# Acceder a una columna (devuelve Series)
df['precio']
df.precio  # Atajo (solo si el nombre no tiene espacios)

# Acceder a m√∫ltiples columnas (devuelve DataFrame)
df[['producto', 'precio']]

# Acceder a filas por √≠ndice
df.iloc[0]  # Primera fila (por posici√≥n)
df.loc[0]   # Fila con √≠ndice 0 (por etiqueta)

# Filtrado condicional
df[df['precio'] > 100]
```

### Informaci√≥n B√°sica del DataFrame

```python
# Dimensiones
df.shape  # (4, 4) - 4 filas, 4 columnas

# Tipos de datos
df.dtypes
# producto     object
# precio      float64
# stock         int64
# categoria    object

# Informaci√≥n general
df.info()

# Estad√≠sticas descriptivas
df.describe()

# Primeras/√∫ltimas filas
df.head(2)
df.tail(2)
```

---

## 3. Lectura y Escritura de Archivos

### Lectura de CSV

```python
# Lectura b√°sica
df = pd.read_csv('datos.csv')

# Con opciones comunes
df = pd.read_csv(
    'datos.csv',
    sep=';',                    # Separador personalizado
    encoding='utf-8',           # Codificaci√≥n
    decimal=',',                # Separador decimal
    thousands='.',              # Separador de miles
    na_values=['N/A', 'NULL'],  # Valores nulos personalizados
    parse_dates=['fecha'],      # Convertir columnas a fecha
    dtype={'codigo': str},      # Forzar tipos de datos
    usecols=['id', 'nombre'],   # Leer solo columnas espec√≠ficas
    nrows=1000                  # Leer solo N filas
)
```

**Best practices para CSV grandes:**

```python
# Leer en chunks (bloques)
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('datos_grandes.csv', chunksize=chunk_size):
    # Procesar cada chunk
    chunk_procesado = chunk[chunk['activo'] == True]
    chunks.append(chunk_procesado)

df_final = pd.concat(chunks, ignore_index=True)
```

### Lectura de otros formatos

```python
# Excel
df = pd.read_excel('datos.xlsx', sheet_name='Ventas')

# JSON
df = pd.read_json('datos.json')

# JSON Lines (cada l√≠nea es un JSON)
df = pd.read_json('datos.jsonl', lines=True)

# Parquet (formato columnar eficiente)
df = pd.read_parquet('datos.parquet')

# SQL
import sqlite3
conn = sqlite3.connect('database.db')
df = pd.read_sql_query('SELECT * FROM ventas WHERE a√±o = 2024', conn)

# Desde una URL
df = pd.read_csv('https://ejemplo.com/datos.csv')
```

### Escritura de Archivos

```python
# CSV
df.to_csv('salida.csv', index=False)  # index=False evita guardar el √≠ndice

# Excel
df.to_excel('salida.xlsx', sheet_name='Resultados', index=False)

# JSON
df.to_json('salida.json', orient='records', lines=True)

# Parquet (recomendado para almacenamiento)
df.to_parquet('salida.parquet', compression='snappy')

# SQL
df.to_sql('tabla_ventas', conn, if_exists='replace', index=False)
# if_exists: 'fail', 'replace', 'append'
```

---

## 4. Operaciones de Transformaci√≥n

### 4.1. Filtrado (Filter)

```python
# Filtro simple
df_filtrado = df[df['precio'] > 100]

# M√∫ltiples condiciones (AND)
df_filtrado = df[(df['precio'] > 50) & (df['stock'] > 10)]

# M√∫ltiples condiciones (OR)
df_filtrado = df[(df['categoria'] == 'Accesorios') | (df['precio'] < 30)]

# Negaci√≥n
df_filtrado = df[~(df['categoria'] == 'Computadoras')]

# isin() - pertenencia a una lista
categorias_deseadas = ['Accesorios', 'Perif√©ricos']
df_filtrado = df[df['categoria'].isin(categorias_deseadas)]

# str.contains() - b√∫squeda de texto
df_filtrado = df[df['producto'].str.contains('Laptop', case=False, na=False)]
```

### 4.2. Transformaci√≥n de Columnas

```python
# Crear nueva columna con c√°lculo
df['precio_con_iva'] = df['precio'] * 1.21

# Operaciones entre columnas
df['valor_inventario'] = df['precio'] * df['stock']

# Transformaci√≥n condicional con np.where
import numpy as np
df['tipo_precio'] = np.where(df['precio'] > 100, 'Caro', 'Econ√≥mico')

# M√∫ltiples condiciones con np.select
condiciones = [
    df['precio'] < 50,
    (df['precio'] >= 50) & (df['precio'] < 200),
    df['precio'] >= 200
]
valores = ['Econ√≥mico', 'Medio', 'Premium']
df['rango_precio'] = np.select(condiciones, valores, default='Desconocido')
```

### 4.3. Map - Mapeo de Valores

```python
# Mapeo con diccionario
mapeo_categorias = {
    'Computadoras': 'COMP',
    'Accesorios': 'ACC',
    'Perif√©ricos': 'PERI'
}
df['categoria_codigo'] = df['categoria'].map(mapeo_categorias)

# Map con funci√≥n
def calcular_descuento(precio: float) -> float:
    if precio > 500:
        return precio * 0.15
    elif precio > 200:
        return precio * 0.10
    return precio * 0.05

df['descuento'] = df['precio'].map(calcular_descuento)
```

### 4.4. Apply - Aplicar Funciones

```python
# Apply a una columna (Series)
df['precio_redondeado'] = df['precio'].apply(round)

# Apply a m√∫ltiples columnas (axis=1)
def calcular_urgencia_restock(row):
    if row['stock'] < 10:
        return 'URGENTE'
    elif row['stock'] < 30:
        return 'MEDIO'
    return 'NORMAL'

df['urgencia_restock'] = df.apply(calcular_urgencia_restock, axis=1)

# Apply con lambda (funciones an√≥nimas)
df['precio_formateado'] = df['precio'].apply(lambda x: f'${x:.2f}')

# Apply con argumentos adicionales
def aplicar_impuesto(precio: float, tasa_impuesto: float) -> float:
    return precio * (1 + tasa_impuesto)

df['precio_con_impuesto'] = df['precio'].apply(aplicar_impuesto, tasa_impuesto=0.21)
```

### 4.5. Operaciones Vectorizadas (M√°s Eficientes)

```python
# ‚ùå LENTO: Usar apply con operaciones simples
df['total'] = df.apply(lambda row: row['precio'] * row['cantidad'], axis=1)

# ‚úÖ R√ÅPIDO: Operaciones vectorizadas
df['total'] = df['precio'] * df['cantidad']

# ‚ùå LENTO: Bucle manual
totales = []
for idx, row in df.iterrows():
    totales.append(row['precio'] * row['cantidad'])
df['total'] = totales

# ‚úÖ R√ÅPIDO: Operaciones vectorizadas con NumPy
df['total'] = np.multiply(df['precio'].values, df['cantidad'].values)
```

**Regla de oro**: Siempre prefiere operaciones vectorizadas sobre `apply()`, y `apply()` sobre bucles `iterrows()`.

---

## 5. GroupBy y Agregaciones

### 5.1. GroupBy B√°sico

```python
# Agrupar por una columna y calcular suma
df.groupby('categoria')['precio'].sum()

# M√∫ltiples agregaciones
df.groupby('categoria')['precio'].agg(['sum', 'mean', 'count', 'min', 'max'])

# Agrupar por m√∫ltiples columnas
df.groupby(['categoria', 'marca'])['precio'].mean()
```

### 5.2. Agregaciones Personalizadas

```python
# Agregaciones con nombres personalizados
resultado = df.groupby('categoria').agg(
    precio_promedio=('precio', 'mean'),
    precio_total=('precio', 'sum'),
    cantidad_productos=('producto', 'count'),
    stock_total=('stock', 'sum')
).reset_index()

# Funci√≥n de agregaci√≥n personalizada
def rango_precios(serie):
    return serie.max() - serie.min()

df.groupby('categoria')['precio'].agg(['mean', rango_precios])
```

### 5.3. Transform - Mantener Dimensionalidad

```python
# Transform: devuelve un valor por cada fila original
df['precio_promedio_categoria'] = df.groupby('categoria')['precio'].transform('mean')

# Ahora cada fila tiene el promedio de su categor√≠a
# √ötil para calcular desviaciones, porcentajes, etc.
df['desviacion_precio'] = df['precio'] - df['precio_promedio_categoria']
df['porcentaje_del_promedio'] = (df['precio'] / df['precio_promedio_categoria']) * 100
```

### 5.4. Filter - Filtrar Grupos Completos

```python
# Mantener solo grupos con m√°s de 5 productos
df_filtrado = df.groupby('categoria').filter(lambda x: len(x) > 5)

# Mantener solo categor√≠as con precio promedio > 100
df_filtrado = df.groupby('categoria').filter(lambda x: x['precio'].mean() > 100)
```

---

## 6. Combinaci√≥n de Datasets

### 6.1. Concat - Concatenaci√≥n

```python
# Concatenar verticalmente (apilar filas)
df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})

df_concat = pd.concat([df1, df2], ignore_index=True)
# ignore_index=True resetea el √≠ndice

# Concatenar horizontalmente (apilar columnas)
df_concat = pd.concat([df1, df2], axis=1)
```

### 6.2. Merge - Uniones SQL-style

```python
# Datos de ejemplo
ventas = pd.DataFrame({
    'venta_id': [1, 2, 3],
    'producto_id': [101, 102, 103],
    'cantidad': [2, 1, 5]
})

productos = pd.DataFrame({
    'producto_id': [101, 102, 103],
    'nombre': ['Laptop', 'Mouse', 'Teclado'],
    'precio': [850, 25, 75]
})

# INNER JOIN (por defecto)
df_inner = pd.merge(ventas, productos, on='producto_id', how='inner')

# LEFT JOIN (mantiene todas las filas de la izquierda)
df_left = pd.merge(ventas, productos, on='producto_id', how='left')

# RIGHT JOIN (mantiene todas las filas de la derecha)
df_right = pd.merge(ventas, productos, on='producto_id', how='right')

# OUTER JOIN (mantiene todas las filas de ambos)
df_outer = pd.merge(ventas, productos, on='producto_id', how='outer')

# Merge con columnas de diferentes nombres
df_merge = pd.merge(
    ventas,
    productos,
    left_on='producto_id',
    right_on='id_producto'
)

# Merge con m√∫ltiples columnas
df_merge = pd.merge(
    df1,
    df2,
    on=['columna1', 'columna2'],
    how='inner'
)
```

### 6.3. Join - Uni√≥n por √çndice

```python
# Join por √≠ndice (similar a merge pero usa √≠ndices)
df1 = pd.DataFrame({'A': [1, 2]}, index=['x', 'y'])
df2 = pd.DataFrame({'B': [3, 4]}, index=['x', 'y'])

df_joined = df1.join(df2)

# Join con sufijos para columnas duplicadas
df_joined = df1.join(df2, how='left', lsuffix='_izq', rsuffix='_der')
```

### 6.4. Diferencias: Merge vs Join vs Concat

| Operaci√≥n    | Cu√°ndo usar                        | Comportamiento                    |
| ------------ | ---------------------------------- | --------------------------------- |
| **merge()**  | Unir por columnas comunes (llaves) | SQL-style joins                   |
| **join()**   | Unir por √≠ndices                   | Similar a merge pero usa √≠ndices  |
| **concat()** | Apilar DataFrames                  | Concatenaci√≥n vertical/horizontal |

---

## 7. Manejo de Valores Nulos

### 7.1. Detecci√≥n de Nulos

```python
# Verificar si hay nulos
df.isnull()  # DataFrame de booleanos
df.isnull().sum()  # Cantidad de nulos por columna

# Verificar si hay valores NO nulos
df.notnull()

# Filtrar filas con cualquier nulo
df_con_nulos = df[df.isnull().any(axis=1)]

# Filtrar filas sin nulos
df_sin_nulos = df.dropna()
```

### 7.2. Eliminaci√≥n de Nulos

```python
# Eliminar filas con cualquier nulo
df_clean = df.dropna()

# Eliminar filas donde TODAS las columnas son nulas
df_clean = df.dropna(how='all')

# Eliminar filas con nulos en columnas espec√≠ficas
df_clean = df.dropna(subset=['precio', 'stock'])

# Eliminar columnas con nulos
df_clean = df.dropna(axis=1)
```

### 7.3. Rellenado de Nulos

```python
# Rellenar con un valor constante
df['precio'].fillna(0, inplace=True)

# Rellenar con la media/mediana
df['precio'].fillna(df['precio'].mean(), inplace=True)
df['stock'].fillna(df['stock'].median(), inplace=True)

# Forward fill (propagar √∫ltimo valor v√°lido hacia adelante)
df['categoria'].fillna(method='ffill', inplace=True)

# Backward fill (propagar siguiente valor v√°lido hacia atr√°s)
df['categoria'].fillna(method='bfill', inplace=True)

# Rellenar con valores diferentes por columna
valores_relleno = {
    'precio': 0,
    'stock': df['stock'].median(),
    'categoria': 'Desconocido'
}
df.fillna(valores_relleno, inplace=True)
```

### 7.4. Interpolaci√≥n

```python
# Interpolaci√≥n lineal (√∫til para series temporales)
df['temperatura'].interpolate(method='linear', inplace=True)

# Interpolaci√≥n por tiempo
df['valor'].interpolate(method='time', inplace=True)
```

---

## 8. Pivoting y Reshape

### 8.1. Pivot - Tabla Din√°mica

```python
# Datos de ejemplo
ventas = pd.DataFrame({
    'fecha': ['2024-01', '2024-01', '2024-02', '2024-02'],
    'producto': ['Laptop', 'Mouse', 'Laptop', 'Mouse'],
    'ventas': [10, 50, 15, 60]
})

# Crear tabla din√°mica
pivot = ventas.pivot(index='fecha', columns='producto', values='ventas')
#         Laptop  Mouse
# 2024-01     10     50
# 2024-02     15     60
```

### 8.2. Pivot Table - Con Agregaciones

```python
# Pivot table con agregaci√≥n
pivot_table = pd.pivot_table(
    ventas,
    values='ventas',
    index='fecha',
    columns='producto',
    aggfunc='sum',  # Puede ser: 'mean', 'count', 'sum', etc.
    fill_value=0    # Reemplazar NaN con 0
)
```

### 8.3. Melt - Formato Largo

```python
# Convertir de formato ancho a largo
df_ancho = pd.DataFrame({
    'producto': ['Laptop', 'Mouse'],
    'ene': [10, 50],
    'feb': [15, 60]
})

df_largo = pd.melt(
    df_ancho,
    id_vars=['producto'],
    value_vars=['ene', 'feb'],
    var_name='mes',
    value_name='ventas'
)
#   producto mes  ventas
# 0   Laptop ene      10
# 1    Mouse ene      50
# 2   Laptop feb      15
# 3    Mouse feb      60
```

### 8.4. Stack y Unstack

```python
# Stack: columnas -> √≠ndice (hace el df m√°s largo)
df_stacked = df.stack()

# Unstack: √≠ndice -> columnas (hace el df m√°s ancho)
df_unstacked = df.unstack()
```

---

## 9. Optimizaci√≥n de Performance

### 9.1. Tipos de Datos Eficientes

```python
# Verificar uso de memoria
df.info(memory_usage='deep')

# Optimizar tipos num√©ricos
df['edad'] = df['edad'].astype('int8')  # Si valores 0-127
df['cantidad'] = df['cantidad'].astype('int16')  # Si valores 0-32,767

# Optimizar strings con category
df['categoria'] = df['categoria'].astype('category')
# Ahorro significativo si hay muchos valores repetidos
```

### 9.2. Operaciones Vectorizadas

```python
# ‚ùå LENTO
resultado = []
for idx, row in df.iterrows():
    resultado.append(row['a'] * row['b'])
df['resultado'] = resultado

# ‚úÖ R√ÅPIDO
df['resultado'] = df['a'] * df['b']
```

### 9.3. Procesamiento en Chunks

```python
# Para archivos grandes
def procesar_chunk(chunk):
    chunk['total'] = chunk['precio'] * chunk['cantidad']
    return chunk[chunk['total'] > 1000]

chunks_procesados = []
for chunk in pd.read_csv('archivo_grande.csv', chunksize=10000):
    chunks_procesados.append(procesar_chunk(chunk))

df_final = pd.concat(chunks_procesados, ignore_index=True)
```

### 9.4. eval() y query() para Expresiones

```python
# Para operaciones complejas, eval() puede ser m√°s r√°pido
df['resultado'] = df.eval('(precio * cantidad) + (precio * 0.21)')

# query() para filtrado
df_filtrado = df.query('precio > 100 and stock < 50')
```

### 9.5. Evitar Copias Innecesarias

```python
# ‚ùå Crea copias
df_temp = df
df_temp['nueva_col'] = 0  # Modifica el original tambi√©n

# ‚úÖ Copia expl√≠cita
df_temp = df.copy()
df_temp['nueva_col'] = 0  # No afecta al original

# ‚úÖ Operaci√≥n in-place (sin copia)
df.drop('columna', axis=1, inplace=True)
```

---

## 10. Errores Comunes

### Error 1: SettingWithCopyWarning

```python
# ‚ùå INCORRECTO - Advertencia de copia
df_filtrado = df[df['precio'] > 100]
df_filtrado['descuento'] = 0.1  # ‚ö†Ô∏è Warning!

# ‚úÖ CORRECTO - Copia expl√≠cita
df_filtrado = df[df['precio'] > 100].copy()
df_filtrado['descuento'] = 0.1
```

### Error 2: Modificar Durante Iteraci√≥n

```python
# ‚ùå INCORRECTO
for idx, row in df.iterrows():
    df.loc[idx, 'nueva_col'] = row['precio'] * 2

# ‚úÖ CORRECTO - Operaci√≥n vectorizada
df['nueva_col'] = df['precio'] * 2
```

### Error 3: Comparaci√≥n de Strings sin Normalizar

```python
# ‚ùå INCORRECTO - Sensible a may√∫sculas/espacios
df_filtrado = df[df['categoria'] == 'accesorios']

# ‚úÖ CORRECTO
df_filtrado = df[df['categoria'].str.lower().str.strip() == 'accesorios']
```

### Error 4: No Manejar Valores Nulos en Comparaciones

```python
# ‚ùå INCORRECTO - NaN no se compara correctamente
df_filtrado = df[df['precio'] > 100]  # Descarta NaN

# ‚úÖ CORRECTO - Manejar nulos expl√≠citamente
df_filtrado = df[(df['precio'] > 100) | (df['precio'].isnull())]
```

### Error 5: Merge sin Verificar Duplicados

```python
# ‚ùå RIESGO - Puede duplicar filas inesperadamente
df_merged = pd.merge(ventas, productos, on='producto_id')

# ‚úÖ CORRECTO - Verificar antes
print(f"Filas antes: {len(ventas)}")
df_merged = pd.merge(ventas, productos, on='producto_id', validate='m:1')
print(f"Filas despu√©s: {len(df_merged)}")
# validate='m:1' asegura que productos no tenga duplicados
```

---

## 11. Buenas Pr√°cticas

### 1. Nombres Descriptivos y Consistentes

```python
# ‚ùå EVITAR
df1 = pd.read_csv('data.csv')
df2 = df1[df1['x'] > 10]
df3 = df2.groupby('y').sum()

# ‚úÖ MEJOR
ventas_raw = pd.read_csv('ventas.csv')
ventas_filtradas = ventas_raw[ventas_raw['precio'] > 10]
ventas_por_categoria = ventas_filtradas.groupby('categoria').sum()
```

### 2. Validar Datos de Entrada

```python
def transformar_ventas(df: pd.DataFrame) -> pd.DataFrame:
    """Transforma datos de ventas."""
    # Validaciones
    columnas_requeridas = ['producto_id', 'precio', 'cantidad']
    columnas_faltantes = set(columnas_requeridas) - set(df.columns)

    if columnas_faltantes:
        raise ValueError(f"Columnas faltantes: {columnas_faltantes}")

    if df.empty:
        raise ValueError("DataFrame vac√≠o")

    # Transformaciones
    df_transformado = df.copy()
    df_transformado['total'] = df_transformado['precio'] * df_transformado['cantidad']

    return df_transformado
```

### 3. Documentar Transformaciones Complejas

```python
def calcular_metricas_cliente(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calcula m√©tricas agregadas por cliente.

    Args:
        df: DataFrame con columnas ['cliente_id', 'fecha', 'monto']

    Returns:
        DataFrame con m√©tricas por cliente:
        - total_compras: suma de montos
        - promedio_compra: promedio de montos
        - num_compras: cantidad de compras
        - ultima_compra: fecha m√°s reciente
    """
    metricas = df.groupby('cliente_id').agg(
        total_compras=('monto', 'sum'),
        promedio_compra=('monto', 'mean'),
        num_compras=('monto', 'count'),
        ultima_compra=('fecha', 'max')
    ).reset_index()

    return metricas
```

### 4. Usar Method Chaining para Pipelines

```python
# ‚úÖ Pipeline claro y legible
df_procesado = (
    df
    .dropna(subset=['precio', 'cantidad'])
    .assign(total=lambda x: x['precio'] * x['cantidad'])
    .query('total > 100')
    .groupby('categoria')
    .agg({'total': 'sum', 'cantidad': 'sum'})
    .reset_index()
    .sort_values('total', ascending=False)
)
```

### 5. Separar L√≥gica de Negocio de Transformaciones

```python
# ‚ùå EVITAR - Todo mezclado
def procesar():
    df = pd.read_csv('ventas.csv')
    df['total'] = df['precio'] * df['cantidad']
    df = df[df['total'] > 100]
    df.to_csv('resultado.csv')

# ‚úÖ MEJOR - Funciones separadas
def cargar_ventas(path: str) -> pd.DataFrame:
    return pd.read_csv(path)

def calcular_total(df: pd.DataFrame) -> pd.DataFrame:
    df_copia = df.copy()
    df_copia['total'] = df_copia['precio'] * df_copia['cantidad']
    return df_copia

def filtrar_ventas_mayores(df: pd.DataFrame, umbral: float) -> pd.DataFrame:
    return df[df['total'] > umbral]

def guardar_resultado(df: pd.DataFrame, path: str) -> None:
    df.to_csv(path, index=False)

# Orquestaci√≥n clara
def pipeline_ventas():
    df = cargar_ventas('ventas.csv')
    df = calcular_total(df)
    df = filtrar_ventas_mayores(df, umbral=100)
    guardar_resultado(df, 'resultado.csv')
```

### 6. Manejo Expl√≠cito de Errores

```python
def procesar_ventas(path: str) -> pd.DataFrame:
    """Procesa archivo de ventas con manejo de errores."""
    try:
        df = pd.read_csv(path)
    except FileNotFoundError:
        raise FileNotFoundError(f"Archivo no encontrado: {path}")
    except pd.errors.EmptyDataError:
        raise ValueError(f"Archivo vac√≠o: {path}")

    if 'precio' not in df.columns:
        raise ValueError("Columna 'precio' no encontrada en el archivo")

    # Validar tipos de datos
    if not pd.api.types.is_numeric_dtype(df['precio']):
        raise TypeError("La columna 'precio' debe ser num√©rica")

    return df
```

### 7. Testing de Transformaciones

```python
import pytest

def test_calcular_total():
    """Test de la funci√≥n calcular_total."""
    # Arrange
    df_input = pd.DataFrame({
        'precio': [100, 200],
        'cantidad': [2, 3]
    })

    # Act
    df_resultado = calcular_total(df_input)

    # Assert
    assert 'total' in df_resultado.columns
    assert df_resultado['total'].tolist() == [200, 600]
    assert len(df_resultado) == 2
```

---

## üìù Resumen

En este tema has aprendido:

‚úÖ **DataFrames y Series**: Estructuras fundamentales de Pandas
‚úÖ **Operaciones de transformaci√≥n**: filter, map, apply, lambda
‚úÖ **GroupBy y agregaciones**: C√°lculos por grupos
‚úÖ **Combinaci√≥n de datasets**: merge, join, concat
‚úÖ **Manejo de valores nulos**: Detecci√≥n, eliminaci√≥n y rellenado
‚úÖ **Pivoting y reshape**: Cambiar estructura de datos
‚úÖ **Optimizaci√≥n**: Tipos eficientes, vectorizaci√≥n, chunks
‚úÖ **Buenas pr√°cticas**: C√≥digo limpio, mantenible y testeable

### Pr√≥ximos Pasos

1. Revisa los **ejemplos pr√°cticos** en `02-EJEMPLOS.md`
2. Practica con los **ejercicios** en `03-EJERCICIOS.md`
3. Construye el **proyecto pr√°ctico** en `04-proyecto-practico/`

---

**Tiempo de lectura estimado**: 45-60 minutos
**Palabras**: ~4,500
**√öltima actualizaci√≥n**: 2025-10-30
