# Proyecto PrÃ¡ctico: Conversor Multi-formato

**MÃ³dulo 3 - Tema 5**: Formatos de Datos Modernos
**VersiÃ³n**: 1.0.0
**MetodologÃ­a**: Test-Driven Development (TDD)

---

## ğŸ“‹ DescripciÃ³n

Framework completo para conversiÃ³n entre formatos de datos (CSV, JSON, JSON Lines, Parquet), gestiÃ³n de compresiÃ³n y anÃ¡lisis de formatos. DiseÃ±ado para optimizar pipelines de datos en Data Engineering.

### CaracterÃ­sticas Principales

- âœ… **ConversiÃ³n universal** entre CSV, JSON, JSON Lines y Parquet
- âœ… **CompresiÃ³n y descompresiÃ³n** con mÃºltiples algoritmos (gzip, bz2, xz)
- âœ… **AnÃ¡lisis y benchmarking** de formatos y compresiones
- âœ… **Particionamiento** de datos para Big Data
- âœ… **AutodetecciÃ³n** de formatos
- âœ… **Cobertura de tests** >85%
- âœ… **Tipado explÃ­cito** con type hints
- âœ… **DocumentaciÃ³n completa** con docstrings

---

## ğŸ—ï¸ Estructura del Proyecto

```
04-proyecto-practico/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conversor_formatos.py        # 8 funciones de conversiÃ³n
â”‚   â”œâ”€â”€ gestor_compresion.py          # 4 funciones de compresiÃ³n
â”‚   â””â”€â”€ analizador_formatos.py        # 5 funciones de anÃ¡lisis
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                   # Fixtures compartidas
â”‚   â”œâ”€â”€ test_conversor_formatos.py   # 26 tests
â”‚   â”œâ”€â”€ test_gestor_compresion.py     # 17 tests
â”‚   â””â”€â”€ test_analizador_formatos.py   # 15 tests
â”œâ”€â”€ datos/
â”‚   â”œâ”€â”€ ventas_ejemplo.csv            # Datos de ejemplo
â”‚   â””â”€â”€ pedidos.json                  # JSON nested
â”œâ”€â”€ ejemplos/
â”‚   â””â”€â”€ ejemplo_pipeline_completo.py  # Pipeline completo
â”œâ”€â”€ htmlcov/                          # Reporte de cobertura
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pytest.ini
â””â”€â”€ README.md                         # Este archivo
```

---

## ğŸš€ Inicio RÃ¡pido

### 1. InstalaciÃ³n

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# En Windows:
venv\Scripts\activate
# En Linux/Mac:
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt
```

### 2. Ejecutar Tests

```bash
# Todos los tests
pytest tests/ -v

# Con cobertura
pytest tests/ --cov=src --cov-report=term-missing --cov-report=html

# Tests especÃ­ficos
pytest tests/test_conversor_formatos.py -v

# Ver cobertura en HTML
# Abrir: htmlcov/index.html
```

### 3. Uso BÃ¡sico

```python
from src.conversor_formatos import convertir_csv_a_parquet, leer_multiple_formatos
from src.gestor_compresion import comparar_compresiones
from src.analizador_formatos import generar_reporte_formato

# Convertir CSV a Parquet
convertir_csv_a_parquet('datos.csv', 'datos.parquet', compresion='snappy')

# Leer cualquier formato automÃ¡ticamente
df = leer_multiple_formatos('datos.csv')  # o .json, .jsonl, .parquet

# Comparar algoritmos de compresiÃ³n
resultados = comparar_compresiones('datos.csv', ['gzip', 'bz2', 'xz'])

# Generar reporte de un archivo
reporte = generar_reporte_formato('datos.parquet')
print(reporte)
```

---

## ğŸ“š MÃ³dulos

### 1. conversor_formatos.py

Conversiones entre formatos de datos.

#### Funciones

**`convertir_csv_a_parquet(ruta_csv, ruta_parquet, compresion='snappy')`**

Convierte CSV a Parquet con compresiÃ³n.

```python
convertir_csv_a_parquet('ventas.csv', 'ventas.parquet', compresion='snappy')
```

**`convertir_json_a_parquet(ruta_json, ruta_parquet, orient='records')`**

Convierte JSON o JSON Lines a Parquet.

```python
convertir_json_a_parquet('pedidos.json', 'pedidos.parquet')
convertir_json_a_parquet('logs.jsonl', 'logs.parquet', orient='lines')
```

**`convertir_parquet_a_csv(ruta_parquet, ruta_csv)`**

Convierte Parquet a CSV.

```python
convertir_parquet_a_csv('datos.parquet', 'datos.csv')
```

**`convertir_json_a_csv(ruta_json, ruta_csv)`**

Convierte JSON a CSV.

```python
convertir_json_a_csv('datos.json', 'datos.csv')
```

**`convertir_csv_a_json_lines(ruta_csv, ruta_jsonl)`**

Convierte CSV a JSON Lines.

```python
convertir_csv_a_json_lines('ventas.csv', 'ventas.jsonl')
```

**`leer_multiple_formatos(ruta) â†’ pd.DataFrame`**

Lee archivo detectando formato automÃ¡ticamente.

```python
df = leer_multiple_formatos('datos.csv')      # CSV
df = leer_multiple_formatos('datos.json')     # JSON
df = leer_multiple_formatos('datos.jsonl')    # JSON Lines
df = leer_multiple_formatos('datos.parquet')  # Parquet
```

**`guardar_formato_automatico(df, ruta, formato=None)`**

Guarda DataFrame detectando formato por extensiÃ³n.

```python
guardar_formato_automatico(df, 'salida.csv')
guardar_formato_automatico(df, 'salida.parquet')
guardar_formato_automatico(df, 'salida.datos', formato='parquet')
```

**`convertir_con_particiones(df, ruta_base, columnas_particion, formato='parquet')`**

Guarda DataFrame particionado (solo Parquet).

```python
df['aÃ±o'] = df['fecha'].dt.year
df['mes'] = df['fecha'].dt.month
convertir_con_particiones(df, 'ventas/', ['aÃ±o', 'mes'])
```

---

### 2. gestor_compresion.py

GestiÃ³n de compresiÃ³n y descompresiÃ³n de archivos.

#### Funciones

**`comprimir_archivo(ruta_entrada, algoritmo='gzip') â†’ str`**

Comprime archivo con algoritmo especificado.

```python
ruta_comprimida = comprimir_archivo('datos.csv', algoritmo='gzip')
# Retorna: 'datos.csv.gz'
```

**`descomprimir_archivo(ruta_comprimida) â†’ str`**

Descomprime archivo detectando formato automÃ¡ticamente.

```python
ruta_descomprimida = descomprimir_archivo('datos.csv.gz')
# Retorna: 'datos.csv'
```

**`comparar_compresiones(ruta_archivo, algoritmos) â†’ Dict`**

Compara mÃºltiples algoritmos de compresiÃ³n.

```python
resultados = comparar_compresiones('datos.csv', ['gzip', 'bz2', 'xz'])

# Retorna:
# {
#   'gzip': {
#     'tamanio_original_kb': 120.5,
#     'tamanio_comprimido_kb': 25.3,
#     'ratio_compresion': 4.76,
#     'reduccion_pct': 79.0,
#     'tiempo_compresion_s': 0.0432
#   },
#   ...
# }
```

**`comprimir_dataframe_memoria(df, algoritmo='gzip') â†’ bytes`**

Comprime DataFrame en memoria sin escribir a disco.

```python
bytes_comprimidos = comprimir_dataframe_memoria(df, algoritmo='gzip')

# Para descomprimir:
import gzip
import io
buffer = io.BytesIO(gzip.decompress(bytes_comprimidos))
df_reconstruido = pd.read_csv(buffer)
```

---

### 3. analizador_formatos.py

AnÃ¡lisis y comparaciÃ³n de formatos de datos.

#### Funciones

**`detectar_formato_archivo(ruta) â†’ str`**

Detecta formato de archivo automÃ¡ticamente.

```python
formato = detectar_formato_archivo('datos.csv')      # â†’ 'csv'
formato = detectar_formato_archivo('datos.parquet')  # â†’ 'parquet'
formato = detectar_formato_archivo('logs.jsonl')     # â†’ 'jsonl'
```

**`obtener_metadata_parquet(ruta) â†’ Dict`**

Obtiene metadata detallada de archivo Parquet.

```python
metadata = obtener_metadata_parquet('datos.parquet')

# Retorna:
# {
#   'num_filas': 1000,
#   'num_columnas': 5,
#   'columnas': ['id', 'nombre', 'edad', ...],
#   'tipos_datos': {'id': 'int64', 'nombre': 'object', ...},
#   'tamanio_archivo_mb': 1.25,
#   'compresion': 'SNAPPY',
#   'num_row_groups': 1
# }
```

**`comparar_tamanios_formatos(df) â†’ Dict[str, float]`**

Compara tamaÃ±os de DataFrame en diferentes formatos.

```python
tamanios = comparar_tamanios_formatos(df)

# Retorna:
# {
#   'csv': 120.5,              # KB
#   'json': 185.2,
#   'jsonl': 185.2,
#   'parquet_snappy': 15.3,
#   'parquet_gzip': 12.8
# }
```

**`benchmark_lectura_escritura(df, formatos) â†’ pd.DataFrame`**

Realiza benchmark de lectura/escritura para diferentes formatos.

```python
resultados = benchmark_lectura_escritura(df, ['csv', 'json', 'parquet'])

# Retorna DataFrame:
#   formato  tiempo_escritura_s  tiempo_lectura_s  tamanio_kb
#   csv      0.8500              1.2000            120.50
#   json     1.9500              2.8500            185.20
#   parquet  0.7200              0.3800             15.30
```

**`generar_reporte_formato(ruta) â†’ Dict`**

Genera reporte completo de un archivo de datos.

```python
reporte = generar_reporte_formato('datos.parquet')

# Retorna:
# {
#   'formato': 'parquet',
#   'tamanio_mb': 1.25,
#   'num_registros': 1000,
#   'num_columnas': 5,
#   'columnas': ['id', 'nombre', 'edad', 'salario', 'activo'],
#   'tipos_datos': {'id': 'int64', ...},
#   'compresion': 'SNAPPY',
#   'metadata_parquet': {...}
# }
```

---

## ğŸ§ª Testing

### Cobertura de Tests

- **conversor_formatos.py**: 26 tests (~95% cobertura)
- **gestor_compresion.py**: 17 tests (~92% cobertura)
- **analizador_formatos.py**: 15 tests (~90% cobertura)

**Total**: 58 tests, >90% cobertura global

### Ejecutar Tests

```bash
# Todos los tests con cobertura
pytest tests/ --cov=src --cov-report=html

# Solo tests de conversor
pytest tests/test_conversor_formatos.py -v

# Solo tests de compresiÃ³n
pytest tests/test_gestor_compresion.py -v

# Solo tests de analizador
pytest tests/test_analizador_formatos.py -v

# Tests con marcadores
pytest -m "not slow"  # Excluir tests lentos
```

### Fixtures Disponibles

Definidas en `tests/conftest.py`:

- `df_ejemplo`: DataFrame de 100 registros
- `df_vacio`: DataFrame vacÃ­o
- `directorio_temporal`: Directorio temporal para tests
- `archivo_csv_temporal`: CSV temporal
- `archivo_json_temporal`: JSON temporal
- `archivo_jsonl_temporal`: JSON Lines temporal
- `archivo_parquet_temporal`: Parquet temporal
- `df_con_particiones`: DataFrame con columnas de particiÃ³n
- `df_json_nested`: Datos JSON con estructura nested

---

## ğŸ“Š Ejemplos de Uso

### Ejemplo 1: Pipeline ETL Completo

```python
from src.conversor_formatos import leer_multiple_formatos, convertir_con_particiones
from src.gestor_compresion import comprimir_archivo
import pandas as pd

# 1. Extraer de mÃºltiples fuentes
df_ventas = leer_multiple_formatos('ventas.csv')
df_clientes = leer_multiple_formatos('clientes.json')

# 2. Transformar
df = pd.merge(df_ventas, df_clientes, on='cliente_id')
df['fecha'] = pd.to_datetime(df['fecha'])
df['aÃ±o'] = df['fecha'].dt.year
df['mes'] = df['fecha'].dt.month

# 3. Cargar particionado
convertir_con_particiones(df, 'data_lake/ventas/', ['aÃ±o', 'mes'])

print("Pipeline completado âœ…")
```

### Ejemplo 2: Benchmark de Formatos

```python
from src.analizador_formatos import benchmark_lectura_escritura, comparar_tamanios_formatos
import pandas as pd

# Generar datos de prueba
df = pd.DataFrame({
    'id': range(10000),
    'valor': range(10000, 20000)
})

# Comparar tamaÃ±os
tamanios = comparar_tamanios_formatos(df)
print("TamaÃ±os:", tamanios)

# Benchmark lectura/escritura
benchmark = benchmark_lectura_escritura(df, ['csv', 'json', 'parquet'])
print(benchmark)
```

### Ejemplo 3: OptimizaciÃ³n de CompresiÃ³n

```python
from src.gestor_compresion import comparar_compresiones

# Comparar algoritmos
resultados = comparar_compresiones('datos_grandes.csv', ['gzip', 'bz2', 'xz'])

for algoritmo, metricas in resultados.items():
    print(f"{algoritmo}:")
    print(f"  ReducciÃ³n: {metricas['reduccion_pct']}%")
    print(f"  Tiempo: {metricas['tiempo_compresion_s']}s")
```

---

## ğŸ¯ Casos de Uso

### 1. MigraciÃ³n de Sistema Legacy (CSV) a Data Lake (Parquet)

```python
from src.conversor_formatos import convertir_csv_a_parquet

# Convertir todos los CSV histÃ³ricos
archivos_csv = ['ventas_2022.csv', 'ventas_2023.csv', 'ventas_2024.csv']

for archivo in archivos_csv:
    convertir_csv_a_parquet(archivo, archivo.replace('.csv', '.parquet'))
```

### 2. OptimizaciÃ³n de Almacenamiento

```python
from src.analizador_formatos import comparar_tamanios_formatos
import pandas as pd

df = pd.read_csv('datos_grandes.csv')
tamanios = comparar_tamanios_formatos(df)

print(f"CSV: {tamanios['csv']} KB")
print(f"Parquet: {tamanios['parquet_snappy']} KB")
print(f"Ahorro: {((tamanios['csv'] - tamanios['parquet_snappy']) / tamanios['csv'] * 100):.1f}%")
```

### 3. Pipeline de Logs

```python
from src.conversor_formatos import convertir_csv_a_json_lines

# Logs de CSV a JSON Lines para streaming
convertir_csv_a_json_lines('logs_app.csv', 'logs_app.jsonl')
```

---

## ğŸ”§ ConfiguraciÃ³n

### pytest.ini

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_functions = test_*
addopts =
    -v
    --strict-markers
    --cov=src
    --cov-report=term-missing
    --cov-report=html
    --cov-fail-under=85
```

### requirements.txt

```
pandas>=2.0.0
numpy>=1.24.0
pyarrow>=12.0.0
pytest>=7.4.0
pytest-cov>=4.1.0
```

---

## ğŸš¨ Manejo de Errores

Todas las funciones siguen estas convenciones:

- **ValidaciÃ³n de inputs**: Verificar que archivos existen, DataFrames no vacÃ­os
- **Excepciones especÃ­ficas**: `FileNotFoundError`, `ValueError`, `TypeError`
- **Mensajes claros**: Indicar exactamente quÃ© fallÃ³ y por quÃ©
- **Sin fallos silenciosos**: Siempre lanzar excepciÃ³n en errores

```python
def ejemplo_funcion(ruta: str) -> pd.DataFrame:
    """Ejemplo con validaciones."""
    if not os.path.exists(ruta):
        raise FileNotFoundError(f"Archivo no encontrado: {ruta}")

    df = pd.read_csv(ruta)

    if df.empty:
        raise ValueError("El archivo estÃ¡ vacÃ­o")

    return df
```

---

## ğŸ” Consideraciones de Seguridad

- **ValidaciÃ³n de paths**: Verificar que rutas no contengan caracteres peligrosos
- **LÃ­mites de tamaÃ±o**: Verificar que archivos no excedan lÃ­mites razonables
- **SanitizaciÃ³n**: Limpiar inputs antes de usar en operaciones de archivo
- **Permisos**: Verificar permisos de lectura/escritura antes de operar
- **Logging**: Registrar todas las operaciones importantes

---

## ğŸ“ Mejores PrÃ¡cticas

1. **Usa Parquet para almacenamiento**: Mejor compresiÃ³n y velocidad
2. **Particiona datasets grandes**: >1 GB debe estar particionado
3. **Comprime datos en reposo**: Usa snappy para balance, gzip para mÃ¡xima compresiÃ³n
4. **Lectura selectiva con Parquet**: Usa `columns` y `filters`
5. **Testea conversiones**: Verifica integridad despuÃ©s de convertir
6. **Documenta schemas**: MantÃ©n clara la estructura de datos
7. **Versiona formatos**: MantÃ©n histÃ³rico de cambios en estructura

---

## ğŸ“ Convenciones de CÃ³digo

- **Nombres en espaÃ±ol**: Funciones, variables, comentarios
- **Snake_case**: Funciones y variables
- **PascalCase**: Clases (si las hubiera)
- **Type hints**: Siempre en parÃ¡metros y retornos
- **Docstrings**: En espaÃ±ol con Args, Returns, Raises
- **Sin tildes en cÃ³digo**: Solo en comentarios y docstrings
- **Imports ordenados**: stdlib, third-party, local

---

## ğŸ“ Soporte

Para preguntas o problemas:
- Revisar ejemplos en `ejemplos/`
- Consultar tests en `tests/`
- Ver documentaciÃ³n en archivos fuente

---

## ğŸ“„ Licencia

Este proyecto es parte del Master en IngenierÃ­a de Datos.
Uso educativo exclusivamente.

---

*Ãšltima actualizaciÃ³n: 2025-10-30*
---

## ğŸ§­ NavegaciÃ³n

â¬…ï¸ **Anterior**: [03 Ejercicios](../03-EJERCICIOS.md) | â¡ï¸ **Siguiente**: [Carga y Pipelines - 01 Teoria](../../tema-6-carga-pipelines/01-TEORIA.md)
