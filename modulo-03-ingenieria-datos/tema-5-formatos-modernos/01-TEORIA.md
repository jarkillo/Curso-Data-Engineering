# Tema 5: Formatos de Datos Modernos

**Duraci√≥n estimada**: 1-2 semanas
**Nivel**: Intermedio-Avanzado
**Prerrequisitos**: Python, Pandas, conceptos de ETL, transformaci√≥n de datos

---

## üéØ Objetivos de Aprendizaje

Al completar este tema, ser√°s capaz de:

1. **Comprender formatos de datos modernos** y sus diferencias clave (JSON, Parquet, Avro)
2. **Trabajar con JSON y JSON Lines** para APIs, streaming y logs
3. **Utilizar Parquet** para almacenamiento columnar eficiente
4. **Aplicar compresi√≥n** (gzip, snappy, zstd) seg√∫n el caso de uso
5. **Particionar datos grandes** para optimizar consultas
6. **Elegir el formato adecuado** seg√∫n requisitos de rendimiento y escalabilidad
7. **Convertir entre formatos** de manera eficiente y segura

---

## üìö Tabla de Contenidos

1. [Introducci√≥n a los Formatos Modernos](#1-introducci√≥n-a-los-formatos-modernos)
2. [JSON y JSON Lines](#2-json-y-json-lines)
3. [Parquet: Almacenamiento Columnar](#3-parquet-almacenamiento-columnar)
4. [Avro: Schemas Evolutivos](#4-avro-schemas-evolutivos)
5. [Comparaci√≥n de Formatos](#5-comparaci√≥n-de-formatos)
6. [Compresi√≥n de Datos](#6-compresi√≥n-de-datos)
7. [Particionamiento de Datos](#7-particionamiento-de-datos)
8. [Cu√°ndo Usar Cada Formato](#8-cu√°ndo-usar-cada-formato)
9. [Errores Comunes](#9-errores-comunes)
10. [Buenas Pr√°cticas](#10-buenas-pr√°cticas)

---

## 1. Introducci√≥n a los Formatos Modernos

### ¬øPor qu√© son importantes los formatos de datos?

En Data Engineering, la **elecci√≥n del formato de datos** es una decisi√≥n arquitect√≥nica cr√≠tica que afecta:

- **Rendimiento**: Velocidad de lectura/escritura
- **Almacenamiento**: Tama√±o en disco (costos de infraestructura)
- **Escalabilidad**: Capacidad de manejar datasets grandes
- **Compatibilidad**: Interoperabilidad entre sistemas
- **Mantenibilidad**: Facilidad para evolucionar esquemas

### Evoluci√≥n de los Formatos

**1. CSV (1972)**: Simple, universal, pero limitado
- ‚úÖ F√°cil de leer, compatible universalmente
- ‚ùå Sin tipos de datos, sin compresi√≥n eficiente, lento

**2. JSON (2001)**: Estructuras anidadas, legible
- ‚úÖ Flexible, soporta nested data, legible
- ‚ùå Verboso, consumo alto de memoria, lento para grandes vol√∫menes

**3. Parquet (2013)**: Columnar, eficiente, comprimido
- ‚úÖ Altamente comprimido, lectura selectiva, esquema embebido
- ‚ùå No legible por humanos, overhead de escritura

**4. Avro (2009)**: Binario, schema evolutivo
- ‚úÖ Compacto, evoluci√≥n de schemas, r√°pido en streaming
- ‚ùå Menos adoptado que Parquet, necesita decodificador

### El Problema con CSV

CSV ha sido el formato est√°ndar durante d√©cadas, pero tiene **limitaciones cr√≠ticas**:

```csv
id,nombre,fecha,precio,activo
1,Producto A,2024-01-15,99.99,true
2,Producto B,2024-02-20,149.50,false
```

**Problemas**:
1. **Sin tipos**: Todo es texto, necesita parsing manual
2. **Sin schema**: No hay garant√≠a de estructura
3. **Delimitadores**: Problemas con comas en datos
4. **Sin compresi√≥n**: Archivos grandes ocupan mucho espacio
5. **Sin nested data**: Dif√≠cil representar estructuras complejas
6. **Performance**: Leer todo el archivo para filtrar una columna

Por esto, los formatos modernos son esenciales en Data Engineering.

---

## 2. JSON y JSON Lines

### 2.1. JSON Est√°ndar

**JSON (JavaScript Object Notation)** es el formato m√°s usado para APIs y configuraci√≥n.

**Estructura**:

```json
{
  "pedidos": [
    {
      "pedido_id": 1001,
      "cliente": "Juan P√©rez",
      "fecha": "2024-01-15",
      "total": 250.50,
      "items": [
        {"producto": "Laptop", "cantidad": 1, "precio": 200.00},
        {"producto": "Mouse", "cantidad": 2, "precio": 25.25}
      ]
    },
    {
      "pedido_id": 1002,
      "cliente": "Mar√≠a L√≥pez",
      "fecha": "2024-01-16",
      "total": 80.00,
      "items": [
        {"producto": "Teclado", "cantidad": 1, "precio": 80.00}
      ]
    }
  ]
}
```

**Ventajas**:
- ‚úÖ **Legible**: F√°cil de leer y debuggear
- ‚úÖ **Flexible**: Soporta estructuras nested y arrays
- ‚úÖ **Tipado**: Distingue n√∫meros, strings, booleanos, nulls
- ‚úÖ **Universal**: Todos los lenguajes lo soportan

**Desventajas**:
- ‚ùå **Verboso**: Claves repetidas ocupan espacio
- ‚ùå **Memoria**: Debe cargarse completo en memoria
- ‚ùå **Lento**: Parsing es costoso para archivos grandes
- ‚ùå **No streaming**: No se puede procesar l√≠nea por l√≠nea

### 2.2. JSON Lines (JSONL / NDJSON)

**JSON Lines** es JSON con **un objeto por l√≠nea**, ideal para streaming y logs.

**Estructura**:

```jsonl
{"pedido_id": 1001, "cliente": "Juan P√©rez", "fecha": "2024-01-15", "total": 250.50}
{"pedido_id": 1002, "cliente": "Mar√≠a L√≥pez", "fecha": "2024-01-16", "total": 80.00}
{"pedido_id": 1003, "cliente": "Ana Garc√≠a", "fecha": "2024-01-17", "total": 120.00}
```

**Ventajas sobre JSON est√°ndar**:
- ‚úÖ **Streaming**: Procesar l√≠nea por l√≠nea sin cargar todo
- ‚úÖ **Append-friendly**: A√±adir registros al final es trivial
- ‚úÖ **Robusto**: Un registro corrupto no afecta los dem√°s
- ‚úÖ **Compresi√≥n**: Se comprime muy bien con gzip

**Casos de uso**:
- Logs de aplicaciones
- Exportaci√≥n de datos de APIs (paginadas)
- Streaming de eventos en tiempo real
- Archivos de datos grandes que no caben en memoria

### 2.3. Trabajar con JSON en Pandas

**Leer JSON est√°ndar**:

```python
import pandas as pd

# JSON con array de objetos
df = pd.read_json('datos.json', orient='records')

# JSON con estructura anidada
df = pd.read_json('datos.json')
df_normalizado = pd.json_normalize(df['pedidos'])
```

**Leer JSON Lines**:

```python
# JSON Lines (un objeto por l√≠nea)
df = pd.read_json('datos.jsonl', lines=True)

# Streaming de JSON Lines (para archivos grandes)
chunks = []
for chunk in pd.read_json('datos.jsonl', lines=True, chunksize=10000):
    # Procesar chunk
    chunks.append(chunk)
df = pd.concat(chunks, ignore_index=True)
```

**Escribir JSON**:

```python
# JSON est√°ndar
df.to_json('salida.json', orient='records', indent=2, force_ascii=False)

# JSON Lines
df.to_json('salida.jsonl', orient='records', lines=True, force_ascii=False)
```

**Par√°metro `orient`**:
- `'records'`: Lista de objetos `[{...}, {...}]` (m√°s com√∫n)
- `'split'`: Dict con `{index: [...], columns: [...], data: [...]}`
- `'index'`: Dict con √≠ndice como key `{0: {...}, 1: {...}}`
- `'columns'`: Dict por columnas `{col1: {0: val, 1: val}, ...}`
- `'values'`: Solo valores `[[...], [...]]`

---

## 3. Parquet: Almacenamiento Columnar

### 3.1. ¬øQu√© es Parquet?

**Apache Parquet** es un formato de almacenamiento **columnar** dise√±ado para Big Data.

**Diferencia clave: Row-based vs Column-based**

**Row-based (CSV, JSON)**:
```
[ID=1, Nombre="Juan", Edad=25, Ciudad="Madrid"]
[ID=2, Nombre="Mar√≠a", Edad=30, Ciudad="Barcelona"]
[ID=3, Nombre="Pedro", Edad=28, Ciudad="Madrid"]
```

**Column-based (Parquet)**:
```
ID:      [1, 2, 3]
Nombre:  ["Juan", "Mar√≠a", "Pedro"]
Edad:    [25, 30, 28]
Ciudad:  ["Madrid", "Barcelona", "Madrid"]
```

### 3.2. Ventajas de Parquet

**1. Compresi√≥n eficiente**:
- Columnas del mismo tipo se comprimen mejor
- Ejemplo: `[Madrid, Madrid, Madrid, ...]` ‚Üí compresi√≥n 10-50x

**2. Lectura selectiva** (Column Pruning):
```python
# Solo lee columna 'precio' del disco, ignora el resto
df = pd.read_parquet('ventas.parquet', columns=['precio'])
```

**3. Predicados pushdown** (Row Group Filtering):
```python
# Solo lee row groups que contengan a√±o 2024
df = pd.read_parquet('ventas.parquet', filters=[('a√±o', '==', 2024)])
```

**4. Schema embebido**:
- Tipos de datos preservados autom√°ticamente
- Metadata incluye estad√≠sticas (min, max, null_count)

**5. Optimizado para Big Data**:
- Compatible con Spark, Hive, Presto, Athena
- Particionado nativo

### 3.3. Estructura Interna de Parquet

**Jerarqu√≠a**:
```
Archivo Parquet
‚îú‚îÄ‚îÄ Row Group 1 (chunk de ~128MB)
‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: ID
‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: Nombre
‚îÇ   ‚îî‚îÄ‚îÄ Column Chunk: Precio
‚îú‚îÄ‚îÄ Row Group 2
‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: ID
‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk: Nombre
‚îÇ   ‚îî‚îÄ‚îÄ Column Chunk: Precio
‚îî‚îÄ‚îÄ Footer (metadata + estad√≠sticas)
```

**Metadata** incluye:
- Schema (nombres, tipos)
- Estad√≠sticas por columna (min, max, null_count)
- Codificaci√≥n y compresi√≥n usada
- Ubicaci√≥n de cada row group

### 3.4. Usar Parquet con Pandas

**Instalar PyArrow** (obligatorio):

```bash
pip install pyarrow
```

**Leer Parquet**:

```python
import pandas as pd

# Leer archivo completo
df = pd.read_parquet('ventas.parquet')

# Leer solo columnas espec√≠ficas (lectura selectiva)
df = pd.read_parquet('ventas.parquet', columns=['fecha', 'precio'])

# Aplicar filtros (pushdown de predicados)
df = pd.read_parquet('ventas.parquet',
                     filters=[('a√±o', '==', 2024), ('mes', '>', 6)])

# Leer m√∫ltiples archivos (particionado)
df = pd.read_parquet('ventas/')  # Lee todos los .parquet en directorio
```

**Escribir Parquet**:

```python
# Escritura b√°sica con compresi√≥n snappy (default)
df.to_parquet('salida.parquet')

# Especificar compresi√≥n
df.to_parquet('salida.parquet', compression='gzip')

# Sin compresi√≥n
df.to_parquet('salida.parquet', compression=None)

# Con PyArrow (m√°s opciones)
import pyarrow as pa
import pyarrow.parquet as pq

table = pa.Table.from_pandas(df)
pq.write_table(table, 'salida.parquet',
               compression='snappy',
               use_dictionary=True,
               compression_level=5)
```

### 3.5. Comparaci√≥n: CSV vs Parquet

**Ejemplo real** con 1 mill√≥n de registros:

| M√©trica           | CSV        | Parquet (snappy) | Reducci√≥n |
|-------------------|------------|------------------|-----------|
| Tama√±o en disco   | 120 MB     | 15 MB            | 87%       |
| Tiempo escritura  | 2.5 seg    | 1.8 seg          | 28%       |
| Tiempo lectura    | 3.2 seg    | 0.8 seg          | 75%       |
| Lectura 1 columna | 3.2 seg    | 0.2 seg          | 94%       |
| Memoria en RAM    | 180 MB     | 95 MB            | 47%       |

**Conclusi√≥n**: Parquet es **mucho m√°s eficiente** para datasets grandes.

---

## 4. Avro: Schemas Evolutivos

### 4.1. ¬øQu√© es Avro?

**Apache Avro** es un formato **binario** con **schema embebido**, dise√±ado para sistemas distribuidos.

**Caracter√≠sticas**:
- Formato binario compacto
- Schema incluido en archivo (autodescriptivo)
- Evoluci√≥n de schemas (backward/forward compatibility)
- R√°pido en serializaci√≥n/deserializaci√≥n
- Usado en Kafka, Hadoop, sistemas de streaming

### 4.2. Schema de Avro

**Ejemplo de schema**:

```json
{
  "type": "record",
  "name": "Usuario",
  "namespace": "com.empresa.datos",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "nombre", "type": "string"},
    {"name": "email", "type": ["null", "string"], "default": null},
    {"name": "edad", "type": "int"},
    {"name": "activo", "type": "boolean", "default": true}
  ]
}
```

**Tipos soportados**:
- Primitivos: `null`, `boolean`, `int`, `long`, `float`, `double`, `bytes`, `string`
- Complejos: `record`, `enum`, `array`, `map`, `union`, `fixed`

### 4.3. Evoluci√≥n de Schemas

**Problema**: ¬øQu√© pasa cuando el schema cambia?

**Schema v1**:
```json
{"name": "Usuario", "fields": [
  {"name": "id", "type": "int"},
  {"name": "nombre", "type": "string"}
]}
```

**Schema v2** (a√±ade campo):
```json
{"name": "Usuario", "fields": [
  {"name": "id", "type": "int"},
  {"name": "nombre", "type": "string"},
  {"name": "email", "type": ["null", "string"], "default": null}
]}
```

**Compatibilidad**:
- **Backward**: Lector nuevo puede leer datos viejos ‚úÖ
- **Forward**: Lector viejo puede leer datos nuevos ‚úÖ
- **Full**: Ambas direcciones ‚úÖ

**Reglas**:
- Campos nuevos deben tener `default`
- No se pueden eliminar campos sin default
- Tipos no pueden cambiar (int ‚Üí string ‚ùå)

### 4.4. Usar Avro en Python

**Instalar**:

```bash
pip install fastavro
```

**Escribir Avro**:

```python
from fastavro import writer, parse_schema

schema = {
    'type': 'record',
    'name': 'Usuario',
    'fields': [
        {'name': 'id', 'type': 'int'},
        {'name': 'nombre', 'type': 'string'},
        {'name': 'edad', 'type': 'int'}
    ]
}

registros = [
    {'id': 1, 'nombre': 'Juan', 'edad': 25},
    {'id': 2, 'nombre': 'Mar√≠a', 'edad': 30}
]

parsed_schema = parse_schema(schema)

with open('usuarios.avro', 'wb') as out:
    writer(out, parsed_schema, registros)
```

**Leer Avro**:

```python
from fastavro import reader

with open('usuarios.avro', 'rb') as fo:
    avro_reader = reader(fo)
    for record in avro_reader:
        print(record)
```

**Con Pandas**:

```python
import pandas as pd
from fastavro import reader

with open('usuarios.avro', 'rb') as fo:
    avro_reader = reader(fo)
    registros = [record for record in avro_reader]

df = pd.DataFrame(registros)
```

### 4.5. Parquet vs Avro

| Caracter√≠stica      | Parquet               | Avro                  |
|---------------------|-----------------------|-----------------------|
| Almacenamiento      | Columnar              | Row-based             |
| Caso de uso         | Analytics, queries    | Streaming, eventos    |
| Lectura selectiva   | ‚úÖ Muy eficiente       | ‚ùå Debe leer todo      |
| Escritura secuencial| ‚ùå Overhead            | ‚úÖ Muy r√°pida          |
| Compresi√≥n          | Excelente             | Buena                 |
| Schema evolution    | Limitada              | Avanzada              |
| Ecosistema          | Spark, Hive, Athena   | Kafka, Hadoop         |

**Regla**: Usa **Parquet** para an√°lisis, **Avro** para streaming.

---

## 5. Comparaci√≥n de Formatos

### Tabla Comparativa Completa

| Formato     | Tama√±o | Lectura | Escritura | Compresi√≥n | Schema | Nested | Humano | Caso de Uso           |
|-------------|--------|---------|-----------|------------|--------|--------|--------|-----------------------|
| **CSV**     | üî¥ Grande | üü° Media | üü¢ R√°pida | ‚ùå Pobre | ‚ùå No | ‚ùå No | ‚úÖ S√≠ | Datasets simples, intercambio |
| **JSON**    | üî¥ Grande | üî¥ Lenta | üü° Media | ‚ùå Pobre | ‚ùå No | ‚úÖ S√≠ | ‚úÖ S√≠ | APIs, configs, debugging |
| **JSON Lines** | üü° Media | üü° Media | üü¢ R√°pida | üü° Media | ‚ùå No | ‚úÖ S√≠ | ‚úÖ S√≠ | Logs, streaming, eventos |
| **Parquet** | üü¢ Peque√±o | üü¢ Muy r√°pida | üü° Media | ‚úÖ Excelente | ‚úÖ S√≠ | ‚úÖ S√≠ | ‚ùå No | Analytics, data lakes, Big Data |
| **Avro**    | üü¢ Peque√±o | üü¢ R√°pida | üü¢ R√°pida | ‚úÖ Buena | ‚úÖ S√≠ (evolutivo) | ‚úÖ S√≠ | ‚ùå No | Streaming, Kafka, evoluci√≥n |

### Benchmark Real

**Dataset**: 1 mill√≥n de registros (10 columnas, mix de tipos)

| Formato     | Tama√±o Disco | Tiempo Escritura | Tiempo Lectura | Lectura 1 Columna |
|-------------|--------------|------------------|----------------|-------------------|
| CSV         | 120 MB       | 2.5 seg          | 3.2 seg        | 3.2 seg           |
| JSON        | 185 MB       | 4.1 seg          | 5.8 seg        | 5.8 seg           |
| JSON Lines  | 185 MB       | 3.8 seg          | 4.2 seg        | 4.2 seg           |
| Parquet (snappy) | 15 MB   | 1.8 seg          | 0.8 seg        | 0.2 seg           |
| Parquet (gzip) | 12 MB     | 3.2 seg          | 1.2 seg        | 0.3 seg           |
| Avro        | 18 MB        | 1.5 seg          | 1.1 seg        | 1.1 seg           |

**Conclusiones**:
- Parquet es el m√°s eficiente para almacenamiento y lectura anal√≠tica
- Avro es r√°pido en escritura (ideal para streaming)
- CSV/JSON son lentos e ineficientes para grandes vol√∫menes
- JSON Lines es mejor que JSON para procesamiento incremental

---

## 6. Compresi√≥n de Datos

### 6.1. ¬øPor qu√© comprimir?

**Beneficios**:
1. **Reducci√≥n de costos**: Menos almacenamiento en S3/GCS
2. **Velocidad de transferencia**: Menos datos a mover por red
3. **I/O m√°s r√°pido**: En muchos casos, leer comprimido es m√°s r√°pido
4. **Backup m√°s eficiente**: Menor tama√±o de respaldos

**Trade-off**: CPU (compresi√≥n/descompresi√≥n) vs I/O (lectura/escritura)

### 6.2. Algoritmos de Compresi√≥n

| Algoritmo | Ratio Compresi√≥n | Velocidad Compresi√≥n | Velocidad Descompresi√≥n | Uso en Big Data |
|-----------|------------------|----------------------|-------------------------|-----------------|
| **None**  | 1x (sin compresi√≥n) | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚ö°‚ö°‚ö°‚ö°‚ö° | Solo desarrollo |
| **Snappy** | 2-4x | ‚ö°‚ö°‚ö°‚ö° | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚úÖ Default en Parquet |
| **LZ4**   | 2-3x | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚úÖ M√°s r√°pido |
| **Gzip**  | 4-8x | ‚ö°‚ö° | ‚ö°‚ö°‚ö° | ‚úÖ Mejor compresi√≥n |
| **Zstd**  | 4-8x | ‚ö°‚ö°‚ö° | ‚ö°‚ö°‚ö°‚ö° | ‚úÖ Balanceado |
| **Brotli** | 5-10x | ‚ö° | ‚ö°‚ö° | Archivos est√°ticos |

**Recomendaciones**:
- **Snappy**: Default, buen balance (uso general)
- **Gzip**: M√°xima compresi√≥n, datos archivados
- **Zstd**: Mejor que gzip en velocidad, similar ratio
- **LZ4**: M√°xima velocidad, streaming en tiempo real

### 6.3. Compresi√≥n en Pandas

**Parquet con compresi√≥n**:

```python
# Snappy (default, recomendado)
df.to_parquet('datos.parquet', compression='snappy')

# Gzip (mayor compresi√≥n)
df.to_parquet('datos.parquet', compression='gzip')

# Sin compresi√≥n
df.to_parquet('datos.parquet', compression=None)
```

**CSV con compresi√≥n**:

```python
# Gzip
df.to_csv('datos.csv.gz', compression='gzip')

# Leer CSV comprimido (autom√°tico)
df = pd.read_csv('datos.csv.gz')
```

**JSON con compresi√≥n**:

```python
# Gzip
df.to_json('datos.json.gz', compression='gzip', orient='records', lines=True)

# Leer JSON comprimido
df = pd.read_json('datos.json.gz', lines=True)
```

### 6.4. Cu√°ndo comprimir

**S√ç comprimir**:
- ‚úÖ Datos en reposo (data lakes, archivos hist√≥ricos)
- ‚úÖ Transferencia por red (upload/download)
- ‚úÖ Backups
- ‚úÖ Datasets grandes (>100 MB)

**NO comprimir** (o usar compresi√≥n r√°pida):
- ‚ùå Procesamiento en tiempo real (latencia cr√≠tica)
- ‚ùå Datasets peque√±os (<10 MB, overhead no vale la pena)
- ‚ùå CPU limitado (dispositivos embebidos)
- ‚ùå Lecturas muy frecuentes (cache descomprimido mejor)

---

## 7. Particionamiento de Datos

### 7.1. ¬øQu√© es particionar?

**Particionamiento** es dividir un dataset grande en m√∫ltiples archivos organizados por criterios (fecha, categor√≠a, regi√≥n).

**Sin particionar**:
```
ventas/
‚îî‚îÄ‚îÄ ventas.parquet (10 GB)
```

**Particionado por a√±o y mes**:
```
ventas/
‚îú‚îÄ‚îÄ a√±o=2023/
‚îÇ   ‚îú‚îÄ‚îÄ mes=01/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ datos.parquet (100 MB)
‚îÇ   ‚îú‚îÄ‚îÄ mes=02/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ datos.parquet (120 MB)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ a√±o=2024/
    ‚îú‚îÄ‚îÄ mes=01/
    ‚îÇ   ‚îî‚îÄ‚îÄ datos.parquet (150 MB)
    ‚îî‚îÄ‚îÄ ...
```

### 7.2. Beneficios del Particionamiento

**1. Partition Pruning**: Solo lee particiones relevantes

```python
# Query: ventas de enero 2024
# Sin particionar: Lee 10 GB completos
# Particionado: Lee solo 150 MB (partici√≥n a√±o=2024/mes=01)
df = pd.read_parquet('ventas/', filters=[('a√±o', '==', 2024), ('mes', '==', 1)])
```

**2. Paralelizaci√≥n**: Spark/Dask procesan particiones en paralelo

**3. Gesti√≥n de datos**: F√°cil eliminar datos antiguos

```bash
# Eliminar ventas de 2022
rm -rf ventas/a√±o=2022/
```

**4. Optimizaci√≥n de escritura**: Escribir por lotes (batch)

### 7.3. Estrategias de Particionamiento

**Por fecha** (m√°s com√∫n):
```
datos/a√±o=2024/mes=01/d√≠a=15/datos.parquet
```

**Por categor√≠a**:
```
ventas/region=Europa/pais=Espa√±a/datos.parquet
ventas/region=America/pais=Mexico/datos.parquet
```

**Por hash** (distribuci√≥n uniforme):
```
usuarios/hash=00/datos.parquet
usuarios/hash=01/datos.parquet
...
usuarios/hash=ff/datos.parquet
```

**Mixto** (fecha + categor√≠a):
```
logs/a√±o=2024/mes=10/nivel=ERROR/datos.parquet
```

### 7.4. Implementar Particionamiento en Pandas

**Escribir con particiones**:

```python
import pandas as pd

# Cargar datos
df = pd.read_csv('ventas_raw.csv')
df['fecha'] = pd.to_datetime(df['fecha'])
df['a√±o'] = df['fecha'].dt.year
df['mes'] = df['fecha'].dt.month

# Escribir particionado
df.to_parquet('ventas/', partition_cols=['a√±o', 'mes'])
```

**Resultado**:
```
ventas/
‚îú‚îÄ‚îÄ a√±o=2023/
‚îÇ   ‚îú‚îÄ‚îÄ mes=1/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ xxxxxxxxx.parquet
‚îÇ   ‚îî‚îÄ‚îÄ mes=2/
‚îÇ       ‚îî‚îÄ‚îÄ yyyyyyyyy.parquet
‚îî‚îÄ‚îÄ a√±o=2024/
    ‚îî‚îÄ‚îÄ mes=1/
        ‚îî‚îÄ‚îÄ zzzzzzzzz.parquet
```

**Leer con filtros** (partition pruning):

```python
# Lee solo particiones de 2024
df_2024 = pd.read_parquet('ventas/', filters=[('a√±o', '==', 2024)])

# Lee solo enero 2024
df_ene_2024 = pd.read_parquet('ventas/',
                               filters=[('a√±o', '==', 2024), ('mes', '==', 1)])
```

### 7.5. Buenas Pr√°cticas de Particionamiento

**1. No sobre-particionar**:
- ‚ùå `a√±o/mes/d√≠a/hora/minuto` ‚Üí Miles de particiones peque√±as (overhead)
- ‚úÖ `a√±o/mes` ‚Üí Particiones de 100-500 MB

**2. Elegir columnas con baja cardinalidad**:
- ‚úÖ Fecha, regi√≥n, categor√≠a (pocas opciones)
- ‚ùå ID √∫nico, email (alta cardinalidad)

**3. Orden de particiones**:
- Primera: columna m√°s filtrada (ej: a√±o)
- Segunda: siguiente m√°s filtrada (ej: mes)

**4. Tama√±o de particiones**:
- Ideal: **100-500 MB por partici√≥n**
- Muy peque√±as (<10 MB): overhead de metadatos
- Muy grandes (>1 GB): dif√≠cil de procesar

---

## 8. Cu√°ndo Usar Cada Formato

### Matriz de Decisi√≥n

**CSV**: Usar cuando...
- ‚úÖ Necesitas intercambiar datos con Excel o herramientas simples
- ‚úÖ Dataset peque√±o (<100 MB)
- ‚úÖ No hay anidaci√≥n de datos
- ‚úÖ Lectura humana es importante
- ‚ùå NO usar para Big Data, APIs, o datos anidados

**JSON**: Usar cuando...
- ‚úÖ Datos de APIs REST
- ‚úÖ Configuraci√≥n de aplicaciones
- ‚úÖ Datos anidados complejos
- ‚úÖ Debugging (legibilidad)
- ‚ùå NO usar para datasets grandes (>100 MB)

**JSON Lines**: Usar cuando...
- ‚úÖ Logs de aplicaciones
- ‚úÖ Streaming de eventos
- ‚úÖ Procesamiento incremental (append-only)
- ‚úÖ Datasets grandes que necesitan ser legibles
- ‚ùå NO usar si necesitas consultas anal√≠ticas

**Parquet**: Usar cuando...
- ‚úÖ Data lakes y data warehouses
- ‚úÖ Analytics y BI (consultas sobre columnas espec√≠ficas)
- ‚úÖ Big Data (>1 GB)
- ‚úÖ Necesitas compresi√≥n eficiente
- ‚úÖ Integraci√≥n con Spark, Hive, Athena, BigQuery
- ‚ùå NO usar para streaming en tiempo real

**Avro**: Usar cuando...
- ‚úÖ Streaming con Kafka
- ‚úÖ Evoluci√≥n de schemas frecuente
- ‚úÖ Sistemas distribuidos con esquemas versionados
- ‚úÖ Serializaci√≥n r√°pida de eventos
- ‚ùå NO usar para an√°lisis ad-hoc (usa Parquet)

### Flujo T√≠pico en Data Engineering

```
Fuente            Ingestion        Processing        Storage           Analytics
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ       ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
APIs (JSON)   ‚Üí   JSON Lines   ‚Üí   Parquet       ‚Üí   Data Lake     ‚Üí   Parquet
Logs (text)   ‚Üí   JSON Lines   ‚Üí   Parquet       ‚Üí   Data Lake     ‚Üí   Parquet
Kafka         ‚Üí   Avro         ‚Üí   Parquet       ‚Üí   Data Lake     ‚Üí   Parquet
CSV legacy    ‚Üí   CSV          ‚Üí   Parquet       ‚Üí   Data Lake     ‚Üí   Parquet
```

**Patr√≥n recomendado**:
1. **Ingest**: JSON Lines o Avro (streaming)
2. **Transform**: Pandas/Spark (en memoria)
3. **Store**: Parquet particionado + comprimido (data lake)
4. **Query**: Parquet (Athena, BigQuery, Spark SQL)

---

## 9. Errores Comunes

### ‚ùå Error 1: No considerar el tama√±o de memoria con JSON

**Problema**:
```python
# Archivo JSON de 5 GB
df = pd.read_json('datos_enormes.json')  # MemoryError!
```

**Soluci√≥n**:
```python
# Usar JSON Lines y procesar por chunks
chunks = []
for chunk in pd.read_json('datos_enormes.jsonl', lines=True, chunksize=10000):
    # Procesar chunk
    chunks.append(chunk)
df = pd.concat(chunks, ignore_index=True)
```

### ‚ùå Error 2: Usar CSV para datos complejos nested

**Problema**:
```csv
pedido_id,items
1,"[{'producto': 'A', 'qty': 2}, {'producto': 'B', 'qty': 1}]"
```

**Soluci√≥n**: Usar JSON o Parquet con nested structures

```python
# JSON Lines con nested data
df.to_json('pedidos.jsonl', orient='records', lines=True)

# O normalizar antes de CSV
df_items = df.explode('items')
```

### ‚ùå Error 3: No particionar datasets grandes

**Problema**:
```python
# 50 GB en un solo archivo
df.to_parquet('ventas_historicas.parquet')  # Lento, dif√≠cil de consultar
```

**Soluci√≥n**:
```python
# Particionar por a√±o y mes
df['a√±o'] = df['fecha'].dt.year
df['mes'] = df['fecha'].dt.month
df.to_parquet('ventas/', partition_cols=['a√±o', 'mes'])
```

### ‚ùå Error 4: Ignorar la compresi√≥n

**Problema**:
```python
# 10 GB sin comprimir
df.to_parquet('datos.parquet', compression=None)
```

**Soluci√≥n**:
```python
# Con compresi√≥n snappy (default, pero expl√≠cito es mejor)
df.to_parquet('datos.parquet', compression='snappy')  # ~1.5 GB
```

### ‚ùå Error 5: No aprovechar lectura columnar

**Problema**:
```python
# Lee todo el archivo para obtener solo una columna
df = pd.read_parquet('ventas.parquet')
precios = df['precio']
```

**Soluci√≥n**:
```python
# Lee solo la columna necesaria
precios = pd.read_parquet('ventas.parquet', columns=['precio'])
# 10-100x m√°s r√°pido
```

---

## 10. Buenas Pr√°cticas

### ‚úÖ 1. Elegir formato seg√∫n caso de uso

```python
# API response ‚Üí JSON
datos_api = requests.get(url).json()
pd.DataFrame(datos_api).to_json('api_data.json', orient='records')

# Logs ‚Üí JSON Lines
logs.to_json('logs.jsonl', orient='records', lines=True)

# Analytics ‚Üí Parquet
df_limpio.to_parquet('data_lake/ventas/', partition_cols=['a√±o', 'mes'])

# Streaming ‚Üí Avro (si aplica)
# usar librer√≠as espec√≠ficas de Kafka
```

### ‚úÖ 2. Siempre comprimir datos en reposo

```python
# Default snappy es perfecto para casi todos los casos
df.to_parquet('datos.parquet', compression='snappy')

# Gzip para m√°xima compresi√≥n (datos archivados)
df.to_parquet('historico_2020.parquet', compression='gzip')
```

### ‚úÖ 3. Particionar datos grandes

```python
# Regla: si >1 GB, particionar
if tama√±o_dataset > 1_000_000_000:  # 1 GB
    df.to_parquet('datos/', partition_cols=['a√±o', 'mes'])
else:
    df.to_parquet('datos.parquet')
```

### ‚úÖ 4. Documentar esquemas

```python
# Comentar tipos y restricciones
"""
Schema de ventas:
- venta_id: int64 (√∫nico, no nulo)
- fecha: datetime64[ns] (rango: 2020-2024)
- monto: float64 (>0, no nulo)
- categoria: str (valores: [A, B, C, D])
- cliente_id: int64 (FK a clientes)
"""
```

### ‚úÖ 5. Testear conversiones

```python
def test_conversion_csv_a_parquet():
    """Verifica que conversi√≥n preserve datos."""
    df_original = pd.read_csv('datos.csv')
    df_original.to_parquet('temp.parquet')
    df_convertido = pd.read_parquet('temp.parquet')

    pd.testing.assert_frame_equal(df_original, df_convertido)
```

### ‚úÖ 6. Validar integridad despu√©s de conversi√≥n

```python
def validar_conversion(df_original, df_convertido):
    """Valida que conversi√≥n no perdi√≥ datos."""
    assert len(df_original) == len(df_convertido), "Filas no coinciden"
    assert set(df_original.columns) == set(df_convertido.columns), "Columnas no coinciden"
    assert df_original['id'].nunique() == df_convertido['id'].nunique(), "IDs √∫nicos no coinciden"
    print("‚úÖ Conversi√≥n validada")
```

### ‚úÖ 7. Usar lectura selectiva con Parquet

```python
# Leer solo columnas necesarias
df_columnas = pd.read_parquet('datos.parquet', columns=['id', 'precio'])

# Aplicar filtros (partition pruning)
df_filtrado = pd.read_parquet('datos/',
                               filters=[('a√±o', '==', 2024)])
```

### ‚úÖ 8. Configurar PyArrow para mejor performance

```python
import pyarrow.parquet as pq

# Escribir con configuraci√≥n optimizada
pq.write_table(
    pa.Table.from_pandas(df),
    'datos.parquet',
    compression='snappy',
    use_dictionary=True,  # Compresi√≥n adicional para strings repetidos
    write_statistics=True,  # Metadata para filtros
    row_group_size=100_000  # Tama√±o √≥ptimo de row groups
)
```

---

## üìö Resumen

### Puntos Clave

1. **Formato importa**: La elecci√≥n afecta performance, costo y escalabilidad
2. **Parquet para analytics**: Columnar, comprimido, eficiente
3. **JSON Lines para streaming**: Append-only, procesamiento incremental
4. **Comprimir siempre**: Snappy para balance, gzip para m√°xima compresi√≥n
5. **Particionar datasets grandes**: >1 GB debe estar particionado
6. **Lectura selectiva**: Usa `columns` y `filters` con Parquet
7. **Documentar schemas**: Mant√©n clara la estructura de datos
8. **Testear conversiones**: Validar integridad despu√©s de convertir

### Reglas de Oro

| Caso de Uso                 | Formato Recomendado |
|-----------------------------|---------------------|
| An√°lisis de grandes vol√∫menes | Parquet (snappy)  |
| Logs y eventos              | JSON Lines (gzip)   |
| API responses               | JSON                |
| Intercambio simple          | CSV                 |
| Streaming evolutivo         | Avro                |
| Data Lake storage           | Parquet particionado |

### Pr√≥ximos Pasos

- **Pr√°ctica**: Convierte datasets entre formatos
- **Benchmark**: Mide diferencias de performance
- **Implementa**: Crea un conversor multi-formato
- **Optimiza**: Particiona y comprime tus datos

---

**Recursos Adicionales**:
- [Documentaci√≥n Parquet](https://parquet.apache.org/docs/)
- [JSON Lines Spec](https://jsonlines.org/)
- [Apache Avro](https://avro.apache.org/docs/)
- [PyArrow Documentation](https://arrow.apache.org/docs/python/)

---

*√öltima actualizaci√≥n: 2025-10-30*
