# IntroducciÃ³n a Apache Spark: Procesamiento Distribuido de Datos

## IntroducciÃ³n

### Â¿Por quÃ© necesitamos Spark?

Imagina que eres el Data Engineer de **StreamFlix**, una plataforma de streaming con 50 millones de usuarios. Cada dÃ­a generas:

- **500 millones** de eventos de reproducciÃ³n
- **100 millones** de bÃºsquedas
- **50 GB** de logs de servidor

Tu script de Python con Pandas funciona perfectamente con 1 millÃ³n de filas... pero con 500 millones, tu laptop se queda sin memoria y el proceso tardarÃ­a **dÃ­as**.

**Â¿La soluciÃ³n?** Distribuir el trabajo entre muchas mÃ¡quinas que trabajan en paralelo. Eso es exactamente lo que hace Apache Spark.

### AnalogÃ­a del Mundo Real: La FÃ¡brica de Pizzas

Piensa en una pizzerÃ­a:

| Escenario | Pandas (1 cocinero) | Spark (Equipo de cocineros) |
|-----------|---------------------|----------------------------|
| 10 pizzas | âœ… FÃ¡cil, 30 min | âš ï¸ Overkill, setup innecesario |
| 100 pizzas | ğŸ˜“ 5 horas, agotador | âœ… 1 hora, 5 cocineros en paralelo |
| 10,000 pizzas | âŒ Imposible | âœ… 2 horas, 50 cocineros coordinados |

Spark es como tener un **chef ejecutivo** (Driver) que coordina a muchos **cocineros** (Executors) trabajando en paralelo.

### Contexto en Data Engineering

```
Fuentes de Datos    â†’    Procesamiento    â†’    Destino
(TB de datos)            (Spark)               (Data Warehouse)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka     â”‚         â”‚   Spark     â”‚        â”‚  Snowflake  â”‚
â”‚   S3/HDFS   â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚   Cluster   â”‚ â”€â”€â”€â”€â”€â–º â”‚  Delta Lake â”‚
â”‚   APIs      â”‚         â”‚ (Distribuido)â”‚        â”‚  PostgreSQL â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Fundamentos de Spark

### Â¿QuÃ© es Apache Spark?

Apache Spark es un **motor de procesamiento distribuido** diseÃ±ado para:

1. **Velocidad**: Procesa datos en memoria (100x mÃ¡s rÃ¡pido que Hadoop MapReduce)
2. **Facilidad**: APIs en Python, Scala, Java y R
3. **Generalidad**: Batch, streaming, ML, grafos - todo en una plataforma
4. **Escalabilidad**: De laptop a miles de nodos

### Arquitectura de Spark

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DRIVER PROGRAM                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   SparkSession                        â”‚  â”‚
â”‚  â”‚  - Punto de entrada a Spark                          â”‚  â”‚
â”‚  â”‚  - Coordina la ejecuciÃ³n                             â”‚  â”‚
â”‚  â”‚  - Divide el trabajo en tasks                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CLUSTER MANAGER                           â”‚
â”‚         (Standalone / YARN / Kubernetes / Mesos)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 â”‚                 â”‚
       â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXECUTOR  â”‚    â”‚  EXECUTOR  â”‚    â”‚  EXECUTOR  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  Task  â”‚ â”‚    â”‚ â”‚  Task  â”‚ â”‚    â”‚ â”‚  Task  â”‚ â”‚
â”‚ â”‚  Task  â”‚ â”‚    â”‚ â”‚  Task  â”‚ â”‚    â”‚ â”‚  Task  â”‚ â”‚
â”‚ â”‚  Cache â”‚ â”‚    â”‚ â”‚  Cache â”‚ â”‚    â”‚ â”‚  Cache â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Worker 1          Worker 2          Worker 3
```

**Componentes clave:**

| Componente | Rol | AnalogÃ­a PizzerÃ­a |
|------------|-----|-------------------|
| **Driver** | Programa principal que coordina | Chef ejecutivo |
| **SparkSession** | Punto de entrada a Spark | Cocina central |
| **Cluster Manager** | Asigna recursos (CPU, RAM) | Gerente de personal |
| **Executor** | Proceso que ejecuta tasks | Cocinero |
| **Task** | Unidad mÃ­nima de trabajo | Preparar 1 pizza |
| **Partition** | Fragmento de datos | Bandeja de ingredientes |

### SparkSession: Tu Punto de Entrada

```python
from pyspark.sql import SparkSession

# Crear SparkSession (siempre es el primer paso)
spark = SparkSession.builder \
    .appName("MiPrimerJob") \
    .master("local[*]") \
    .config("spark.sql.shuffle.partitions", "200") \
    .getOrCreate()

# Verificar que funciona
print(f"Spark version: {spark.version}")
print(f"App name: {spark.sparkContext.appName}")
```

**Modos de ejecuciÃ³n:**

| Modo | DescripciÃ³n | Uso |
|------|-------------|-----|
| `local` | 1 thread en tu mÃ¡quina | Testing bÃ¡sico |
| `local[4]` | 4 threads en tu mÃ¡quina | Desarrollo local |
| `local[*]` | Todos los cores disponibles | Desarrollo local |
| `yarn` | Cluster Hadoop YARN | ProducciÃ³n |
| `k8s://...` | Cluster Kubernetes | ProducciÃ³n cloud |

---

## RDDs, DataFrames y Datasets

### EvoluciÃ³n de las APIs de Spark

```
                    Spark 1.0          Spark 1.3          Spark 2.0+
                    â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AbstracciÃ³n:          RDD      â†’      DataFrame     â†’      Dataset
                   (bajo nivel)      (alto nivel)       (tipado fuerte)

Rendimiento:         BÃ¡sico     â†’      Optimizado    â†’     Optimizado
                   (sin Catalyst)    (Catalyst)          (Catalyst)
```

### RDD (Resilient Distributed Dataset)

El RDD es la abstracciÃ³n fundamental de Spark: una colecciÃ³n distribuida e inmutable de objetos.

```python
# Crear RDD desde una lista
numeros_rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# Crear RDD desde archivo
logs_rdd = spark.sparkContext.textFile("hdfs://logs/*.txt")

# Operaciones bÃ¡sicas
resultado = numeros_rdd \
    .filter(lambda x: x % 2 == 0) \
    .map(lambda x: x * 2) \
    .collect()

print(resultado)  # [4, 8, 12, 16, 20]
```

**CaracterÃ­sticas del RDD:**

| CaracterÃ­stica | Significado |
|----------------|-------------|
| **Resilient** | Tolerante a fallos (se puede reconstruir) |
| **Distributed** | Dividido en particiones en mÃºltiples nodos |
| **Dataset** | ColecciÃ³n de elementos |
| **Immutable** | No se modifica, se crean nuevos RDDs |

### DataFrame: La API Moderna

El DataFrame es una colecciÃ³n distribuida organizada en **columnas con nombre** (como una tabla SQL o un DataFrame de Pandas).

```python
# Crear DataFrame desde lista de diccionarios
datos = [
    {"nombre": "Ana", "edad": 28, "ciudad": "Madrid"},
    {"nombre": "Luis", "edad": 35, "ciudad": "Barcelona"},
    {"nombre": "MarÃ­a", "edad": 42, "ciudad": "Madrid"},
]

df = spark.createDataFrame(datos)
df.show()
# +------+----+---------+
# |nombre|edad|   ciudad|
# +------+----+---------+
# |   Ana|  28|   Madrid|
# |  Luis|  35|Barcelona|
# | MarÃ­a|  42|   Madrid|
# +------+----+---------+

# Ver esquema
df.printSchema()
# root
#  |-- nombre: string (nullable = true)
#  |-- edad: long (nullable = true)
#  |-- ciudad: string (nullable = true)
```

**Â¿Por quÃ© usar DataFrames en lugar de RDDs?**

| Aspecto | RDD | DataFrame |
|---------|-----|-----------|
| **OptimizaciÃ³n** | Manual | AutomÃ¡tica (Catalyst) |
| **SerializaciÃ³n** | Python objects (lento) | Tungsten binary (rÃ¡pido) |
| **Sintaxis** | Funcional (lambda) | Declarativa (SQL-like) |
| **Esquema** | ImplÃ­cito | ExplÃ­cito |
| **Interoperabilidad** | Limitada | SQL, Pandas, Parquet |

### Leer Datos en DataFrames

```python
# Desde CSV
df_csv = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("datos/ventas.csv")

# Desde Parquet (formato columnar optimizado)
df_parquet = spark.read.parquet("datos/ventas.parquet")

# Desde JSON
df_json = spark.read.json("datos/eventos.json")

# Desde base de datos JDBC
df_jdbc = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://localhost/db") \
    .option("dbtable", "ventas") \
    .option("user", "usuario") \
    .option("password", "contraseÃ±a") \
    .load()
```

---

## Transformaciones vs Acciones

### El Concepto MÃ¡s Importante: Lazy Evaluation

Spark **NO ejecuta nada** hasta que realmente necesitas el resultado. Esto permite optimizar toda la cadena de operaciones.

```python
# Estas lÃ­neas NO ejecutan nada todavÃ­a
df_filtrado = df.filter(df.edad > 30)           # TransformaciÃ³n
df_proyectado = df_filtrado.select("nombre")    # TransformaciÃ³n
df_ordenado = df_proyectado.orderBy("nombre")   # TransformaciÃ³n

# AQUÃ es cuando Spark ejecuta TODO el plan
resultado = df_ordenado.collect()  # AcciÃ³n - Â¡AHORA se ejecuta!
```

**AnalogÃ­a**: Es como escribir una receta de cocina. Escribir los pasos no cocina nada; solo cuando dices "Â¡Cocina!" se ejecuta todo.

### Transformaciones (Lazy)

Las transformaciones crean un **nuevo DataFrame** sin ejecutar nada:

| TransformaciÃ³n | DescripciÃ³n | Ejemplo |
|----------------|-------------|---------|
| `select()` | Seleccionar columnas | `df.select("nombre", "edad")` |
| `filter()` / `where()` | Filtrar filas | `df.filter(df.edad > 30)` |
| `withColumn()` | AÃ±adir/modificar columna | `df.withColumn("mayor", df.edad > 18)` |
| `drop()` | Eliminar columnas | `df.drop("columna_temp")` |
| `groupBy()` | Agrupar por columna | `df.groupBy("ciudad")` |
| `orderBy()` | Ordenar | `df.orderBy("edad", ascending=False)` |
| `join()` | Unir DataFrames | `df1.join(df2, "id")` |
| `distinct()` | Eliminar duplicados | `df.distinct()` |
| `limit()` | Limitar filas | `df.limit(100)` |

**Tipos de transformaciones:**

```
Narrow Transformations (sin shuffle):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Partitionâ”‚ â”€â”€â–º â”‚Partitionâ”‚   Datos permanecen en el mismo nodo
â”‚   1     â”‚     â”‚   1'    â”‚   Ejemplos: filter, map, select
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Wide Transformations (con shuffle):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Partitionâ”‚ â”€â”€â”
â”‚   1     â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”œâ”€â–º â”‚ New     â”‚   Datos se redistribuyen entre nodos
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚Partitionâ”‚   Ejemplos: groupBy, join, orderBy
â”‚Partitionâ”‚ â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   2     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Acciones (Trigger Execution)

Las acciones **disparan la ejecuciÃ³n** y devuelven un resultado:

| AcciÃ³n | DescripciÃ³n | Retorna |
|--------|-------------|---------|
| `show()` | Mostrar primeras filas | Nada (imprime) |
| `collect()` | Traer todos los datos al driver | Lista Python |
| `count()` | Contar filas | Entero |
| `first()` | Primera fila | Row |
| `take(n)` | Primeras n filas | Lista de Rows |
| `write` | Escribir a archivo/BD | Nada |
| `foreach()` | Aplicar funciÃ³n a cada fila | Nada |

```python
# Ejemplo completo
from pyspark.sql.functions import col, avg, count

# Transformaciones (no ejecutan nada)
ventas_madrid = df_ventas \
    .filter(col("ciudad") == "Madrid") \
    .filter(col("monto") > 100) \
    .groupBy("categoria") \
    .agg(
        count("*").alias("num_ventas"),
        avg("monto").alias("monto_promedio")
    ) \
    .orderBy(col("num_ventas").desc())

# AcciÃ³n - AHORA se ejecuta todo
ventas_madrid.show()
```

---

## Spark UI: Monitoreo y Debugging

Cuando ejecutas Spark, puedes acceder a la **Spark UI** en `http://localhost:4040`:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SPARK UI                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Jobs    â”‚  Stages   â”‚  Storage  â”‚  Environment â”‚  SQL      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Job 0: count at script.py:25                               â”‚
â”‚  â”œâ”€â”€ Stage 0: scan parquet â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%               â”‚
â”‚  â””â”€â”€ Stage 1: aggregate    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  67%               â”‚
â”‚                                                             â”‚
â”‚  DAG Visualization:                                         â”‚
â”‚  [Scan] â†’ [Filter] â†’ [Project] â†’ [Aggregate] â†’ [Collect]   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Conceptos clave:**

| Concepto | DescripciÃ³n |
|----------|-------------|
| **Job** | Conjunto de stages para una acciÃ³n |
| **Stage** | Conjunto de tasks que pueden ejecutarse en paralelo |
| **Task** | Unidad mÃ­nima de trabajo (1 task por particiÃ³n) |
| **DAG** | Grafo Dirigido AcÃ­clico del plan de ejecuciÃ³n |

---

## Particionamiento

### Â¿QuÃ© es una ParticiÃ³n?

Una particiÃ³n es un **fragmento de datos** que se procesa en un executor. El nÃºmero de particiones determina el paralelismo.

```python
# Ver nÃºmero de particiones
df = spark.read.parquet("datos/ventas.parquet")
print(f"Particiones: {df.rdd.getNumPartitions()}")

# Reparticionar (redistribuye datos - shuffle)
df_reparticionado = df.repartition(100)

# Coalesce (reduce particiones sin shuffle completo)
df_reducido = df.coalesce(10)
```

**Reglas generales:**

| SituaciÃ³n | RecomendaciÃ³n |
|-----------|---------------|
| Lectura inicial | 2-4 particiones por core |
| DespuÃ©s de filtrar mucho | `coalesce()` para reducir |
| Antes de join grande | `repartition()` por clave de join |
| Escribir archivos | Controlar particiones = controlar archivos |

### Particionamiento por Columna

```python
# Escribir particionado por fecha
df_ventas.write \
    .partitionBy("aÃ±o", "mes") \
    .parquet("output/ventas_particionadas")

# Estructura resultante:
# output/ventas_particionadas/
# â”œâ”€â”€ aÃ±o=2024/
# â”‚   â”œâ”€â”€ mes=01/
# â”‚   â”‚   â”œâ”€â”€ part-00000.parquet
# â”‚   â”‚   â””â”€â”€ part-00001.parquet
# â”‚   â”œâ”€â”€ mes=02/
# â”‚   â”‚   â””â”€â”€ ...
```

---

## Operaciones Comunes

### Funciones de Columna

```python
from pyspark.sql.functions import (
    col, lit, when, coalesce,
    upper, lower, trim, concat,
    year, month, dayofmonth, current_date,
    sum, avg, count, max, min,
    explode, array, struct
)

# Crear/modificar columnas
df = df.withColumn("nombre_upper", upper(col("nombre")))
df = df.withColumn("edad_categoria",
    when(col("edad") < 30, "joven")
    .when(col("edad") < 50, "adulto")
    .otherwise("senior")
)

# Extraer fecha
df = df.withColumn("aÃ±o", year(col("fecha")))
df = df.withColumn("mes", month(col("fecha")))
```

### Agregaciones

```python
from pyspark.sql.functions import sum, avg, count, countDistinct

# AgregaciÃ³n simple
df.groupBy("ciudad") \
    .agg(
        count("*").alias("total_registros"),
        countDistinct("cliente_id").alias("clientes_unicos"),
        sum("monto").alias("monto_total"),
        avg("monto").alias("monto_promedio")
    ) \
    .show()
```

### Joins

```python
# Inner join (default)
df_resultado = df_ventas.join(df_clientes, "cliente_id")

# Left join
df_resultado = df_ventas.join(df_clientes, "cliente_id", "left")

# Join con condiciÃ³n compleja
from pyspark.sql.functions import col

df_resultado = df_ventas.join(
    df_clientes,
    (df_ventas.cliente_id == df_clientes.id) &
    (df_ventas.fecha >= df_clientes.fecha_registro),
    "inner"
)
```

---

## Escribir Datos

```python
# A Parquet (recomendado)
df.write \
    .mode("overwrite") \
    .parquet("output/datos.parquet")

# A CSV
df.write \
    .mode("append") \
    .option("header", "true") \
    .csv("output/datos.csv")

# A tabla Hive/Delta
df.write \
    .mode("overwrite") \
    .saveAsTable("database.tabla")

# Modos de escritura:
# - overwrite: Reemplaza todo
# - append: AÃ±ade al final
# - ignore: No hace nada si existe
# - error: Error si existe (default)
```

---

## Errores Comunes y Soluciones

### 1. OutOfMemoryError

```python
# âŒ Mal: traer todo al driver
todos_los_datos = df_gigante.collect()  # OOM!

# âœ… Bien: procesar en chunks o agregar primero
resultado = df_gigante.groupBy("categoria").count().collect()
```

### 2. Shuffle Excesivo

```python
# âŒ Mal: mÃºltiples shuffles
df.groupBy("a").count().groupBy("b").count()

# âœ… Bien: minimizar shuffles
df.groupBy("a", "b").count()
```

### 3. PequeÃ±os Archivos

```python
# âŒ Mal: 1000 particiones = 1000 archivos pequeÃ±os
df.repartition(1000).write.parquet("output/")

# âœ… Bien: coalesce antes de escribir
df.coalesce(10).write.parquet("output/")
```

---

## Checklist de Aprendizaje

- [ ] Entiendo por quÃ© Spark es necesario para Big Data
- [ ] Puedo crear una SparkSession correctamente
- [ ] Conozco la diferencia entre Driver y Executors
- [ ] Entiendo la diferencia entre RDD y DataFrame
- [ ] SÃ© cuÃ¡ndo usar transformaciones vs acciones
- [ ] Comprendo lazy evaluation y por quÃ© es importante
- [ ] Puedo leer datos de CSV, Parquet y JSON
- [ ] SÃ© usar filter, select, groupBy, join
- [ ] Entiendo el concepto de particiones
- [ ] Puedo usar la Spark UI para debugging bÃ¡sico

---

## Recursos Adicionales

- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Spark SQL Functions Reference](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)
- [Databricks Learning](https://www.databricks.com/learn)
- [Learning Spark, 2nd Edition (O'Reilly)](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050049/)

---

**Siguiente**: [02-EJEMPLOS.md](02-EJEMPLOS.md) - Ejemplos prÃ¡cticos con PySpark
