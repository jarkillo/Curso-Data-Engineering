# Proyecto PrÃ¡ctico: Procesamiento Batch con PySpark

Sistema de procesamiento batch para anÃ¡lisis de ventas de e-commerce usando Apache Spark.

## Objetivos

- Procesar datos de ventas en formato CSV/Parquet
- Calcular mÃ©tricas de negocio (revenue, AOV, top productos)
- Generar reportes agregados por diferentes dimensiones
- Escribir resultados optimizados para anÃ¡lisis

## Estructura del Proyecto

```
04-proyecto-practico/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ spark_session.py      # ConfiguraciÃ³n de SparkSession
â”‚   â”œâ”€â”€ transformations.py    # Transformaciones de datos
â”‚   â”œâ”€â”€ aggregations.py       # Agregaciones y mÃ©tricas
â”‚   â””â”€â”€ io_utils.py           # Lectura/escritura de datos
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â””â”€â”€ test_aggregations.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_sales.csv      # Datos de ejemplo
â”œâ”€â”€ output/                   # Resultados procesados
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## InstalaciÃ³n

```bash
# Crear entorno virtual
python -m venv venv

# Activar (Windows)
.\venv\Scripts\Activate.ps1

# Activar (Linux/Mac)
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt
```

## Uso

```python
from src.spark_session import create_spark_session
from src.transformations import clean_sales_data
from src.aggregations import calculate_revenue_metrics

# Crear sesiÃ³n
spark = create_spark_session("SalesAnalysis")

# Leer datos
df = spark.read.csv("data/sample_sales.csv", header=True, inferSchema=True)

# Procesar
df_clean = clean_sales_data(df)
metrics = calculate_revenue_metrics(df_clean)

# Guardar
metrics.write.parquet("output/revenue_metrics.parquet")
```

## Ejecutar Tests

```bash
pytest -v --cov=src tests/
```

## TecnologÃ­as

- Python 3.11+
- PySpark 3.5.0
- pytest

## Estado

ðŸš§ En desarrollo
