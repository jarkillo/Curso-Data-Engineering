# ğŸ’» Tema 2: Web Scraping - Ejemplos PrÃ¡cticos

**MÃ³dulo 4: APIs y Web Scraping**
**Master en IngenierÃ­a de Datos**

---

## ğŸ“‹ Ãndice de Ejemplos

1. [Ejemplo 1: Scraping BÃ¡sico con BeautifulSoup](#ejemplo-1-scraping-bÃ¡sico-con-beautifulsoup) ğŸ“—
2. [Ejemplo 2: Extraer Tabla de Datos](#ejemplo-2-extraer-tabla-de-datos) ğŸ“—
3. [Ejemplo 3: Scraping con NavegaciÃ³n](#ejemplo-3-scraping-con-navegaciÃ³n) ğŸ“™
4. [Ejemplo 4: Scraping DinÃ¡mico con Selenium](#ejemplo-4-scraping-dinÃ¡mico-con-selenium) ğŸ“™
5. [Ejemplo 5: Scraper Masivo con Almacenamiento](#ejemplo-5-scraper-masivo-con-almacenamiento) ğŸ“•

**Leyenda:**
- ğŸ“— **BÃ¡sico** - Conceptos fundamentales (15-20 min)
- ğŸ“™ **Intermedio** - IntegraciÃ³n de conceptos (25-35 min)
- ğŸ“• **Avanzado** - Soluciones completas (40-60 min)

---

## ğŸ¯ Objetivos de este Documento

Al completar estos ejemplos, habrÃ¡s:

âœ… Scrapeado pÃ¡ginas HTML estÃ¡ticas con BeautifulSoup
âœ… ExtraÃ­do datos de tablas HTML a CSV
âœ… Navegado mÃºltiples pÃ¡ginas para scraping masivo
âœ… Usado Selenium para pÃ¡ginas con JavaScript
âœ… Creado un scraper completo con almacenamiento en SQLite

---

## ğŸ“— Ejemplo 1: Scraping BÃ¡sico con BeautifulSoup

### ğŸ¯ Objetivo

Extraer **tÃ­tulos de noticias** de una pÃ¡gina HTML estÃ¡tica.

### ğŸ“ Contexto Empresarial

**DataHub Inc.** necesita analizar las noticias del sector tech para detectar tendencias. Tu tarea es scrapear los titulares de un blog de tecnologÃ­a.

---

### ğŸŒ HTML de Ejemplo

Para este ejemplo, usaremos este HTML simulado (guardado como `noticias_ejemplo.html`):

```html
<!DOCTYPE html>
<html>
<head>
    <title>Blog de TecnologÃ­a - DataHub Inc.</title>
</head>
<body>
    <header>
        <h1>Ãšltimas Noticias de Tech</h1>
    </header>

    <main>
        <article class="noticia" data-id="1">
            <h2 class="titulo">Python 3.12 mejora el rendimiento en un 25%</h2>
            <p class="autor">Por Ana GarcÃ­a</p>
            <time class="fecha">2025-10-20</time>
            <p class="resumen">
                La nueva versiÃ³n de Python incluye optimizaciones significativas...
            </p>
        </article>

        <article class="noticia" data-id="2">
            <h2 class="titulo">AWS lanza nuevo servicio de IA Generativa</h2>
            <p class="autor">Por Carlos MÃ©ndez</p>
            <time class="fecha">2025-10-19</time>
            <p class="resumen">
                Amazon Web Services presenta Bedrock Studio para desarrolladores...
            </p>
        </article>

        <article class="noticia" data-id="3">
            <h2 class="titulo">ChatGPT alcanza 200 millones de usuarios activos</h2>
            <p class="autor">Por MarÃ­a LÃ³pez</p>
            <time class="fecha">2025-10-18</time>
            <p class="resumen">
                OpenAI anuncia que su modelo supera las expectativas de adopciÃ³n...
            </p>
        </article>
    </main>
</body>
</html>
```

---

### ğŸ’» CÃ³digo Python Completo

```python
from bs4 import BeautifulSoup

# 1ï¸âƒ£ LEER EL ARCHIVO HTML
with open('noticias_ejemplo.html', 'r', encoding='utf-8') as f:
    html = f.read()

# 2ï¸âƒ£ PARSEAR CON BEAUTIFULSOUP
soup = BeautifulSoup(html, 'html.parser')

# 3ï¸âƒ£ EXTRAER TODAS LAS NOTICIAS
noticias = soup.find_all('article', class_='noticia')

print(f"ğŸ“° Encontradas {len(noticias)} noticias\n")

# 4ï¸âƒ£ ITERAR Y EXTRAER DATOS
for i, noticia in enumerate(noticias, 1):
    # Extraer elementos individuales
    titulo = noticia.find('h2', class_='titulo').text.strip()
    autor = noticia.find('p', class_='autor').text.strip()
    fecha = noticia.find('time', class_='fecha').text.strip()
    resumen = noticia.find('p', class_='resumen').text.strip()

    # Extraer atributo data-id
    data_id = noticia.get('data-id')

    # Mostrar resultados
    print(f"ğŸ“Œ Noticia {i} (ID: {data_id})")
    print(f"   TÃ­tulo: {titulo}")
    print(f"   Autor: {autor}")
    print(f"   Fecha: {fecha}")
    print(f"   Resumen: {resumen[:50]}...")
    print()
```

---

### ğŸ“¤ Salida Esperada

```
ğŸ“° Encontradas 3 noticias

ğŸ“Œ Noticia 1 (ID: 1)
   TÃ­tulo: Python 3.12 mejora el rendimiento en un 25%
   Autor: Por Ana GarcÃ­a
   Fecha: 2025-10-20
   Resumen: La nueva versiÃ³n de Python incluye optimizaci...

ğŸ“Œ Noticia 2 (ID: 2)
   TÃ­tulo: AWS lanza nuevo servicio de IA Generativa
   Autor: Por Carlos MÃ©ndez
   Fecha: 2025-10-19
   Resumen: Amazon Web Services presenta Bedrock Studio p...

ğŸ“Œ Noticia 3 (ID: 3)
   TÃ­tulo: ChatGPT alcanza 200 millones de usuarios activos
   Autor: Por MarÃ­a LÃ³pez
   Fecha: 2025-10-18
   Resumen: OpenAI anuncia que su modelo supera las expec...
```

---

### ğŸ“ Conceptos Clave

| Concepto     | ExplicaciÃ³n                                     |
| ------------ | ----------------------------------------------- |
| `find_all()` | Encuentra **todos** los elementos que coincidan |
| `.text`      | Extrae el **texto** dentro del elemento         |
| `.strip()`   | Elimina **espacios** al inicio y final          |
| `.get()`     | Extrae el **valor de un atributo** HTML         |

**Tip:** Usa `.text.strip()` siempre para limpiar espacios innecesarios.

---

## ğŸ“— Ejemplo 2: Extraer Tabla de Datos

### ğŸ¯ Objetivo

Scrapear una **tabla HTML de productos** y convertirla a **CSV**.

### ğŸ“ Contexto Empresarial

**DataHub Inc.** necesita analizar la competencia. Tu tarea es extraer la tabla de productos de un e-commerce y guardarla en CSV para anÃ¡lisis con Excel.

---

### ğŸŒ HTML de Ejemplo

```html
<!DOCTYPE html>
<html>
<head>
    <title>Productos - E-Commerce</title>
</head>
<body>
    <h1>CatÃ¡logo de Laptops</h1>

    <table id="productos">
        <thead>
            <tr>
                <th>ID</th>
                <th>Producto</th>
                <th>Precio</th>
                <th>Stock</th>
                <th>Rating</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>001</td>
                <td>Dell XPS 13</td>
                <td>$1,299</td>
                <td>15</td>
                <td>4.8</td>
            </tr>
            <tr>
                <td>002</td>
                <td>MacBook Pro M3</td>
                <td>$2,499</td>
                <td>8</td>
                <td>4.9</td>
            </tr>
            <tr>
                <td>003</td>
                <td>Lenovo ThinkPad X1</td>
                <td>$1,799</td>
                <td>12</td>
                <td>4.7</td>
            </tr>
            <tr>
                <td>004</td>
                <td>HP Spectre x360</td>
                <td>$1,599</td>
                <td>0</td>
                <td>4.6</td>
            </tr>
        </tbody>
    </table>
</body>
</html>
```

---

### ğŸ’» CÃ³digo Python Completo

```python
from bs4 import BeautifulSoup
import csv

# 1ï¸âƒ£ LEER HTML
with open('productos_tabla.html', 'r', encoding='utf-8') as f:
    html = f.read()

# 2ï¸âƒ£ PARSEAR
soup = BeautifulSoup(html, 'html.parser')

# 3ï¸âƒ£ ENCONTRAR LA TABLA
tabla = soup.find('table', id='productos')

# 4ï¸âƒ£ EXTRAER ENCABEZADOS
encabezados = []
for th in tabla.find('thead').find_all('th'):
    encabezados.append(th.text.strip())

print(f"ğŸ“Š Encabezados: {encabezados}\n")

# 5ï¸âƒ£ EXTRAER FILAS
productos = []
tbody = tabla.find('tbody')

for tr in tbody.find_all('tr'):
    fila = []
    for td in tr.find_all('td'):
        fila.append(td.text.strip())
    productos.append(fila)

print(f"âœ… ExtraÃ­dos {len(productos)} productos\n")

# 6ï¸âƒ£ MOSTRAR PRIMEROS 3
for i, producto in enumerate(productos[:3], 1):
    print(f"ğŸ“¦ Producto {i}: {producto}")

# 7ï¸âƒ£ GUARDAR EN CSV
with open('productos.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(encabezados)  # Header
    writer.writerows(productos)   # Datos

print("\nğŸ’¾ Datos guardados en 'productos.csv'")
```

---

### ğŸ“¤ Salida Esperada

```
ğŸ“Š Encabezados: ['ID', 'Producto', 'Precio', 'Stock', 'Rating']

âœ… ExtraÃ­dos 4 productos

ğŸ“¦ Producto 1: ['001', 'Dell XPS 13', '$1,299', '15', '4.8']
ğŸ“¦ Producto 2: ['002', 'MacBook Pro M3', '$2,499', '8', '4.9']
ğŸ“¦ Producto 3: ['003', 'Lenovo ThinkPad X1', '$1,799', '12', '4.7']

ğŸ’¾ Datos guardados en 'productos.csv'
```

---

### ğŸ“„ Contenido de `productos.csv`

```csv
ID,Producto,Precio,Stock,Rating
001,Dell XPS 13,$1,299,15,4.8
002,MacBook Pro M3,$2,499,8,4.9
003,Lenovo ThinkPad X1,$1,799,12,4.7
004,HP Spectre x360,$1,599,0,4.6
```

---

### ğŸ“ Conceptos Clave

| Concepto         | ExplicaciÃ³n                                 |
| ---------------- | ------------------------------------------- |
| `find('table')`  | Encuentra la **tabla** HTML                 |
| `find_all('tr')` | Encuentra todas las **filas** (table row)   |
| `find_all('td')` | Encuentra todas las **celdas** (table data) |
| `csv.writer()`   | Escribe datos en formato **CSV**            |

**Tip:** Las tablas HTML tienen estructura predecible: `<table>` â†’ `<thead>` / `<tbody>` â†’ `<tr>` â†’ `<th>` / `<td>`.

---

## ğŸ“™ Ejemplo 3: Scraping con NavegaciÃ³n

### ğŸ¯ Objetivo

Scrapear **mÃºltiples pÃ¡ginas** de un sitio web siguiendo enlaces.

### ğŸ“ Contexto Empresarial

**DataHub Inc.** quiere analizar las publicaciones de su blog. El blog tiene 3 pÃ¡ginas de artÃ­culos. Tu tarea es scrapear **todas las pÃ¡ginas** automÃ¡ticamente.

---

### ğŸŒ Estructura del Sitio

```
https://ejemplo.com/blog?page=1  â† 3 artÃ­culos
https://ejemplo.com/blog?page=2  â† 3 artÃ­culos
https://ejemplo.com/blog?page=3  â† 2 artÃ­culos

Total: 8 artÃ­culos
```

---

### ğŸ’» CÃ³digo Python Completo

```python
import requests
from bs4 import BeautifulSoup
import time

BASE_URL = "https://jsonplaceholder.typicode.com/posts"

# Usaremos JSONPlaceholder como API de prueba
# pero simularemos scraping con paginaciÃ³n

def scrapear_pagina(numero_pagina):
    """
    Simula scraping de una pÃ¡gina especÃ­fica.
    En la realidad, harÃ­as requests a URLs diferentes.
    """
    print(f"\nğŸ” Scraping pÃ¡gina {numero_pagina}...")

    # Simular paginaciÃ³n: obtener posts del 1-3, 4-6, 7-9
    inicio = (numero_pagina - 1) * 3 + 1
    fin = inicio + 2

    articulos = []
    for post_id in range(inicio, fin + 1):
        url = f"{BASE_URL}/{post_id}"

        try:
            response = requests.get(url, timeout=5)
            response.raise_for_status()

            data = response.json()

            articulo = {
                'id': data['id'],
                'titulo': data['title'],
                'cuerpo': data['body'][:50] + '...'  # Primeros 50 chars
            }

            articulos.append(articulo)
            print(f"   âœ… ArtÃ­culo {post_id}: {articulo['titulo'][:40]}...")

        except requests.exceptions.RequestException as e:
            print(f"   âŒ Error en artÃ­culo {post_id}: {e}")

    # âš ï¸ RATE LIMITING: Esperar entre pÃ¡ginas
    time.sleep(1)  # 1 segundo entre pÃ¡ginas

    return articulos


# ğŸš€ SCRAPING DE MÃšLTIPLES PÃGINAS
print("=" * 60)
print("ğŸ“° SCRAPER MULTI-PÃGINA - DataHub Inc.")
print("=" * 60)

total_articulos = []
NUM_PAGINAS = 3

for pagina in range(1, NUM_PAGINAS + 1):
    articulos_pagina = scrapear_pagina(pagina)
    total_articulos.extend(articulos_pagina)

# ğŸ“Š RESUMEN
print("\n" + "=" * 60)
print(f"âœ… SCRAPING COMPLETADO")
print(f"ğŸ“Š Total de pÃ¡ginas scrapeadas: {NUM_PAGINAS}")
print(f"ğŸ“„ Total de artÃ­culos extraÃ­dos: {len(total_articulos)}")
print("=" * 60)

# Mostrar primeros 3 artÃ­culos
print("\nğŸ“‹ Primeros 3 artÃ­culos:")
for i, art in enumerate(total_articulos[:3], 1):
    print(f"\n{i}. {art['titulo']}")
    print(f"   ID: {art['id']}")
    print(f"   Extracto: {art['cuerpo']}")

# ğŸ’¾ GUARDAR EN JSON
import json

with open('articulos_scrapeados.json', 'w', encoding='utf-8') as f:
    json.dump(total_articulos, f, indent=2, ensure_ascii=False)

print("\nğŸ’¾ Datos guardados en 'articulos_scrapeados.json'")
```

---

### ğŸ“¤ Salida Esperada

```
============================================================
ğŸ“° SCRAPER MULTI-PÃGINA - DataHub Inc.
============================================================

ğŸ” Scraping pÃ¡gina 1...
   âœ… ArtÃ­culo 1: sunt aut facere repellat provident occ...
   âœ… ArtÃ­culo 2: qui est esse...
   âœ… ArtÃ­culo 3: ea molestias quasi exercitationem rep...

ğŸ” Scraping pÃ¡gina 2...
   âœ… ArtÃ­culo 4: eum et est occaecati...
   âœ… ArtÃ­culo 5: nesciunt quas odio...
   âœ… ArtÃ­culo 6: dolorem eum magni eos aperiam quia...

ğŸ” Scraping pÃ¡gina 3...
   âœ… ArtÃ­culo 7: magnam facilis autem...
   âœ… ArtÃ­culo 8: dolorem dolore est ipsam...
   âœ… ArtÃ­culo 9: nesciunt iure omnis dolorem tempora...

============================================================
âœ… SCRAPING COMPLETADO
ğŸ“Š Total de pÃ¡ginas scrapeadas: 3
ğŸ“„ Total de artÃ­culos extraÃ­dos: 9
============================================================

ğŸ“‹ Primeros 3 artÃ­culos:

1. sunt aut facere repellat provident occaecati excepturi optio reprehenderit
   ID: 1
   Extracto: quia et suscipit\nsuscipit recusandae consequuntur...

2. qui est esse
   ID: 2
   Extracto: est rerum tempore vitae\nsequi sint nihil repr...

3. ea molestias quasi exercitationem repellat qui ipsa sit aut
   ID: 3
   Extracto: et iusto sed quo iure\nvoluptatem occaecati om...

ğŸ’¾ Datos guardados en 'articulos_scrapeados.json'
```

---

### ğŸ“ Conceptos Clave

| Concepto              | ExplicaciÃ³n                                |
| --------------------- | ------------------------------------------ |
| **Loop de pÃ¡ginas**   | `for pagina in range(1, NUM_PAGINAS + 1)`  |
| **Rate limiting**     | `time.sleep(1)` entre requests             |
| **AcumulaciÃ³n**       | `total_articulos.extend(articulos_pagina)` |
| **Manejo de errores** | `try/except` para requests fallidos        |

**Tip:** Siempre usa `time.sleep()` entre requests para no sobrecargar el servidor.

---

## ğŸ“™ Ejemplo 4: Scraping DinÃ¡mico con Selenium

### ğŸ¯ Objetivo

Scrapear una pÃ¡gina que carga contenido con **JavaScript dinÃ¡mico** usando Selenium.

### ğŸ“ Contexto Empresarial

**DataHub Inc.** necesita datos de un sitio con **carga lazy** (scroll infinito). BeautifulSoup NO puede ver el contenido porque se genera con JavaScript. Usaremos Selenium.

---

### ğŸ’» CÃ³digo Python Completo

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import time

def scrapear_con_selenium():
    """
    Scrapea una pÃ¡gina con JavaScript dinÃ¡mico.
    Ejemplo: Quotes to Scrape (pÃ¡gina de prÃ¡ctica)
    """
    print("ğŸš€ Iniciando navegador...")

    # 1ï¸âƒ£ INICIALIZAR DRIVER
    options = webdriver.ChromeOptions()
    # options.add_argument('--headless')  # Descomenta para modo sin GUI
    driver = webdriver.Chrome(options=options)

    try:
        # 2ï¸âƒ£ NAVEGAR A LA PÃGINA
        url = "http://quotes.toscrape.com/js/"  # PÃ¡gina con JS
        print(f"ğŸŒ Abriendo: {url}")
        driver.get(url)

        # 3ï¸âƒ£ ESPERAR A QUE CARGUE EL CONTENIDO
        print("â³ Esperando a que cargue el contenido JavaScript...")
        wait = WebDriverWait(driver, 10)

        # Esperar a que aparezcan las quotes
        quotes = wait.until(
            EC.presence_of_all_elements_located((By.CLASS_NAME, "quote"))
        )

        print(f"âœ… PÃ¡gina cargada. Encontradas {len(quotes)} quotes.\n")

        # 4ï¸âƒ£ EXTRAER DATOS
        datos = []

        for i, quote_elem in enumerate(quotes, 1):
            try:
                # Extraer texto de la quote
                texto = quote_elem.find_element(By.CLASS_NAME, "text").text

                # Extraer autor
                autor = quote_elem.find_element(By.CLASS_NAME, "author").text

                # Extraer tags
                tag_elems = quote_elem.find_elements(By.CLASS_NAME, "tag")
                tags = [tag.text for tag in tag_elems]

                dato = {
                    'numero': i,
                    'texto': texto,
                    'autor': autor,
                    'tags': tags
                }

                datos.append(dato)

                print(f"ğŸ“ Quote {i}")
                print(f"   Texto: {texto[:50]}...")
                print(f"   Autor: {autor}")
                print(f"   Tags: {', '.join(tags)}")
                print()

            except Exception as e:
                print(f"âš ï¸ Error al extraer quote {i}: {e}")

        # 5ï¸âƒ£ HACER CLIC EN "NEXT" (OPCIONAL)
        try:
            print("ğŸ”„ Intentando ir a la pÃ¡gina siguiente...")
            next_button = driver.find_element(By.CLASS_NAME, "next")
            next_link = next_button.find_element(By.TAG_NAME, "a")
            next_link.click()

            time.sleep(2)  # Esperar a que cargue

            print("âœ… NavegaciÃ³n exitosa a pÃ¡gina 2")

        except Exception:
            print("â„¹ï¸ No hay botÃ³n 'Next' o ya estamos en la Ãºltima pÃ¡gina")

        return datos

    except TimeoutException:
        print("âŒ Timeout: El contenido no cargÃ³ a tiempo")
        return []

    except Exception as e:
        print(f"âŒ Error inesperado: {e}")
        return []

    finally:
        # 6ï¸âƒ£ SIEMPRE CERRAR EL NAVEGADOR
        print("\nğŸ”’ Cerrando navegador...")
        driver.quit()


# ğŸš€ EJECUTAR
print("=" * 70)
print("ğŸ•·ï¸ SELENIUM SCRAPER - Contenido JavaScript DinÃ¡mico")
print("=" * 70)
print()

quotes = scrapear_con_selenium()

print("\n" + "=" * 70)
print(f"âœ… Scraping completado: {len(quotes)} quotes extraÃ­das")
print("=" * 70)

# Guardar en JSON
import json

with open('quotes_selenium.json', 'w', encoding='utf-8') as f:
    json.dump(quotes, f, indent=2, ensure_ascii=False)

print("\nğŸ’¾ Datos guardados en 'quotes_selenium.json'")
```

---

### ğŸ“¤ Salida Esperada

```
======================================================================
ğŸ•·ï¸ SELENIUM SCRAPER - Contenido JavaScript DinÃ¡mico
======================================================================

ğŸš€ Iniciando navegador...
ğŸŒ Abriendo: http://quotes.toscrape.com/js/
â³ Esperando a que cargue el contenido JavaScript...
âœ… PÃ¡gina cargada. Encontradas 10 quotes.

ğŸ“ Quote 1
   Texto: "The world as we have created it is a process...
   Autor: Albert Einstein
   Tags: change, deep-thoughts, thinking, world

ğŸ“ Quote 2
   Texto: "It is our choices, Harry, that show what we...
   Autor: J.K. Rowling
   Tags: abilities, choices

ğŸ“ Quote 3
   Texto: "There are only two ways to live your life. O...
   Autor: Albert Einstein
   Tags: inspirational, life, live, miracle, miracles

[... mÃ¡s quotes ...]

ğŸ”„ Intentando ir a la pÃ¡gina siguiente...
âœ… NavegaciÃ³n exitosa a pÃ¡gina 2

ğŸ”’ Cerrando navegador...

======================================================================
âœ… Scraping completado: 10 quotes extraÃ­das
======================================================================

ğŸ’¾ Datos guardados en 'quotes_selenium.json'
```

---

### ğŸ“ Conceptos Clave

| Concepto                              | ExplicaciÃ³n                                        |
| ------------------------------------- | -------------------------------------------------- |
| `WebDriverWait`                       | Espera **inteligente** hasta que aparezca elemento |
| `EC.presence_of_all_elements_located` | CondiciÃ³n: elementos **presentes** en DOM          |
| `.click()`                            | Simula **clic** de usuario                         |
| `driver.quit()`                       | **Cierra** el navegador (importante en `finally`)  |

**Tip:** Usa `--headless` en opciones para ejecutar sin abrir ventana (mÃ¡s rÃ¡pido en servidores).

---

## ğŸ“• Ejemplo 5: Scraper Masivo con Almacenamiento

### ğŸ¯ Objetivo

Crear un **scraper completo** que:
1. Scrapea mÃºltiples productos
2. Valida datos
3. Almacena en **SQLite**
4. Maneja errores y logging

### ğŸ“ Contexto Empresarial

**DataHub Inc.** necesita monitorear precios de competidores diariamente. Tu tarea es crear un scraper robusto que almacene datos en base de datos para anÃ¡lisis histÃ³rico.

---

### ğŸ’» CÃ³digo Python Completo

```python
import requests
from bs4 import BeautifulSoup
import sqlite3
from datetime import datetime
import time
import logging

# ğŸ”§ CONFIGURACIÃ“N DE LOGGING
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ProductoScraper:
    """
    Scraper robusto para extraer productos de e-commerce.
    """

    def __init__(self, db_name='productos.db'):
        self.db_name = db_name
        self.setup_database()

    def setup_database(self):
        """Crea la tabla de productos si no existe."""
        logger.info(f"ğŸ“‚ Configurando base de datos: {self.db_name}")

        conn = sqlite3.connect(self.db_name)
        cursor = conn.cursor()

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS productos (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                nombre TEXT NOT NULL,
                precio REAL,
                stock INTEGER,
                rating REAL,
                url TEXT,
                fecha_scraping TEXT NOT NULL,
                UNIQUE(nombre, fecha_scraping)
            )
        ''')

        conn.commit()
        conn.close()
        logger.info("âœ… Base de datos configurada")

    def validar_precio(self, precio_str):
        """
        Valida y convierte precio de string a float.
        Ejemplo: "$1,299.99" -> 1299.99
        """
        try:
            # Eliminar $, comas y espacios
            precio_limpio = precio_str.replace('$', '').replace(',', '').strip()
            precio = float(precio_limpio)

            if precio < 0:
                logger.warning(f"âš ï¸ Precio negativo detectado: {precio}")
                return None

            return precio

        except (ValueError, AttributeError) as e:
            logger.error(f"âŒ Error al validar precio '{precio_str}': {e}")
            return None

    def validar_stock(self, stock_str):
        """Valida stock."""
        try:
            stock = int(stock_str)
            return stock if stock >= 0 else 0
        except (ValueError, TypeError):
            return 0

    def validar_rating(self, rating_str):
        """Valida rating (0-5)."""
        try:
            rating = float(rating_str)
            if 0 <= rating <= 5:
                return rating
            return None
        except (ValueError, TypeError):
            return None

    def scrapear_productos(self, url, max_productos=10):
        """
        Scrapea productos de una URL.
        """
        logger.info(f"ğŸ” Iniciando scraping: {url}")

        headers = {
            'User-Agent': 'DataHubScraper/1.0 (contacto@datahub.com)'
        }

        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # En un caso real, aquÃ­ adaptarÃ­as los selectores
            # Para este ejemplo, usaremos datos simulados
            productos_scrapeados = self._simular_scraping(max_productos)

            logger.info(f"âœ… Scraping exitoso: {len(productos_scrapeados)} productos")
            return productos_scrapeados

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Error en request: {e}")
            return []

    def _simular_scraping(self, cantidad):
        """Simula scraping de productos (para demo)."""
        productos = [
            {'nombre': 'Dell XPS 13', 'precio': '$1,299.99', 'stock': '15', 'rating': '4.8'},
            {'nombre': 'MacBook Pro M3', 'precio': '$2,499.00', 'stock': '8', 'rating': '4.9'},
            {'nombre': 'Lenovo ThinkPad', 'precio': '$1,799.50', 'stock': '12', 'rating': '4.7'},
            {'nombre': 'HP Spectre x360', 'precio': '$1,599.99', 'stock': '0', 'rating': '4.6'},
            {'nombre': 'ASUS ZenBook', 'precio': '$1,099.00', 'stock': '20', 'rating': '4.5'},
        ]
        return productos[:cantidad]

    def guardar_productos(self, productos):
        """Guarda productos en SQLite."""
        logger.info(f"ğŸ’¾ Guardando {len(productos)} productos en DB...")

        conn = sqlite3.connect(self.db_name)
        cursor = conn.cursor()

        fecha_actual = datetime.now().isoformat()
        guardados = 0
        errores = 0

        for prod in productos:
            try:
                # Validar datos
                nombre = prod['nombre']
                precio = self.validar_precio(prod['precio'])
                stock = self.validar_stock(prod['stock'])
                rating = self.validar_rating(prod['rating'])

                if precio is None:
                    logger.warning(f"âš ï¸ Producto '{nombre}' tiene precio invÃ¡lido")
                    errores += 1
                    continue

                # Insertar en DB
                cursor.execute('''
                    INSERT OR IGNORE INTO productos
                    (nombre, precio, stock, rating, url, fecha_scraping)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (nombre, precio, stock, rating, None, fecha_actual))

                if cursor.rowcount > 0:
                    guardados += 1
                    logger.info(f"   âœ… {nombre}: ${precio} (stock: {stock})")
                else:
                    logger.info(f"   â„¹ï¸ {nombre}: Ya existe para hoy")

            except sqlite3.Error as e:
                logger.error(f"   âŒ Error DB al guardar {prod['nombre']}: {e}")
                errores += 1

        conn.commit()
        conn.close()

        logger.info(f"âœ… Guardados: {guardados} | âš ï¸ Errores: {errores}")
        return guardados

    def obtener_estadisticas(self):
        """Obtiene estadÃ­sticas de la DB."""
        conn = sqlite3.connect(self.db_name)
        cursor = conn.cursor()

        # Total de productos
        cursor.execute('SELECT COUNT(*) FROM productos')
        total = cursor.fetchone()[0]

        # Precio promedio
        cursor.execute('SELECT AVG(precio) FROM productos WHERE precio IS NOT NULL')
        precio_avg = cursor.fetchone()[0]

        # Producto mÃ¡s caro
        cursor.execute('''
            SELECT nombre, precio
            FROM productos
            WHERE precio IS NOT NULL
            ORDER BY precio DESC
            LIMIT 1
        ''')
        mas_caro = cursor.fetchone()

        conn.close()

        return {
            'total_productos': total,
            'precio_promedio': round(precio_avg, 2) if precio_avg else 0,
            'producto_mas_caro': mas_caro if mas_caro else ('N/A', 0)
        }


# ğŸš€ EJECUTAR SCRAPER
if __name__ == '__main__':
    print("=" * 70)
    print("ğŸ•·ï¸ SCRAPER MASIVO CON ALMACENAMIENTO - DataHub Inc.")
    print("=" * 70)
    print()

    # Crear scraper
    scraper = ProductoScraper(db_name='productos_competencia.db')

    # Scrapear (en la realidad, aquÃ­ irÃ­an URLs reales)
    url_simulada = "https://ejemplo-ecommerce.com/laptops"
    productos = scraper.scrapear_productos(url_simulada, max_productos=5)

    # Guardar en DB
    if productos:
        scraper.guardar_productos(productos)

        # Mostrar estadÃ­sticas
        stats = scraper.obtener_estadisticas()
        print("\n" + "=" * 70)
        print("ğŸ“Š ESTADÃSTICAS")
        print("=" * 70)
        print(f"ğŸ“¦ Total de productos en DB: {stats['total_productos']}")
        print(f"ğŸ’° Precio promedio: ${stats['precio_promedio']}")
        print(f"ğŸ‘‘ Producto mÃ¡s caro: {stats['producto_mas_caro'][0]} "
              f"(${stats['producto_mas_caro'][1]})")
        print("=" * 70)

    else:
        print("âŒ No se pudieron scrapear productos")
```

---

### ğŸ“¤ Salida Esperada

```
======================================================================
ğŸ•·ï¸ SCRAPER MASIVO CON ALMACENAMIENTO - DataHub Inc.
======================================================================

2025-10-23 14:35:22 - INFO - ğŸ“‚ Configurando base de datos: productos_competencia.db
2025-10-23 14:35:22 - INFO - âœ… Base de datos configurada
2025-10-23 14:35:22 - INFO - ğŸ” Iniciando scraping: https://ejemplo-ecommerce.com/laptops
2025-10-23 14:35:22 - INFO - âœ… Scraping exitoso: 5 productos
2025-10-23 14:35:22 - INFO - ğŸ’¾ Guardando 5 productos en DB...
2025-10-23 14:35:22 - INFO -    âœ… Dell XPS 13: $1299.99 (stock: 15)
2025-10-23 14:35:22 - INFO -    âœ… MacBook Pro M3: $2499.0 (stock: 8)
2025-10-23 14:35:22 - INFO -    âœ… Lenovo ThinkPad: $1799.5 (stock: 12)
2025-10-23 14:35:22 - INFO -    âœ… HP Spectre x360: $1599.99 (stock: 0)
2025-10-23 14:35:22 - INFO -    âœ… ASUS ZenBook: $1099.0 (stock: 20)
2025-10-23 14:35:22 - INFO - âœ… Guardados: 5 | âš ï¸ Errores: 0

======================================================================
ğŸ“Š ESTADÃSTICAS
======================================================================
ğŸ“¦ Total de productos en DB: 5
ğŸ’° Precio promedio: $1659.5
ğŸ‘‘ Producto mÃ¡s caro: MacBook Pro M3 ($2499.0)
======================================================================
```

---

### ğŸ“ Conceptos Clave

| Concepto                | ExplicaciÃ³n                                |
| ----------------------- | ------------------------------------------ |
| **Clase scraper**       | Encapsula lÃ³gica en una clase reutilizable |
| **ValidaciÃ³n de datos** | Limpia y valida antes de guardar           |
| **SQLite**              | Base de datos local sin servidor           |
| **Logging**             | Registra eventos para debugging            |
| **UNIQUE constraint**   | Evita duplicados en DB                     |

**Tip:** Usa clases para scrapers complejos, funciones simples para scrapers pequeÃ±os.

---

## ğŸ“Š ComparaciÃ³n de MÃ©todos

| CaracterÃ­stica  | BeautifulSoup           | Selenium                       |
| --------------- | ----------------------- | ------------------------------ |
| **Velocidad**   | âš¡ Muy rÃ¡pido (segundos) | ğŸŒ Lento (abre navegador real)  |
| **Memoria**     | ğŸ’š Baja (~50 MB)         | ğŸ”´ Alta (~500 MB)               |
| **JavaScript**  | âŒ No puede ejecutar     | âœ… Ejecuta JS completo          |
| **InteracciÃ³n** | âŒ No puede hacer clic   | âœ… Simula usuario real          |
| **Uso tÃ­pico**  | PÃ¡ginas estÃ¡ticas       | PÃ¡ginas dinÃ¡micas (React, Vue) |

**Regla general:**
1. Intenta con **BeautifulSoup** primero
2. Si no funciona (contenido dinÃ¡mico), usa **Selenium**

---

## âœ… Resumen de Conceptos

### BeautifulSoup
- `find()` - Primer elemento
- `find_all()` - Todos los elementos
- `.text` - Extraer texto
- `.get('atributo')` - Extraer atributo

### Selenium
- `WebDriverWait` - Esperas inteligentes
- `find_element(By.CLASS_NAME)` - Encontrar por clase
- `.click()` - Simular clic
- `driver.quit()` - Cerrar navegador

### Buenas PrÃ¡cticas
- âœ… Rate limiting con `time.sleep()`
- âœ… User-Agent identificable
- âœ… ValidaciÃ³n de datos antes de guardar
- âœ… Manejo de errores con try/except
- âœ… Logging para debugging

---

## ğŸ¯ Siguiente Paso

Â¡Excelente trabajo! ğŸ‰ Ahora dominas los conceptos fundamentales de web scraping.

**PrÃ³ximo paso:** [03-EJERCICIOS.md](03-EJERCICIOS.md) - 15 ejercicios progresivos para practicar.

---

*Ãšltima actualizaciÃ³n: 2025-10-23*
*Tema 2 - MÃ³dulo 4 - Master en IngenierÃ­a de Datos*
---

## ğŸ§­ NavegaciÃ³n

â¬…ï¸ **Anterior**: [01 Teoria](01-TEORIA.md) | â¡ï¸ **Siguiente**: [03 Ejercicios](03-EJERCICIOS.md)
