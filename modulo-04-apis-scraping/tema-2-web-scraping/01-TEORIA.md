# üìñ Tema 2: Web Scraping - Teor√≠a

**M√≥dulo 4: APIs y Web Scraping**
**Master en Ingenier√≠a de Datos**

---

## üìã √çndice

1. [Introducci√≥n](#introducci√≥n)
2. [¬øQu√© es Web Scraping?](#qu√©-es-web-scraping)
3. [HTML, CSS y DOM](#html-css-y-dom)
4. [BeautifulSoup: Parsing de HTML](#beautifulsoup-parsing-de-html)
5. [Selenium: Scraping Din√°mico](#selenium-scraping-din√°mico)
6. [Robots.txt y √âtica del Scraping](#robotstxt-y-√©tica-del-scraping)
7. [User-Agent y Headers](#user-agent-y-headers)
8. [XPath y CSS Selectors](#xpath-y-css-selectors)
9. [Almacenamiento de Datos Scrapeados](#almacenamiento-de-datos-scrapeados)
10. [Web Scraping vs APIs](#web-scraping-vs-apis)
11. [Legalidad y √âtica](#legalidad-y-√©tica)
12. [Errores Comunes](#errores-comunes)
13. [Buenas Pr√°cticas](#buenas-pr√°cticas)
14. [Checklist de Aprendizaje](#checklist-de-aprendizaje)

---

## üéØ Objetivos de Aprendizaje

Al finalizar este tema, ser√°s capaz de:

‚úÖ **Entender qu√© es web scraping** y cu√°ndo es la herramienta adecuada
‚úÖ **Parsear HTML** con BeautifulSoup para extraer datos estructurados
‚úÖ **Usar Selenium** para scraping de p√°ginas con JavaScript din√°mico
‚úÖ **Respetar robots.txt** y aplicar scraping √©tico
‚úÖ **Configurar User-Agent** y headers apropiados
‚úÖ **Dominar selectores** (CSS y XPath) para extraer datos precisos
‚úÖ **Almacenar datos scrapeados** de forma estructurada (CSV, JSON, SQLite)
‚úÖ **Decidir entre scraping y APIs** seg√∫n el contexto
‚úÖ **Aplicar consideraciones legales** (GDPR, t√©rminos de servicio)

---

## üìö Introducci√≥n

### ¬øPor qu√© importa el Web Scraping en Data Engineering?

Imagina que necesitas analizar los precios de 10,000 productos de un e-commerce para un estudio de mercado, pero **no tienen una API p√∫blica**. O quieres monitorear las ofertas de empleo en LinkedIn para detectar tendencias del sector, pero su API est√° limitada. ¬øQu√© haces? ü§î

**Web scraping** es tu respuesta.

En **Data Engineering**, aproximadamente el **30-40% de la extracci√≥n de datos** proviene de scraping porque:
- Muchas fuentes NO tienen APIs
- Algunas APIs son muy costosas o restrictivas
- A veces necesitas datos que solo est√°n en la interfaz visual

**Empresas que usan scraping masivo:**
- **Google**: Scrapers para indexar la web
- **Amazon**: Monitoreo de precios de competidores
- **DataHub Inc.**: Extracci√≥n de datos de mercado para an√°lisis

---

## üåê ¬øQu√© es Web Scraping?

### Definici√≥n

> **Web scraping** es el proceso automatizado de extraer datos de sitios web mediante la descarga y an√°lisis del HTML de las p√°ginas.

### Analog√≠a: La Biblioteca

Imagina que eres un investigador en una **biblioteca gigante sin sistema computarizado**:

- üìö **Libros** = P√°ginas web
- üìñ **P√°ginas de libro** = HTML
- üîç **T√∫ buscando manualmente** = Navegador humano
- ü§ñ **Tu asistente robot** = Web scraper

Tu robot:
1. Va a la estanter√≠a (hace request HTTP)
2. Abre el libro (descarga HTML)
3. Lee l√≠nea por l√≠nea (parsea HTML)
4. Anota lo importante en una ficha (extrae datos)
5. Guarda las fichas en un archivero (almacena en CSV/DB)

¬°Hace en segundos lo que te tomar√≠a horas! ‚ö°

---

### El Proceso de Web Scraping

```
1Ô∏è‚É£ REQUEST HTTP
   ‚Üì
   GET https://ejemplo.com/productos

2Ô∏è‚É£ RESPUESTA HTML
   ‚Üì
   <html><body><div class="producto">...</div></body></html>

3Ô∏è‚É£ PARSING
   ‚Üì
   BeautifulSoup/Selenium parsea el HTML

4Ô∏è‚É£ EXTRACCI√ìN
   ‚Üì
   Encuentra elementos con selectores (CSS, XPath)

5Ô∏è‚É£ LIMPIEZA
   ‚Üì
   Elimina espacios, formatea datos

6Ô∏è‚É£ ALMACENAMIENTO
   ‚Üì
   Guarda en CSV, JSON, SQLite, etc.
```

---

## üèóÔ∏è HTML, CSS y DOM

### ¬øQu√© es HTML?

**HTML** (HyperText Markup Language) es el lenguaje de estructura de las p√°ginas web.

**Ejemplo de HTML simple:**

```html
<!DOCTYPE html>
<html>
<head>
    <title>Productos - DataHub Store</title>
</head>
<body>
    <h1>Nuestros Productos</h1>

    <div class="producto" id="prod-001">
        <h2 class="titulo">Laptop Dell XPS 13</h2>
        <span class="precio">$1,299.99</span>
        <p class="descripcion">Ultrabook potente para profesionales</p>
        <span class="stock">En stock</span>
    </div>

    <div class="producto" id="prod-002">
        <h2 class="titulo">Mouse Logitech MX</h2>
        <span class="precio">$99.99</span>
        <p class="descripcion">Mouse ergon√≥mico inal√°mbrico</p>
        <span class="stock">Agotado</span>
    </div>
</body>
</html>
```

### Anatom√≠a de una Etiqueta HTML

```html
<div class="producto" id="prod-001">
  ‚Üë     ‚Üë               ‚Üë
  |     |               ‚îî‚îÄ Atributo: id (√∫nico)
  |     ‚îî‚îÄ Atributo: class (puede repetirse)
  ‚îî‚îÄ Tag (etiqueta)
```

**Etiquetas comunes en scraping:**

| Etiqueta       | Uso T√≠pico          | Ejemplo                                 |
| -------------- | ------------------- | --------------------------------------- |
| `<div>`        | Contenedor gen√©rico | `<div class="card">`                    |
| `<span>`       | Contenedor inline   | `<span class="price">$99</span>`        |
| `<h1>`, `<h2>` | T√≠tulos             | `<h1>Producto</h1>`                     |
| `<p>`          | P√°rrafos            | `<p>Descripci√≥n...</p>`                 |
| `<a>`          | Enlaces             | `<a href="/producto">Ver</a>`           |
| `<table>`      | Tablas              | `<table><tr><td>Dato</td></tr></table>` |
| `<ul>`, `<li>` | Listas              | `<ul><li>Item 1</li></ul>`              |

---

### ¬øQu√© es el DOM?

**DOM** (Document Object Model) es la representaci√≥n en forma de **√°rbol** del HTML.

**Analog√≠a:** El DOM es como un **√°rbol geneal√≥gico** de una familia.

```
html (abuelo)
‚îú‚îÄ‚îÄ head (t√≠o)
‚îÇ   ‚îî‚îÄ‚îÄ title (primo)
‚îî‚îÄ‚îÄ body (padre)
    ‚îú‚îÄ‚îÄ h1 (hermano mayor)
    ‚îî‚îÄ‚îÄ div.producto (t√∫)
        ‚îú‚îÄ‚îÄ h2.titulo (hijo 1)
        ‚îú‚îÄ‚îÄ span.precio (hijo 2)
        ‚îî‚îÄ‚îÄ p.descripcion (hijo 3)
```

**Terminolog√≠a:**
- **Padre** (parent): Elemento que contiene a otro
- **Hijo** (child): Elemento contenido dentro de otro
- **Hermano** (sibling): Elementos al mismo nivel
- **Descendiente** (descendant): Cualquier elemento anidado (hijo, nieto, etc.)

---

### ¬øQu√© es CSS?

**CSS** (Cascading Style Sheets) define c√≥mo se ven los elementos HTML.

**Para scraping, lo importante son los SELECTORES CSS:**

```css
/* Selector por clase */
.producto { color: blue; }

/* Selector por ID */
#prod-001 { font-weight: bold; }

/* Selector por etiqueta */
h2 { font-size: 20px; }

/* Selector combinado */
div.producto span.precio { color: green; }
```

Usaremos estos selectores para **encontrar elementos** al hacer scraping.

---

## ü•£ BeautifulSoup: Parsing de HTML

### ¬øQu√© es BeautifulSoup?

**BeautifulSoup** es una biblioteca de Python para parsear HTML/XML y extraer datos.

**Analog√≠a:** Es como tener un **GPS inteligente** para navegar por el HTML.

### Instalaci√≥n

```bash
pip install beautifulsoup4 requests
```

### Ejemplo B√°sico

```python
from bs4 import BeautifulSoup
import requests

# 1. Descargar HTML
url = "https://ejemplo.com/productos"
response = requests.get(url)
html = response.text

# 2. Parsear con BeautifulSoup
soup = BeautifulSoup(html, 'html.parser')

# 3. Extraer datos
titulo = soup.find('h1').text
print(titulo)  # "Nuestros Productos"

# 4. Encontrar todos los productos
productos = soup.find_all('div', class_='producto')
for prod in productos:
    nombre = prod.find('h2', class_='titulo').text
    precio = prod.find('span', class_='precio').text
    print(f"{nombre}: {precio}")
```

### M√©todos Principales de BeautifulSoup

| M√©todo         | Descripci√≥n                                 | Ejemplo                                   |
| -------------- | ------------------------------------------- | ----------------------------------------- |
| `find()`       | Encuentra EL PRIMER elemento que coincida   | `soup.find('h1')`                         |
| `find_all()`   | Encuentra TODOS los elementos que coincidan | `soup.find_all('div', class_='producto')` |
| `select()`     | Usa selectores CSS                          | `soup.select('div.producto span.precio')` |
| `select_one()` | Primer elemento con selector CSS            | `soup.select_one('#prod-001')`            |
| `.text`        | Extrae el texto dentro del elemento         | `elemento.text`                           |
| `.get()`       | Extrae el valor de un atributo              | `enlace.get('href')`                      |

---

### Navegaci√≥n en el DOM

```python
# Obtener el padre
producto = soup.find('div', class_='producto')
body = producto.parent

# Obtener hijos directos
hijos = list(producto.children)

# Obtener todos los descendientes
descendientes = list(producto.descendants)

# Obtener el siguiente hermano
siguiente = producto.next_sibling

# Obtener el hermano anterior
anterior = producto.previous_sibling
```

---

## üîß Selenium: Scraping Din√°mico

### ¬øQu√© es Selenium?

**Selenium** es una herramienta para **automatizar navegadores web**. Simula un usuario real interactuando con la p√°gina.

### ¬øCu√°ndo usar Selenium?

‚úÖ **Usa Selenium cuando:**
- La p√°gina carga contenido con **JavaScript** (React, Vue, Angular)
- Necesitas **hacer clic** en botones o llenar formularios
- La p√°gina tiene **scroll infinito** o carga lazy
- Necesitas **esperar** a que aparezcan elementos

‚ùå **NO uses Selenium si:**
- El HTML est√°tico ya tiene los datos (usa BeautifulSoup, es m√°s r√°pido)
- La p√°gina tiene una API oculta que puedes usar

**Analog√≠a:** BeautifulSoup es como **leer el peri√≥dico**, Selenium es como **ver un video interactivo**.

---

### Instalaci√≥n de Selenium

```bash
pip install selenium

# Descargar ChromeDriver (o Firefox geckodriver)
# https://chromedriver.chromium.org/
```

### Ejemplo B√°sico con Selenium

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# 1. Inicializar el navegador
driver = webdriver.Chrome()

# 2. Abrir p√°gina
driver.get("https://ejemplo.com/productos")

# 3. Esperar a que cargue el contenido JavaScript
wait = WebDriverWait(driver, 10)
productos = wait.until(
    EC.presence_of_all_elements_located((By.CLASS_NAME, "producto"))
)

# 4. Extraer datos
for prod in productos:
    titulo = prod.find_element(By.CLASS_NAME, "titulo").text
    precio = prod.find_element(By.CLASS_NAME, "precio").text
    print(f"{titulo}: {precio}")

# 5. Cerrar navegador
driver.quit()
```

---

### Estrategias de Espera en Selenium

**Problema:** JavaScript tarda en cargar, ¬°no puedes extraer lo que no ha aparecido!

| Tipo de Espera | Descripci√≥n                    | Cu√°ndo Usar         |
| -------------- | ------------------------------ | ------------------- |
| **Impl√≠cita**  | Espera global autom√°tica       | Para todo el script |
| **Expl√≠cita**  | Espera a condici√≥n espec√≠fica  | Elementos cr√≠ticos  |
| `time.sleep()` | ‚ùå NO RECOMENDADO (tiempo fijo) | Solo para debugging |

**Ejemplo de espera expl√≠cita:**

```python
from selenium.webdriver.support import expected_conditions as EC

# Esperar hasta que aparezca un elemento
elemento = WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.ID, "producto-123"))
)

# Esperar hasta que sea clickeable
boton = WebDriverWait(driver, 10).until(
    EC.element_to_be_clickable((By.ID, "cargar-mas"))
)
boton.click()
```

---

## ü§ñ Robots.txt y √âtica del Scraping

### ¬øQu√© es robots.txt?

**robots.txt** es un archivo en la ra√≠z del sitio web que indica qu√© partes pueden ser scrapeadas.

**Ubicaci√≥n:** `https://ejemplo.com/robots.txt`

**Ejemplo de robots.txt:**

```txt
User-agent: *
Disallow: /admin/
Disallow: /carrito/
Crawl-delay: 2

User-agent: Googlebot
Allow: /

User-agent: BadBot
Disallow: /
```

**Interpretaci√≥n:**
- `User-agent: *` = Para todos los bots
- `Disallow: /admin/` = NO scrapees /admin/
- `Crawl-delay: 2` = Espera 2 segundos entre requests
- `Allow: /` = S√ç puedes scrapear todo

---

### ¬øC√≥mo respetar robots.txt en Python?

```python
import urllib.robotparser

# 1. Parsear robots.txt
rp = urllib.robotparser.RobotFileParser()
rp.set_url("https://ejemplo.com/robots.txt")
rp.read()

# 2. Verificar si puedes scrapear una URL
url = "https://ejemplo.com/productos"
user_agent = "MiScraper/1.0"

if rp.can_fetch(user_agent, url):
    print("‚úÖ Permitido scrapear")
    # Hacer scraping...
else:
    print("‚ùå No permitido por robots.txt")
```

---

### √âtica del Scraping

**Regla de oro:** ü•á **Trata los servidores ajenos como te gustar√≠a que trataran el tuyo.**

‚úÖ **Haz:**
- Respeta robots.txt
- Limita velocidad (rate limiting)
- Identif√≠cate con User-Agent claro
- Scrapea en horarios de baja demanda
- Cachea respuestas para no repetir requests

‚ùå **NO hagas:**
- Ignorar robots.txt
- Hacer cientos de requests por segundo
- Scraping de datos personales sensibles
- Revender datos scrapeados sin permiso
- Sobrecargar servidores peque√±os

**Analog√≠a:** Es como visitar una tienda f√≠sica:
- ‚úÖ Est√° bien mirar productos y anotar precios
- ‚ùå NO est√° bien entrar a la bodega ni robar
- ‚úÖ Est√° bien tomar fotos (si est√° permitido)
- ‚ùå NO est√° bien bloquear la entrada para otros clientes

---

## üïµÔ∏è User-Agent y Headers

### ¬øQu√© es un User-Agent?

**User-Agent** es una cadena de texto que identifica qui√©n est√° haciendo el request (navegador, bot, etc.).

**Ejemplo de User-Agent de Chrome:**

```
Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36
```

### ¬øPor qu√© importa?

Muchos sitios **bloquean requests sin User-Agent** o con User-Agents de bots conocidos.

**Ejemplo de bloqueo:**

```python
import requests

# ‚ùå Request sin User-Agent (puede ser bloqueado)
response = requests.get("https://ejemplo.com")

# ‚úÖ Request con User-Agent (se ve como navegador)
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}
response = requests.get("https://ejemplo.com", headers=headers)
```

---

### Headers Importantes para Scraping

| Header            | Descripci√≥n                  | Ejemplo                      |
| ----------------- | ---------------------------- | ---------------------------- |
| `User-Agent`      | Identifica el cliente        | `Mozilla/5.0 ...`            |
| `Accept`          | Tipos de contenido aceptados | `text/html,application/json` |
| `Accept-Language` | Idioma preferido             | `es-ES,es;q=0.9,en;q=0.8`    |
| `Referer`         | De d√≥nde vienes              | `https://google.com`         |
| `Cookie`          | Cookies de sesi√≥n            | `session_id=abc123`          |

**Ejemplo completo de headers:**

```python
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "es-ES,es;q=0.9",
    "Referer": "https://google.com",
    "DNT": "1",  # Do Not Track
}

response = requests.get("https://ejemplo.com", headers=headers)
```

---

## üéØ XPath y CSS Selectors

### ¬øQu√© son los Selectores?

**Selectores** son patrones para encontrar elementos espec√≠ficos en el HTML.

**Analog√≠a:** Son como **direcciones postales** para encontrar casas en una ciudad.

---

### CSS Selectors

**Sintaxis m√°s com√∫n:**

| Selector                | Significado            | Ejemplo               |
| ----------------------- | ---------------------- | --------------------- |
| `.clase`                | Por clase              | `.producto`           |
| `#id`                   | Por ID                 | `#prod-001`           |
| `elemento`              | Por etiqueta           | `div`, `span`, `h2`   |
| `padre > hijo`          | Hijo directo           | `div > span`          |
| `ancestro descendiente` | Cualquier descendiente | `div span`            |
| `[atributo]`            | Con atributo           | `[href]`, `[data-id]` |
| `[atributo="valor"]`    | Atributo con valor     | `[href="/productos"]` |

**Ejemplos pr√°cticos:**

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(html, 'html.parser')

# Seleccionar por clase
productos = soup.select('.producto')

# Seleccionar por ID
prod_001 = soup.select_one('#prod-001')

# Selector combinado: dentro de div.producto, todos los span.precio
precios = soup.select('div.producto span.precio')

# Elementos con atributo href
enlaces = soup.select('a[href]')

# Enlaces que empiezan con /productos
enlaces_productos = soup.select('a[href^="/productos"]')
```

---

### XPath

**XPath** es un lenguaje para navegar por XML/HTML (m√°s potente pero m√°s complejo).

**Sintaxis b√°sica:**

| Expresi√≥n     | Significado        | Ejemplo                    |
| ------------- | ------------------ | -------------------------- |
| `/`           | Desde ra√≠z         | `/html/body/div`           |
| `//`          | Cualquier nivel    | `//div[@class="producto"]` |
| `@atributo`   | Atributo           | `//a/@href`                |
| `text()`      | Texto del elemento | `//h2/text()`              |
| `[condici√≥n]` | Filtro             | `//div[@class="producto"]` |

**Ejemplos pr√°cticos con Selenium:**

```python
from selenium.webdriver.common.by import By

# Por XPath absoluto (fr√°gil, no recomendado)
elemento = driver.find_element(By.XPATH, "/html/body/div[1]/h2")

# Por XPath con clase
productos = driver.find_elements(By.XPATH, "//div[@class='producto']")

# Texto que contiene
titulo = driver.find_element(By.XPATH, "//h2[contains(text(), 'Laptop')]")

# Por atributo
enlace = driver.find_element(By.XPATH, "//a[@href='/productos']")

# Combinado: precio dentro de producto con ID espec√≠fico
precio = driver.find_element(
    By.XPATH,
    "//div[@id='prod-001']//span[@class='precio']"
)
```

---

### CSS vs XPath: ¬øCu√°l usar?

| Aspecto                     | CSS Selectors | XPath            |
| --------------------------- | ------------- | ---------------- |
| **Sintaxis**                | ‚úÖ M√°s simple  | ‚ùå M√°s compleja   |
| **Velocidad**               | ‚úÖ M√°s r√°pido  | ‚ùå M√°s lento      |
| **Poder**                   | ‚ùå Limitado    | ‚úÖ Muy potente    |
| **Navegaci√≥n hacia arriba** | ‚ùå No puede    | ‚úÖ Puede (parent) |
| **Texto exacto**            | ‚ùå Dif√≠cil     | ‚úÖ F√°cil          |

**Recomendaci√≥n:** Usa **CSS Selectors** por defecto, solo usa **XPath** cuando necesites:
- Navegar hacia arriba (parent)
- Buscar por texto exacto
- L√≥gica compleja (AND, OR)

---

## üíæ Almacenamiento de Datos Scrapeados

### Opciones de Almacenamiento

| Formato              | Pros                     | Contras                     | Cu√°ndo Usar                 |
| -------------------- | ------------------------ | --------------------------- | --------------------------- |
| **CSV**              | Simple, Excel-compatible | No soporta anidaci√≥n        | Datos tabulares simples     |
| **JSON**             | Flexible, anidado        | Menos eficiente para tablas | Datos jer√°rquicos           |
| **SQLite**           | Relacional, queries SQL  | M√°s complejo                | Datos estructurados grandes |
| **Pandas DataFrame** | An√°lisis f√°cil           | En memoria                  | An√°lisis inmediato          |

---

### Ejemplo: Guardar en CSV

```python
import csv
from bs4 import BeautifulSoup

# Scrapear datos
productos = []
for prod in soup.find_all('div', class_='producto'):
    productos.append({
        'nombre': prod.find('h2', class_='titulo').text.strip(),
        'precio': prod.find('span', class_='precio').text.strip(),
        'stock': prod.find('span', class_='stock').text.strip()
    })

# Guardar en CSV
with open('productos.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=['nombre', 'precio', 'stock'])
    writer.writeheader()
    writer.writerows(productos)
```

---

### Ejemplo: Guardar en JSON

```python
import json

# Scrapear datos (igual que antes)
productos = [...]  # Lista de diccionarios

# Guardar en JSON
with open('productos.json', 'w', encoding='utf-8') as f:
    json.dump(productos, f, indent=2, ensure_ascii=False)
```

---

### Ejemplo: Guardar en SQLite

```python
import sqlite3

# Crear base de datos
conn = sqlite3.connect('productos.db')
cursor = conn.cursor()

# Crear tabla
cursor.execute('''
    CREATE TABLE IF NOT EXISTS productos (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        nombre TEXT NOT NULL,
        precio TEXT,
        stock TEXT,
        fecha_scraping TEXT
    )
''')

# Insertar datos
from datetime import datetime

for prod in productos:
    cursor.execute('''
        INSERT INTO productos (nombre, precio, stock, fecha_scraping)
        VALUES (?, ?, ?, ?)
    ''', (
        prod['nombre'],
        prod['precio'],
        prod['stock'],
        datetime.now().isoformat()
    ))

conn.commit()
conn.close()
```

---

## ‚öñÔ∏è Web Scraping vs APIs

### ¬øCu√°ndo usar cada uno?

| Criterio                | API REST                | Web Scraping           |
| ----------------------- | ----------------------- | ---------------------- |
| **Disponibilidad**      | ‚úÖ Existe API p√∫blica    | ‚ùå No hay API           |
| **Costo**               | ‚ùå A veces pago          | ‚úÖ Gratis (si es legal) |
| **Velocidad**           | ‚úÖ R√°pido (JSON)         | ‚ùå M√°s lento (HTML)     |
| **Estabilidad**         | ‚úÖ Estable (versionado)  | ‚ùå Cambia con UI        |
| **Legalidad**           | ‚úÖ Siempre legal         | ‚ö†Ô∏è Depende (TOS)        |
| **L√≠mites**             | ‚ö†Ô∏è Rate limits estrictos | ‚ö†Ô∏è Puedes ser bloqueado |
| **Datos estructurados** | ‚úÖ JSON estructurado     | ‚ùå HTML no estructurado |

**Regla general:**
1. ü•á **Primero busca una API**
2. ü•à Si no hay API, revisa **t√©rminos de servicio**
3. ü•â Si es legal y √©tico, **usa scraping**

---

### Ejemplo de Decisi√≥n

**Caso 1: Twitter**
- ‚úÖ Tiene API oficial (Twitter API v2)
- ‚ùå Scraping viola t√©rminos de servicio
- **Decisi√≥n:** Usa la API (aunque sea de pago)

**Caso 2: E-commerce local sin API**
- ‚ùå No tiene API p√∫blica
- ‚úÖ Robots.txt permite scraping
- ‚úÖ T√©rminos de servicio no lo proh√≠ben
- **Decisi√≥n:** Scraping √©tico con rate limiting

**Caso 3: Sitio de noticias**
- ‚úÖ Tiene RSS feed (como API simple)
- ‚ö†Ô∏è Scraping t√©cnicamente posible pero innecesario
- **Decisi√≥n:** Usa RSS feed (m√°s eficiente)

---

## ‚öñÔ∏è Legalidad y √âtica

### Marco Legal

**GDPR (Europa):**
- ‚ùå NO scrapees datos personales sin consentimiento
- ‚ùå NO almacenes datos de ciudadanos EU sin cumplir GDPR
- ‚úÖ Datos p√∫blicos de empresas suelen estar OK

**CCPA (California):**
- ‚ùå NO vendas datos scrapeados de residentes de California
- ‚úÖ Derecho a eliminar datos si te lo piden

**CFAA (EE.UU.):**
- ‚ùå "Acceso no autorizado a sistemas" puede incluir scraping agresivo
- ‚ö†Ô∏è Violar t√©rminos de servicio puede tener consecuencias legales

---

### T√©rminos de Servicio (TOS)

**Ejemplo de TOS que proh√≠be scraping:**

> "Est√° prohibido el uso de bots, scrapers o cualquier m√©todo automatizado para acceder a nuestro sitio sin permiso expl√≠cito."

**¬øQu√© hacer?**
1. Lee SIEMPRE los t√©rminos de servicio
2. Si proh√≠ben scraping, **NO lo hagas** (o pide permiso)
3. Si no dicen nada, procede con **√©tica y rate limiting**

---

### Casos Legales Importantes

**hiQ Labs vs LinkedIn (2019):**
- LinkedIn bloque√≥ scraping de perfiles p√∫blicos
- Corte decidi√≥: **Datos p√∫blicos pueden ser scrapeados**
- Pero LinkedIn puede actualizar TOS y bloquear

**Conclusion:** Legal ‚â† √âtico. Incluso si es legal, respeta los servidores.

---

## ‚ùå Errores Comunes

### 1. No Verificar robots.txt

```python
# ‚ùå MAL: Scrapear sin verificar
response = requests.get("https://ejemplo.com/admin")

# ‚úÖ BIEN: Verificar primero
import urllib.robotparser

rp = urllib.robotparser.RobotFileParser()
rp.set_url("https://ejemplo.com/robots.txt")
rp.read()

if rp.can_fetch("MiBot/1.0", url):
    response = requests.get(url)
```

---

### 2. No Usar User-Agent

```python
# ‚ùå MAL: Sin User-Agent (alto riesgo de bloqueo)
response = requests.get(url)

# ‚úÖ BIEN: Con User-Agent realista
headers = {"User-Agent": "Mozilla/5.0 ..."}
response = requests.get(url, headers=headers)
```

---

### 3. Hacer Requests Demasiado R√°pido

```python
# ‚ùå MAL: Sin rate limiting (abusivo)
for url in urls:
    response = requests.get(url)

# ‚úÖ BIEN: Con rate limiting
import time

for url in urls:
    response = requests.get(url)
    time.sleep(2)  # Espera 2 segundos entre requests
```

---

### 4. No Manejar Errores

```python
# ‚ùå MAL: Sin manejo de errores
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
titulo = soup.find('h1').text  # ¬°Puede fallar!

# ‚úÖ BIEN: Con manejo de errores
try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    titulo_elem = soup.find('h1')
    titulo = titulo_elem.text.strip() if titulo_elem else "Sin t√≠tulo"

except requests.exceptions.RequestException as e:
    print(f"Error al hacer request: {e}")
except AttributeError:
    print("Elemento no encontrado en HTML")
```

---

### 5. Usar Selenium cuando BeautifulSoup es Suficiente

```python
# ‚ùå MAL: Selenium innecesario (lento)
driver = webdriver.Chrome()
driver.get(url)
titulo = driver.find_element(By.TAG_NAME, "h1").text

# ‚úÖ BIEN: BeautifulSoup si HTML es est√°tico
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
titulo = soup.find('h1').text
```

**Regla:** Usa Selenium solo si BeautifulSoup no funciona (JavaScript din√°mico).

---

## ‚úÖ Buenas Pr√°cticas

### 1. Limita la Velocidad (Rate Limiting)

```python
import time

DELAY_SECONDS = 2  # 2 segundos entre requests

for url in urls:
    response = requests.get(url)
    # Procesar...
    time.sleep(DELAY_SECONDS)
```

---

### 2. Identif√≠cate con User-Agent Claro

```python
headers = {
    "User-Agent": "MiProyectoAnalisis/1.0 (contacto@ejemplo.com)"
}
```

Si eres identificable, el administrador puede contactarte en vez de bloquearte.

---

### 3. Cachea Respuestas

```python
import requests_cache

# Cachear por 1 hora
requests_cache.install_cache('scraper_cache', expire_after=3600)

# Ahora requests usa cach√© autom√°ticamente
response = requests.get(url)  # Primera vez: request real
response = requests.get(url)  # Segunda vez: desde cach√©
```

---

### 4. Usa Timeouts

```python
response = requests.get(url, timeout=10)  # Falla si tarda >10 segundos
```

---

### 5. Logging Completo

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info(f"Scraping URL: {url}")
logger.warning(f"Elemento no encontrado: {selector}")
logger.error(f"Error HTTP {response.status_code}: {url}")
```

---

### 6. Respeta Horarios de Baja Demanda

Scrapea en:
- ‚úÖ Madrugada (2am - 6am hora local del servidor)
- ‚úÖ Fines de semana (si es sitio B2B)
- ‚ùå Evita horas pico (12pm - 2pm, 6pm - 9pm)

---

## ‚úÖ Checklist de Aprendizaje

Marca lo que ya dominas:

### Conceptos Fundamentales
- [ ] Entiendo qu√© es web scraping y cu√°ndo usarlo
- [ ] Conozco la estructura b√°sica de HTML (tags, clases, IDs)
- [ ] Comprendo el DOM como √°rbol de elementos
- [ ] S√© qu√© es el CSS y c√≥mo funcionan los selectores

### BeautifulSoup
- [ ] Puedo parsear HTML con BeautifulSoup
- [ ] Uso `find()` y `find_all()` correctamente
- [ ] Domino `select()` con CSS selectors
- [ ] Extraigo texto con `.text` y atributos con `.get()`

### Selenium
- [ ] Entiendo cu√°ndo usar Selenium vs BeautifulSoup
- [ ] Puedo inicializar un navegador con WebDriver
- [ ] Uso esperas expl√≠citas con `WebDriverWait`
- [ ] Extraigo datos de p√°ginas con JavaScript din√°mico

### √âtica y Legalidad
- [ ] S√© verificar y respetar robots.txt
- [ ] Configuro User-Agent apropiado
- [ ] Aplico rate limiting en mis scrapers
- [ ] Conozco las implicaciones legales (GDPR, TOS)

### Selectores
- [ ] Domino selectores CSS b√°sicos (`.clase`, `#id`, `elemento`)
- [ ] Uso selectores combinados (`padre > hijo`)
- [ ] Conozco selectores de atributos (`[href]`)
- [ ] Puedo usar XPath cuando CSS no es suficiente

### Almacenamiento
- [ ] Puedo guardar datos en CSV
- [ ] Puedo guardar datos en JSON
- [ ] Puedo guardar datos en SQLite
- [ ] S√© elegir el formato seg√∫n el caso

### Buenas Pr√°cticas
- [ ] Manejo errores con try/except
- [ ] Uso timeouts en requests
- [ ] Implemento logging en mis scrapers
- [ ] Cacheo respuestas para evitar requests duplicados
- [ ] Conozco la diferencia entre scraping y uso de APIs

---

## üéì Conclusi√≥n

**Web scraping** es una herramienta poderosa pero que **requiere responsabilidad**:

‚úÖ **Usa scraping cuando:**
- No hay API disponible
- Es legal y √©tico seg√∫n TOS
- Respetas robots.txt y rate limiting

‚ùå **NO uses scraping si:**
- Existe una API p√∫blica
- Los t√©rminos de servicio lo proh√≠ben
- Extraes datos personales sin consentimiento

**Recuerda:** El scraping es como conducir un coche: puedes llegar lejos, pero debes **respetar las reglas de tr√°fico** (robots.txt, rate limiting, √©tica) para evitar accidentes (bloqueos, problemas legales).

---

## üìö Recursos Adicionales

**Documentaci√≥n Oficial:**
- [BeautifulSoup Docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Selenium Python Docs](https://selenium-python.readthedocs.io/)
- [CSS Selectors Reference](https://www.w3schools.com/cssref/css_selectors.asp)

**Herramientas √ötiles:**
- [SelectorGadget](https://selectorgadget.com/) - Encontrar selectores CSS
- [Scrapy](https://scrapy.org/) - Framework avanzado de scraping
- [Requests-HTML](https://requests.readthedocs.io/projects/requests-html/) - Alternativa a BeautifulSoup

**Lecturas Recomendadas:**
- "Web Scraping with Python" - Ryan Mitchell
- Robots Exclusion Protocol: [robotstxt.org](https://www.robotstxt.org/)

---

**¬°Ahora est√°s listo para los ejemplos pr√°cticos! üöÄ**

üìù **Siguiente paso:** [02-EJEMPLOS.md](02-EJEMPLOS.md) - 5 ejemplos trabajados de scraping con BeautifulSoup y Selenium.

---

*√öltima actualizaci√≥n: 2025-10-23*
*Tema 2 - M√≥dulo 4 - Master en Ingenier√≠a de Datos*
