# üìò Tema 2: Web Scraping - Extracci√≥n √âtica de Datos Web

**M√≥dulo 4:** APIs y Web Scraping
**Duraci√≥n estimada:** 1-1.5 semanas (12-18 horas)
**Nivel:** Intermedio
**Prerequisitos:** Tema 1 (APIs REST), Python b√°sico, HTTP b√°sico

---

## üéØ Objetivos de Aprendizaje

Al completar este tema, ser√°s capaz de:

‚úÖ **Parsear HTML** con BeautifulSoup para extraer datos estructurados
‚úÖ **Scrapear contenido din√°mico** cargado con JavaScript usando Selenium
‚úÖ **Respetar robots.txt** y aplicar rate limiting √©tico
‚úÖ **Extraer tablas, enlaces y atributos** de p√°ginas web complejas
‚úÖ **Almacenar datos scrapeados** en SQLite con validaci√≥n
‚úÖ **Decidir cu√°ndo usar APIs vs Web Scraping** seg√∫n el caso de uso
‚úÖ **Construir scrapers completos** con manejo de errores, logging y TDD

---

## üìö Conceptos Clave

### üîç Web Scraping
> **Analog√≠a:** Es como ser un bibliotecario que extrae informaci√≥n espec√≠fica de libros (p√°ginas web) y la organiza en un cat√°logo estructurado (base de datos).

**Cu√°ndo usar Web Scraping:**
- ‚ùå El sitio NO tiene API p√∫blica
- ‚úÖ Los datos est√°n en HTML accesible
- ‚úÖ El sitio lo permite (robots.txt)
- ‚úÖ Tienes caso de uso leg√≠timo (an√°lisis, investigaci√≥n)

### üß± Conceptos Fundamentales

| Concepto          | Descripci√≥n                         | Herramienta                  |
| ----------------- | ----------------------------------- | ---------------------------- |
| **HTML/DOM**      | Estructura de la p√°gina web         | Navegador DevTools           |
| **BeautifulSoup** | Parseo de HTML est√°tico             | `beautifulsoup4`             |
| **Selenium**      | Scraping de contenido din√°mico (JS) | `selenium`                   |
| **Robots.txt**    | Reglas de scraping del sitio        | `urllib.robotparser`         |
| **Rate Limiting** | Respetar l√≠mites del servidor       | `time.sleep()`               |
| **CSS Selectors** | Selecci√≥n de elementos HTML         | `.select()`, `.select_one()` |
| **XPath**         | Alternativa a CSS Selectors         | `lxml` (opcional)            |
| **User-Agent**    | Identificaci√≥n en requests HTTP     | `requests.headers`           |

---

## üìÅ Estructura del Tema

```
tema-2-web-scraping/
‚îú‚îÄ‚îÄ 01-TEORIA.md                  # ~5,200 palabras (40-50 min)
‚îÇ   ‚îú‚îÄ‚îÄ HTML, CSS, DOM
‚îÇ   ‚îú‚îÄ‚îÄ BeautifulSoup
‚îÇ   ‚îú‚îÄ‚îÄ Selenium
‚îÇ   ‚îú‚îÄ‚îÄ Robots.txt y √©tica
‚îÇ   ‚îî‚îÄ‚îÄ Comparaci√≥n Scraping vs APIs
‚îÇ
‚îú‚îÄ‚îÄ 02-EJEMPLOS.md                # 5 ejemplos trabajados (60-90 min)
‚îÇ   ‚îú‚îÄ‚îÄ Ejemplo 1: Scraping b√°sico con BeautifulSoup
‚îÇ   ‚îú‚îÄ‚îÄ Ejemplo 2: Extraer tabla HTML ‚Üí CSV
‚îÇ   ‚îú‚îÄ‚îÄ Ejemplo 3: Navegaci√≥n multi-p√°gina
‚îÇ   ‚îú‚îÄ‚îÄ Ejemplo 4: Selenium para contenido din√°mico
‚îÇ   ‚îî‚îÄ‚îÄ Ejemplo 5: Scraper completo con SQLite
‚îÇ
‚îú‚îÄ‚îÄ 03-EJERCICIOS.md              # 15 ejercicios con soluciones (8-12 horas)
‚îÇ   ‚îú‚îÄ‚îÄ B√°sicos (1-5): HTML, BeautifulSoup, robots.txt
‚îÇ   ‚îú‚îÄ‚îÄ Intermedios (6-10): Tablas, navegaci√≥n, Selenium
‚îÇ   ‚îî‚îÄ‚îÄ Avanzados (11-15): Rate limiting, pipelines completos
‚îÇ
‚îú‚îÄ‚îÄ 04-proyecto-practico/         # Proyecto TDD completo (4-6 horas)
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraper_html.py       # BeautifulSoup (5 funciones)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraper_selenium.py   # Selenium (3 funciones)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validador_scraping.py # Robots.txt, rate limiting (4 funciones)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ almacenamiento.py     # SQLite (3 funciones)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utilidades_scraping.py # Logging, headers (4 funciones)
‚îÇ   ‚îú‚îÄ‚îÄ tests/                    # 71 tests, 90% cobertura
‚îÇ   ‚îú‚îÄ‚îÄ ejemplos/                 # HTML de ejemplo
‚îÇ   ‚îú‚îÄ‚îÄ datos/                    # Base de datos SQLite
‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # Documentaci√≥n completa
‚îÇ
‚îú‚îÄ‚îÄ REVISION_PEDAGOGICA.md        # Evaluaci√≥n: 9.3/10 ‚≠ê
‚îî‚îÄ‚îÄ README.md                     # Este archivo

```

---

## üöÄ C√≥mo Estudiar Este Tema

### Ruta de Aprendizaje Recomendada

```
1Ô∏è‚É£ TEOR√çA (40-50 min)
   ‚îî‚îÄ> Lee 01-TEORIA.md completo
   ‚îî‚îÄ> Prueba los snippets en Jupyter/Python REPL
   ‚îî‚îÄ> Instala: beautifulsoup4, selenium, webdriver-manager

2Ô∏è‚É£ EJEMPLOS (60-90 min)
   ‚îî‚îÄ> Sigue 02-EJEMPLOS.md paso a paso
   ‚îî‚îÄ> Ejecuta cada ejemplo completo
   ‚îî‚îÄ> Modifica variables para experimentar

3Ô∏è‚É£ EJERCICIOS B√ÅSICOS (2-3 horas)
   ‚îî‚îÄ> Resuelve ejercicios 1-5 en 03-EJERCICIOS.md
   ‚îî‚îÄ> Consulta hints si te atascas
   ‚îî‚îÄ> Compara con soluciones al final

4Ô∏è‚É£ EJERCICIOS INTERMEDIOS (3-4 horas)
   ‚îî‚îÄ> Resuelve ejercicios 6-10
   ‚îî‚îÄ> Introduce Selenium en ejercicio 10
   ‚îî‚îÄ> Practica navegaci√≥n multi-p√°gina

5Ô∏è‚É£ EJERCICIOS AVANZADOS (3-5 horas)
   ‚îî‚îÄ> Resuelve ejercicios 11-15
   ‚îî‚îÄ> Implementa rate limiting y robots.txt
   ‚îî‚îÄ> Construye pipeline completo (scrape ‚Üí clean ‚Üí store)

6Ô∏è‚É£ PROYECTO PR√ÅCTICO (4-6 horas)
   ‚îî‚îÄ> Estudia arquitectura en 04-proyecto-practico/README.md
   ‚îî‚îÄ> Lee los tests para entender requisitos
   ‚îî‚îÄ> Ejecuta pytest -v para ver 71 tests pasando
   ‚îî‚îÄ> Modifica y extiende funciones para tu caso de uso

7Ô∏è‚É£ PR√ÅCTICA PERSONAL (Opcional, 2-4 horas)
   ‚îî‚îÄ> Elige un sitio web que te interese (revisa robots.txt)
   ‚îî‚îÄ> Construye tu propio scraper desde cero
   ‚îî‚îÄ> Aplica TDD: escribe tests primero
```

**Tiempo total estimado:** 12-18 horas (1-1.5 semanas a ritmo c√≥modo)

---

## üì¶ Instalaci√≥n y Configuraci√≥n

### 1. Instalar Dependencias

```bash
# Navegar al directorio del proyecto pr√°ctico
cd modulo-04-apis-scraping/tema-2-web-scraping/04-proyecto-practico/

# Instalar dependencias
pip install -r requirements.txt
```

### 2. Verificar Instalaci√≥n de ChromeDriver (Selenium)

```python
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

# Esto descargar√° ChromeDriver autom√°ticamente si no existe
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)
driver.get("https://example.com")
print(driver.title)
driver.quit()
```

### 3. Ejecutar Tests del Proyecto

```bash
# En el directorio del proyecto pr√°ctico
pytest -v --cov=src --cov-report=term-missing

# Salida esperada:
# 71 tests passed, 90% coverage ‚úÖ
```

---

## ‚úÖ Criterios de √âxito

Habr√°s completado exitosamente este tema cuando puedas:

### Nivel B√°sico ‚úÖ
- [ ] Parsear HTML con BeautifulSoup (`find()`, `find_all()`, `select()`)
- [ ] Extraer textos, enlaces y atributos de elementos HTML
- [ ] Validar si un sitio permite scraping consultando `robots.txt`
- [ ] Implementar rate limiting b√°sico con `time.sleep()`

### Nivel Intermedio ‚úÖ
- [ ] Extraer tablas HTML y convertirlas a DataFrames de Pandas
- [ ] Navegar m√∫ltiples p√°ginas y consolidar datos
- [ ] Usar Selenium para scrapear contenido cargado con JavaScript
- [ ] Almacenar datos scrapeados en SQLite con validaci√≥n

### Nivel Avanzado ‚úÖ
- [ ] Construir scrapers completos con logging y manejo de errores
- [ ] Implementar scrapers que respetan `robots.txt` autom√°ticamente
- [ ] Decidir cu√°ndo usar scraping vs APIs seg√∫n el caso de uso
- [ ] Integrar scraping en pipelines ETL (Extracci√≥n ‚Üí Transformaci√≥n ‚Üí Carga)

### Proyecto Pr√°ctico ‚úÖ
- [ ] Comprender arquitectura de 5 m√≥dulos del proyecto
- [ ] Ejecutar 71 tests con 90% cobertura exitosamente
- [ ] Modificar funciones existentes para nuevos casos de uso
- [ ] Extender proyecto con nuevas funcionalidades (ej: scraping de im√°genes)

---

## üêõ Troubleshooting Com√∫n

### Problema 1: "ModuleNotFoundError: No module named 'bs4'"

**Soluci√≥n:**
```bash
pip install beautifulsoup4
```

### Problema 2: "selenium.common.exceptions.SessionNotCreatedException"

**Causa:** ChromeDriver desactualizado o no instalado.

**Soluci√≥n:**
```bash
pip install --upgrade selenium webdriver-manager
```

Luego usa `webdriver-manager` en tu c√≥digo:
```python
from webdriver_manager.chrome import ChromeDriverManager
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)
```

### Problema 3: "ValueError: No se encontr√≥ la tabla en el HTML"

**Causa:** El HTML no contiene `<table>` o los selectores son incorrectos.

**Soluci√≥n:**
- Usa DevTools (F12) para inspeccionar el HTML real
- Verifica que `robots.txt` permite scraping
- Aseg√∫rate de que no es contenido din√°mico (usa Selenium si aplica)

### Problema 4: "Request bloqueado por el sitio (403 Forbidden)"

**Causa:** El sitio detecta que eres un bot (falta User-Agent).

**Soluci√≥n:**
```python
import requests

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}
response = requests.get(url, headers=headers)
```

### Problema 5: "Tests de Selenium fallan: TimeoutException"

**Causa:** Elementos no cargan a tiempo o selectores incorrectos.

**Soluci√≥n:**
```python
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

# Esperar hasta que el elemento est√© presente
wait = WebDriverWait(driver, 10)
element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table")))
```

---

## üìä M√©tricas del Tema

| M√©trica                     | Valor                               |
| --------------------------- | ----------------------------------- |
| **Palabras en teor√≠a**      | ~5,200                              |
| **Tiempo de lectura**       | 40-50 min                           |
| **Ejemplos trabajados**     | 5 (BeautifulSoup + Selenium)        |
| **Ejercicios totales**      | 15 (B√°sico ‚Üí Intermedio ‚Üí Avanzado) |
| **Funciones en proyecto**   | 19 funciones (5 m√≥dulos)            |
| **Tests en proyecto**       | 71 tests                            |
| **Cobertura de c√≥digo**     | 90%                                 |
| **Calificaci√≥n pedag√≥gica** | 9.3/10 ‚≠ê                            |

---

## üéì Competencias Desarrolladas

Despu√©s de completar este tema, habr√°s desarrollado:

‚úÖ **Extracci√≥n de datos web** con BeautifulSoup y Selenium
‚úÖ **Scraping √©tico** respetando robots.txt y rate limits
‚úÖ **Navegaci√≥n program√°tica** de sitios web complejos
‚úÖ **Almacenamiento estructurado** en SQLite
‚úÖ **Manejo de contenido din√°mico** con JavaScript
‚úÖ **Validaci√≥n de datos** scrapeados
‚úÖ **TDD aplicado** a proyectos de scraping
‚úÖ **Toma de decisiones t√©cnicas** (API vs Scraping)

---

## üîó Relaci√≥n con Otros Temas

### Prerequisitos
- **Tema 1 (APIs REST):** Comprender HTTP, status codes, headers
- **M√≥dulo 1 (Python B√°sico):** Funciones, diccionarios, listas, CSV
- **M√≥dulo 3 (ETL):** Conceptos de extracci√≥n y transformaci√≥n

### Siguiente Tema
- **Tema 3 (Rate Limiting y Caching):** Optimizar scrapers para escala

### Aplicaciones Futuras
- **M√≥dulo 6 (Apache Airflow):** Programar scrapers peri√≥dicos
- **M√≥dulo 8 (Data Warehousing):** Cargar datos scrapeados a DWH
- **Proyectos Finales:** Pipelines completos de datos web

---

## üìö Recursos Adicionales

### Documentaci√≥n Oficial
- [BeautifulSoup Docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Selenium Python Docs](https://selenium-python.readthedocs.io/)
- [Robots.txt Specification](https://developers.google.com/search/docs/crawling-indexing/robots/intro)

### Sitios para Practicar Scraping (Legalmente)
- [https://quotes.toscrape.com/](https://quotes.toscrape.com/) - Sitio educativo para scraping
- [https://books.toscrape.com/](https://books.toscrape.com/) - E-commerce ficticio
- [https://scrapethissite.com/](https://scrapethissite.com/) - Ejercicios de scraping

### Herramientas √ötiles
- **SelectorGadget:** Extensi√≥n Chrome para generar CSS Selectors
- **XPath Helper:** Extensi√≥n Chrome para probar XPath
- **DevTools (F12):** Inspector de HTML nativo del navegador

### Lecturas Recomendadas
- [Web Scraping with Python (2nd Edition)](https://www.oreilly.com/library/view/web-scraping-with/9781491985564/) - Ryan Mitchell
- [Legal aspects of web scraping](https://www.eff.org/issues/coders/web-scraping) - Electronic Frontier Foundation

---

## ‚öñÔ∏è Consideraciones Legales y √âticas

### ‚úÖ Buenas Pr√°cticas

- Respetar `robots.txt` **SIEMPRE**
- Implementar rate limiting (m√≠nimo 1 segundo entre requests)
- Identificarte con User-Agent descriptivo
- No sobrecargar servidores (m√°x 1-2 requests/seg)
- Almacenar solo datos p√∫blicamente accesibles
- Respetar t√©rminos de servicio del sitio

### ‚ùå Malas Pr√°cticas (NO HACER)

- Ignorar `robots.txt`
- Scraping masivo sin rate limiting (DDoS)
- Falsificar User-Agent para evadir bloqueos
- Scraping de contenido protegido por login (sin permiso)
- Reventa de datos scrapeados
- Scraping de datos personales sin consentimiento (GDPR)

### üìú Marco Legal

- **GDPR (Europa):** No scrapear datos personales sin base legal
- **CFAA (EE.UU.):** No "acceder sin autorizaci√≥n" (t√©rminos difusos)
- **Casos relevantes:** hiQ Labs vs LinkedIn (scraping de datos p√∫blicos es legal)

**Regla de oro:** Si no lo puedes ver p√∫blicamente en un navegador, no lo scrapees.

---

## üéâ Proyecto Final Sugerido

Una vez domines este tema, intenta:

### "Scraper de Inmobiliaria Ficticia"

**Objetivo:** Extraer 100 propiedades de un sitio inmobiliario de ejemplo.

**Requisitos:**
1. Validar `robots.txt` antes de empezar
2. Extraer: direcci√≥n, precio, habitaciones, m¬≤, descripci√≥n
3. Implementar rate limiting (2 seg entre requests)
4. Almacenar en SQLite con validaci√≥n (precio > 0, habitaciones > 0)
5. Logging completo en archivo `.log`
6. TDD: escribir 20+ tests primero
7. README con instrucciones de uso

**Entregables:**
- C√≥digo Python modular (3-5 m√≥dulos)
- Base de datos SQLite con 100 propiedades
- Tests con >80% cobertura
- README con an√°lisis de datos (precio promedio, etc.)

---

## üìû Soporte

Si tienes dudas o problemas:

1. **Revisa:** Secci√≥n de Troubleshooting arriba
2. **Consulta:** REVISION_PEDAGOGICA.md para entender objetivos
3. **Experimenta:** Modifica ejemplos para aprender haciendo
4. **Practica:** Los 15 ejercicios cubren casos comunes

---

## üèÜ Siguiente Paso

Una vez completes este tema:

‚û°Ô∏è **Tema 3: Rate Limiting y Caching** - Optimiza tus scrapers para escala

Aprender√°s:
- Estrategias avanzadas de rate limiting (token bucket, sliding window)
- Caching en memoria y disco (Redis, shelve)
- Async requests con `aiohttp` (10x m√°s r√°pido)
- Monitoreo de throughput y m√©tricas

---

**¬°Feliz Scraping √âtico! üï∑Ô∏è‚ú®**

*√öltima actualizaci√≥n: 2025-10-24*
*Calificaci√≥n pedag√≥gica: 9.3/10 ‚≠ê*
*Duraci√≥n estimada: 12-18 horas*
