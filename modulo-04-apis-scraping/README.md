# 🌐 Módulo 4: APIs y Web Scraping

**Master en Ingeniería de Datos**
**Nivel:** Intermedio
**Duración estimada:** 3-4 semanas
**Estado:** ✅ **COMPLETADO (100%)**

---

## 🎯 Objetivos Generales del Módulo

Al completar este módulo, serás capaz de:

1. ✅ **Consumir APIs REST** profesionalmente (GET, POST, PUT, DELETE, autenticación)
2. ✅ **Extraer datos de sitios web** con web scraping ético (BeautifulSoup, Selenium)
3. ✅ **Optimizar pipelines de extracción** con rate limiting, caching y async programming
4. ✅ **Manejar errores y reintentos** de forma robusta (exponential backoff, timeouts)
5. ✅ **Implementar paginación** automática (Offset/Limit, Cursor-based)
6. ✅ **Respetar ética y legalidad** (robots.txt, rate limits, términos de servicio)
7. ✅ **Medir y monitorear performance** (throughput, latencia, cache hit rate, ROI)
8. ✅ **Aplicar TDD** en extracción de datos (210 tests totales)

---

## 📊 Progreso del Módulo

```
Módulo 4: APIs y Web Scraping
├── ✅ Tema 1: APIs REST (100%)
│   ├── ✅ Teoría: ~4,500 palabras
│   ├── ✅ Ejemplos: 5 ejemplos trabajados
│   ├── ✅ Ejercicios: 15 con soluciones
│   ├── ✅ Proyecto práctico: 98 tests, 100% cobertura
│   └── ✅ Calificación pedagógica: 9.2/10 ⭐⭐⭐⭐⭐
│
├── ✅ Tema 2: Web Scraping (100%)
│   ├── ✅ Teoría: ~4,200 palabras
│   ├── ✅ Ejemplos: 5 ejemplos trabajados
│   ├── ✅ Ejercicios: 15 con soluciones
│   ├── ✅ Proyecto práctico: 71 tests, 90% cobertura
│   └── ✅ Calificación pedagógica: 9.3/10 ⭐⭐⭐⭐⭐
│
└── ✅ Tema 3: Rate Limiting y Caching (100%)
    ├── ✅ Teoría: ~3,500 palabras
    ├── ✅ Ejemplos: 4 ejemplos trabajados
    ├── ✅ Ejercicios: 12 con soluciones
    ├── ✅ Proyecto práctico: 41 tests, 88% cobertura
    └── ✅ Calificación pedagógica: 9.4/10 ⭐⭐⭐⭐⭐
```

**Progreso total:** 3/3 temas completados ✅

---

## 📚 Estructura de los 3 Temas

### Tema 1: APIs REST

**Objetivo:** Dominar el consumo de APIs REST de forma profesional.

**Contenido:**
- 📖 **Teoría:** HTTP, status codes, autenticación, rate limiting, paginación
- 💻 **Ejemplos:** GET, POST, API Key, paginación automática, reintentos
- ✍️ **Ejercicios:** 15 ejercicios progresivos (básico → avanzado)
- 🏗️ **Proyecto:** Cliente HTTP robusto con 5 módulos y 98 tests

**Tecnologías:** `requests`, `dotenv`, type hints, TDD

**Duración:** 1-1.5 semanas

[📂 Ver Tema 1](tema-1-apis-rest/)

---

### Tema 2: Web Scraping

**Objetivo:** Extraer datos de sitios web de forma ética y eficiente.

**Contenido:**
- 📖 **Teoría:** HTML/DOM, BeautifulSoup, Selenium, robots.txt, XPath
- 💻 **Ejemplos:** Scraping estático, tablas, navegación, scraping dinámico
- ✍️ **Ejercicios:** 15 ejercicios con casos reales (noticias, e-commerce)
- 🏗️ **Proyecto:** Scraper completo con 5 módulos y 71 tests

**Tecnologías:** `beautifulsoup4`, `selenium`, `sqlite3`, `pandas`

**Duración:** 1-1.5 semanas

[📂 Ver Tema 2](tema-2-web-scraping/)

---

### Tema 3: Rate Limiting y Caching

**Objetivo:** Optimizar scrapers para ser 10x-20x más rápidos.

**Contenido:**
- 📖 **Teoría:** Rate limiting algorithms, caching strategies, async programming
- 💻 **Ejemplos:** Token Bucket, cache con TTL, async requests, métricas
- ✍️ **Ejercicios:** 12 ejercicios de optimización con ROI
- 🏗️ **Proyecto:** Scraper optimizado con 4 módulos y 55 tests

**Tecnologías:** `aiohttp`, `asyncio`, `shelve`, métricas

**Duración:** 1-1.5 semanas

[📂 Ver Tema 3](tema-3-rate-limiting-caching/)

---

## 🚀 Cómo Estudiar Este Módulo

### Orden Recomendado

#### Semana 1: Tema 1 - APIs REST

1. **Día 1-2:** Teoría + Ejemplos (APIs REST, autenticación)
2. **Día 3-4:** Ejercicios básicos e intermedios
3. **Día 5-7:** Proyecto práctico (TDD: cliente HTTP robusto)

**Hito:** Consumir APIs REST con autenticación y manejo de errores ✅

---

#### Semana 2: Tema 2 - Web Scraping

1. **Día 1-2:** Teoría + Ejemplos (BeautifulSoup, Selenium)
2. **Día 3-4:** Ejercicios básicos e intermedios
3. **Día 5-7:** Proyecto práctico (TDD: scraper completo)

**Hito:** Extraer datos de sitios web estáticos y dinámicos ✅

---

#### Semana 3: Tema 3 - Optimización

1. **Día 1-2:** Teoría + Ejemplos (rate limiting, caching, async)
2. **Día 3-4:** Ejercicios de optimización
3. **Día 5-7:** Proyecto práctico (TDD: scraper optimizado)

**Hito:** Optimizar scrapers 10x-20x más rápido ✅

---

#### Semana 4 (Opcional): Integración y Práctica

1. **Día 1-3:** Proyecto integrador (combinar los 3 temas)
2. **Día 4-5:** Optimización y medición de ROI
3. **Día 6-7:** Documentación y presentación

**Hito:** Pipeline de extracción completo y optimizado ✅

---

## 🎓 Competencias Desarrolladas

Al completar este módulo, habrás dominado:

### Competencias Técnicas

1. ✅ **REST APIs:** GET, POST, PUT, DELETE con `requests`
2. ✅ **Autenticación:** API Key, Bearer Token, Basic Auth, OAuth
3. ✅ **Web Scraping:** BeautifulSoup (estático) y Selenium (dinámico)
4. ✅ **Parsing:** HTML, CSS Selectors, XPath, JSON
5. ✅ **Rate Limiting:** Fixed Window, Token Bucket algorithms
6. ✅ **Caching:** Memoria, disco (`shelve`), TTL strategies
7. ✅ **Async Programming:** `aiohttp`, `asyncio`, concurrency control
8. ✅ **Manejo de Errores:** Exponential backoff, reintentos, timeouts
9. ✅ **Paginación:** Offset/Limit, Cursor-based
10. ✅ **TDD:** Test-Driven Development aplicado a extracción de datos

### Competencias Profesionales

11. ✅ **Ética del Scraping:** Robots.txt, rate limits, términos de servicio
12. ✅ **Performance Tuning:** Medir throughput, latencia, optimizar 10x-20x
13. ✅ **Cálculo de ROI:** Justificar optimizaciones con datos económicos
14. ✅ **Documentación:** README, docstrings, ejemplos ejecutables
15. ✅ **Debugging:** Logging, manejo de errores, troubleshooting

---

## 📊 Métricas del Módulo

| Métrica                     | Valor         | Detalles                                            |
| --------------------------- | ------------- | --------------------------------------------------- |
| **Temas completados**       | 3/3           | 100% ✅                                              |
| **Teoría (palabras)**       | ~12,200       | APIs: 4,500 / Scraping: 4,200 / Optimización: 3,500 |
| **Ejemplos**                | 14 ejemplos   | Tema 1: 5 / Tema 2: 5 / Tema 3: 4                   |
| **Ejercicios**              | 42 ejercicios | Tema 1: 15 / Tema 2: 15 / Tema 3: 12                |
| **Proyectos prácticos**     | 3 proyectos   | TDD estricto en todos                               |
| **Tests totales**           | 210 tests     | Tema 1: 98 / Tema 2: 71 / Tema 3: 41 (ejecutables)  |
| **Cobertura promedio**      | 93%           | Tema 1: 100% / Tema 2: 90% / Tema 3: 88%            |
| **Funciones implementadas** | 55 funciones  | Tema 1: 19 / Tema 2: 19 / Tema 3: 17                |
| **Calificación pedagógica** | 9.3/10 ⭐⭐⭐⭐⭐  | Tema 1: 9.2 / Tema 2: 9.3 / Tema 3: 9.4             |

---

## ✅ Criterios de Éxito del Módulo

### Nivel Básico (Mínimo)

- [ ] Consumo APIs REST con `requests` (GET, POST)
- [ ] Manejo básico de status codes (200, 404, 500)
- [ ] Autenticación con API Key
- [ ] Scraping estático con BeautifulSoup
- [ ] Extracción de texto, enlaces, tablas HTML
- [ ] Rate limiting manual con `time.sleep()`
- [ ] Cache en memoria con diccionarios

### Nivel Intermedio (Recomendado)

- [ ] Autenticación múltiple (API Key, Bearer, Basic Auth)
- [ ] Manejo robusto de errores (try/except, reintentos)
- [ ] Paginación automática (Offset/Limit)
- [ ] Scraping dinámico con Selenium
- [ ] Validación de robots.txt
- [ ] Almacenamiento en SQLite
- [ ] Token Bucket rate limiting
- [ ] Cache persistente con `shelve` y TTL
- [ ] Async requests con `aiohttp`

### Nivel Avanzado (Opcional)

- [ ] Pipeline ETL completo: API/Scraping → Transform → Storage
- [ ] Exponential backoff para reintentos
- [ ] Cursor-based pagination
- [ ] Scraping masivo (100+ URLs) optimizado
- [ ] Integración async + cache + rate limiting + métricas
- [ ] Monitoreo de throughput, latencia, cache hit rate
- [ ] Cálculo de ROI (tiempo, costo, mejora de velocidad)
- [ ] Dashboard de métricas en ASCII art
- [ ] Optimización 10x-20x en velocidad

---

## 🔗 Integración con Otros Módulos

### Pre-requisitos

- **Módulo 1:** Python fundamentals, funciones, tipos de datos
- **Módulo 2:** SQL básico para almacenar datos extraídos
- **Módulo 3:** Conceptos de ETL/ELT

### Complementa con

- **Módulo 5:** Bases de Datos Avanzadas (PostgreSQL, MongoDB) para almacenar datos scrapeados
- **Módulo 6:** Apache Airflow para orquestar scrapers periódicos
- **Módulo 7:** Cloud (AWS/GCP) para desplegar scrapers en producción

---

## 📁 Estructura de Archivos

```
modulo-04-apis-scraping/
├── tema-1-apis-rest/
│   ├── 01-TEORIA.md
│   ├── 02-EJEMPLOS.md
│   ├── 03-EJERCICIOS.md
│   ├── 04-proyecto-practico/
│   │   ├── src/                 # 5 módulos
│   │   ├── tests/               # 98 tests
│   │   ├── ejemplos/            # Ejemplos ejecutables
│   │   ├── README.md
│   │   └── requirements.txt
│   ├── REVISION_PEDAGOGICA.md   # 9.2/10 ⭐⭐⭐⭐⭐
│   └── README.md
│
├── tema-2-web-scraping/
│   ├── 01-TEORIA.md
│   ├── 02-EJEMPLOS.md
│   ├── 03-EJERCICIOS.md
│   ├── 04-proyecto-practico/
│   │   ├── src/                 # 5 módulos
│   │   ├── tests/               # 71 tests
│   │   ├── ejemplos/
│   │   ├── README.md
│   │   └── requirements.txt
│   ├── REVISION_PEDAGOGICA.md   # 9.3/10 ⭐⭐⭐⭐⭐
│   └── README.md
│
├── tema-3-rate-limiting-caching/
│   ├── 01-TEORIA.md
│   ├── 02-EJEMPLOS.md
│   ├── 03-EJERCICIOS.md
│   ├── 04-proyecto-practico/
│   │   ├── src/                 # 4 módulos
│   │   ├── tests/               # 55 tests (41 ejecutables)
│   │   ├── ejemplos/
│   │   ├── README.md
│   │   └── requirements.txt
│   ├── REVISION_PEDAGOGICA.md   # 9.4/10 ⭐⭐⭐⭐⭐
│   └── README.md
│
└── README.md                    # Este archivo
```

---

## 🛠️ Tecnologías Utilizadas

### Extracción de Datos

- `requests` - Cliente HTTP para APIs REST
- `beautifulsoup4` - Parsing HTML (scraping estático)
- `selenium` - Automatización de navegador (scraping dinámico)
- `aiohttp` - Cliente HTTP asíncrono
- `urllib.robotparser` - Validación de robots.txt

### Almacenamiento

- `sqlite3` - Base de datos local
- `pandas` - Manipulación de datos
- `shelve` - Cache persistente

### Testing y Calidad

- `pytest` - Framework de testing
- `pytest-cov` - Cobertura de código
- `pytest-asyncio` - Tests asíncronos
- `black` - Formateo de código
- `flake8` - Linting

### Utilidades

- `python-dotenv` - Variables de entorno
- `typing` - Type hints

---

## 💡 Buenas Prácticas Aplicadas

### Seguridad

- ✅ **HTTPS obligatorio** - Rechazar URLs HTTP
- ✅ **Variables de entorno** - Nunca hardcodear API keys
- ✅ **Validaciones** - Inputs, URLs, status codes
- ✅ **Timeouts** - Evitar requests colgados

### Ética

- ✅ **Respetar robots.txt** - Siempre verificar permisos
- ✅ **Rate limiting** - No sobrecargar servidores
- ✅ **User-Agent** - Identificarse correctamente
- ✅ **Términos de servicio** - Cumplir legalmente

### Código

- ✅ **TDD estricto** - Tests escritos primero
- ✅ **Type hints** - 100% de funciones tipadas
- ✅ **Docstrings** - Documentación completa
- ✅ **Funcional** - Sin efectos secundarios
- ✅ **DRY** - No repetir código
- ✅ **KISS** - Soluciones simples

### Performance

- ✅ **Caching** - Evitar requests redundantes
- ✅ **Async** - Concurrencia para velocidad
- ✅ **Métricas** - Medir throughput y latencia
- ✅ **Optimización basada en datos** - ROI calculado

---

## 🐛 Troubleshooting Común

### Problema: `ConnectionError` al consumir API

**Causa:** Timeout, URL incorrecta, servidor caído

**Solución:**
```python
try:
    response = requests.get(url, timeout=10)
except requests.exceptions.ConnectionError:
    print("Error de conexión. Verificar URL y conectividad.")
```

---

### Problema: `aiohttp` no se instala en Windows

**Causa:** Requiere compilador C

**Solución:**
1. Usar Docker (recomendado)
2. Instalar Microsoft Visual C++ Build Tools
3. Usar WSL2 o Linux/Mac

---

### Problema: Scraping bloqueado (403 Forbidden)

**Causa:** User-Agent no configurado, rate limit excedido

**Solución:**
```python
headers = {"User-Agent": "MiScraper/1.0 (contacto@email.com)"}
time.sleep(2)  # Rate limiting
```

---

### Problema: Selenium no encuentra ChromeDriver

**Causa:** ChromeDriver no instalado o no en PATH

**Solución:**
```bash
pip install selenium webdriver-manager
```

```python
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)
```

---

## 📚 Recursos Adicionales

### Documentación Oficial

- [Requests Documentation](https://docs.python-requests.org/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Selenium Documentation](https://selenium-python.readthedocs.io/)
- [aiohttp Documentation](https://docs.aiohttp.org/)

### APIs Públicas para Practicar

- [JSONPlaceholder](https://jsonplaceholder.typicode.com/) - API de prueba gratuita
- [OpenWeather API](https://openweathermap.org/api) - Datos meteorológicos
- [GitHub API](https://docs.github.com/en/rest) - Repos, issues, users
- [CoinGecko API](https://www.coingecko.com/en/api) - Criptomonedas

### Herramientas Útiles

- [Postman](https://www.postman.com/) - Probar APIs manualmente
- [httpbin.org](https://httpbin.org/) - API de testing HTTP
- [Scrapy](https://scrapy.org/) - Framework avanzado de scraping (opcional)
- [Redis](https://redis.io/) - Cache distribuido (avanzado)

---

## 🎯 Proyecto Final Integrador (Opcional)

**Desafío:** Pipeline completo de extracción de datos optimizado.

### Requisitos

1. **Fuentes de datos:**
   - Consumir 1 API REST (con autenticación)
   - Scrapear 1 sitio web (con Selenium si es dinámico)

2. **Procesamiento:**
   - Validar datos extraídos
   - Transformar a formato estructurado
   - Almacenar en SQLite

3. **Optimización:**
   - Implementar cache con TTL
   - Rate limiting configurable
   - Async para requests paralelos
   - Monitoreo de métricas

4. **Calidad:**
   - TDD con >80% cobertura
   - Type hints y docstrings completos
   - Logging estructurado
   - Manejo robusto de errores

5. **Documentación:**
   - README con instrucciones claras
   - Dashboard de métricas (antes/después)
   - Análisis de ROI

### Entrega Esperada

- Código fuente con tests
- README con métricas comparativas
- CHANGELOG con decisiones técnicas
- Demo ejecutable (video/screenshots)

---

## 🏆 Logros al Completar el Módulo

Al finalizar este módulo, habrás:

✅ **Extraído datos de 20+ APIs y sitios web** diferentes
✅ **Implementado 3 proyectos prácticos** con TDD (210 tests)
✅ **Optimizado scrapers** 10x-20x más rápidos
✅ **Dominado 10+ tecnologías** profesionales
✅ **Escrito 12,000+ palabras** de documentación técnica
✅ **Desarrollado 55+ funciones** robustas
✅ **Aplicado ética y legalidad** en extracción de datos
✅ **Calculado ROI** de optimizaciones con datos reales

**¡Felicitaciones!** 🎉 Estás preparado para roles de:
- Data Engineer (extracción de datos)
- Backend Developer (integración de APIs)
- Web Scraping Specialist
- ETL Developer

---

## 🔜 Próximos Pasos

Después de completar este módulo, continúa con:

1. **Módulo 5:** Bases de Datos Avanzadas (PostgreSQL, MongoDB)
2. **Módulo 6:** Apache Airflow y Orquestación
3. **Módulo 7:** Cloud Computing (AWS/GCP)

---

**Última actualización:** 2025-10-25
**Autor:** DataHub Inc. - Equipo de Data Engineering
**Calificación pedagógica promedio:** 9.3/10 ⭐⭐⭐⭐⭐
**Estado:** ✅ **COMPLETADO (100%)**
**Duración real:** 3-4 semanas
**Nivel:** Intermedio
