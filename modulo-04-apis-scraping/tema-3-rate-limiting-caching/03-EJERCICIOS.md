# ğŸ‹ï¸ Ejercicios PrÃ¡cticos - Tema 3: Rate Limiting y Caching

**MÃ³dulo 4:** APIs y Web Scraping
**DuraciÃ³n estimada:** 6-10 horas de prÃ¡ctica
**Prerequisitos:** Haber completado 01-TEORIA.md y 02-EJEMPLOS.md del Tema 3

---

## ğŸ¯ IntroducciÃ³n

Este archivo contiene **12 ejercicios progresivos** diseÃ±ados para que practiques y domines las tÃ©cnicas de optimizaciÃ³n de extracciÃ³n de datos:

- **BÃ¡sicos (1-4):** Rate limiting, mediciÃ³n de tiempos, cache bÃ¡sico
- **Intermedios (5-8):** Cache con TTL, token bucket, async requests
- **Avanzados (9-12):** Scrapers optimizados completos con mÃ©tricas

**MetodologÃ­a recomendada:**
1. Lee el enunciado completo
2. Intenta resolver sin mirar las pistas
3. Si te atascas, lee las pistas progresivas
4. Compara tu soluciÃ³n con la proporcionada
5. Ejecuta el cÃ³digo y verifica el output

---

## ğŸ“Š Tabla de Contenidos

| #   | Ejercicio                       | Nivel      | DuraciÃ³n | Conceptos                     |
| --- | ------------------------------- | ---------- | -------- | ----------------------------- |
| 1   | Rate Limiting BÃ¡sico            | BÃ¡sico     | 20 min   | `time.sleep()`, mediciÃ³n      |
| 2   | Medir Throughput                | BÃ¡sico     | 15 min   | MÃ©tricas, velocidad           |
| 3   | Cache en Memoria                | BÃ¡sico     | 25 min   | `dict`, cache hits            |
| 4   | Calcular Cache Hit Rate         | BÃ¡sico     | 20 min   | MÃ©tricas de cache             |
| 5   | Cache con TTL                   | Intermedio | 30 min   | ExpiraciÃ³n, timestamps        |
| 6   | Token Bucket                    | Intermedio | 45 min   | Rate limiting avanzado        |
| 7   | Async Requests BÃ¡sico           | Intermedio | 40 min   | `aiohttp`, `asyncio`          |
| 8   | Comparar SÃ­ncrono vs Async      | Intermedio | 35 min   | Benchmarking                  |
| 9   | Cache Persistente con Shelve    | Avanzado   | 50 min   | Persistencia, `shelve`        |
| 10  | Scraper Optimizado con MÃ©tricas | Avanzado   | 60 min   | IntegraciÃ³n completa          |
| 11  | Optimizar Pipeline ETL          | Avanzado   | 70 min   | Async + Cache + Rate Limiting |
| 12  | Dashboard de MÃ©tricas           | Avanzado   | 60 min   | Monitoreo, ROI                |

---

## ğŸ“˜ EJERCICIOS BÃSICOS

### Ejercicio 1: Rate Limiting BÃ¡sico

**DuraciÃ³n:** 20 minutos
**Nivel:** BÃ¡sico
**Conceptos:** `time.sleep()`, rate limiting, mediciÃ³n de tiempo

#### Contexto Empresarial
Trabajas en **DataHub Inc.** y necesitas scrapear 20 productos de un sitio web. Para ser respetuoso con el servidor, debes implementar un delay de **1.5 segundos** entre cada request.

#### Objetivo
Implementar una funciÃ³n que haga requests HTTP con rate limiting y mida el tiempo total de ejecuciÃ³n.

#### URLs de Ejemplo
```python
urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 21)]
```

#### Requisitos
1. FunciÃ³n que reciba lista de URLs y delay en segundos
2. Hacer request a cada URL con `requests.get()`
3. Esperar el delay especificado entre requests (excepto despuÃ©s del Ãºltimo)
4. Medir y mostrar tiempo total de ejecuciÃ³n
5. Calcular y mostrar velocidad (requests/segundo)

#### Output Esperado
```
ğŸš€ Scraping 20 URLs con delay de 1.5s entre cada una
â±ï¸  Tiempo estimado: 28.5s

[1/20] âœ… https://jsonplaceholder.typicode.com/posts/1
[2/20] âœ… https://jsonplaceholder.typicode.com/posts/2
...
[20/20] âœ… https://jsonplaceholder.typicode.com/posts/20

ğŸ“Š Resultados:
â±ï¸  Tiempo total: 30.2s
âš¡ Velocidad: 0.66 req/seg
âœ… Rate limiting respetado: 1.5s entre requests
```

<details>
<summary>ğŸ’¡ Pista 1</summary>

Usa `time.time()` para medir el tiempo:
```python
import time

inicio = time.time()
# ... hacer algo ...
fin = time.time()
tiempo_total = fin - inicio
```
</details>

<details>
<summary>ğŸ’¡ Pista 2</summary>

Usa `time.sleep(delay)` DESPUÃ‰S de cada request, excepto el Ãºltimo:
```python
for i, url in enumerate(urls, 1):
    hacer_request(url)
    if i < len(urls):  # No esperar despuÃ©s del Ãºltimo
        time.sleep(delay)
```
</details>

<details>
<summary>ğŸ’¡ Pista 3</summary>

El tiempo total debe ser aproximadamente: `(num_urls - 1) * delay + tiempo_requests`
</details>

#### SoluciÃ³n

```python
import requests
import time
from typing import List

def scrapear_con_rate_limiting(urls: List[str], delay: float = 1.5) -> List[dict]:
    """
    Scrapea URLs con rate limiting.

    Args:
        urls: Lista de URLs a scrapear
        delay: Segundos de espera entre requests

    Returns:
        Lista de responses
    """
    print(f"ğŸš€ Scraping {len(urls)} URLs con delay de {delay}s entre cada una")
    print(f"â±ï¸  Tiempo estimado: {(len(urls) - 1) * delay}s\n")

    inicio = time.time()
    resultados = []

    for i, url in enumerate(urls, 1):
        try:
            response = requests.get(url, timeout=10)
            resultados.append(response.json())
            print(f"[{i}/{len(urls)}] âœ… {url}")
        except Exception as e:
            print(f"[{i}/{len(urls)}] âŒ {url} - Error: {e}")

        # Rate limiting: esperar entre requests (excepto el Ãºltimo)
        if i < len(urls):
            time.sleep(delay)

    fin = time.time()
    tiempo_total = fin - inicio
    velocidad = len(urls) / tiempo_total

    print(f"\nğŸ“Š Resultados:")
    print(f"â±ï¸  Tiempo total: {tiempo_total:.1f}s")
    print(f"âš¡ Velocidad: {velocidad:.2f} req/seg")
    print(f"âœ… Rate limiting respetado: {delay}s entre requests")

    return resultados


# ===============================
# EJECUCIÃ“N
# ===============================

if __name__ == "__main__":
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 21)]
    resultados = scrapear_con_rate_limiting(urls, delay=1.5)
    print(f"\nğŸ“¦ Total de datos obtenidos: {len(resultados)}")
```

---

### Ejercicio 2: Medir Throughput

**DuraciÃ³n:** 15 minutos
**Nivel:** BÃ¡sico
**Conceptos:** MÃ©tricas, throughput, latencia

#### Contexto Empresarial
Tu jefe te pide comparar dos estrategias de scraping: una con delay de 0.5s y otra con 1.5s. Necesitas medir el **throughput** (requests/segundo) de cada una.

#### Objetivo
Crear una funciÃ³n que mida throughput y latencia promedio de un scraper.

#### Requisitos
1. FunciÃ³n que reciba URLs y delay
2. Calcular throughput (total_requests / tiempo_total)
3. Calcular latencia promedio por request
4. Comparar dos estrategias diferentes

#### Output Esperado
```
Estrategia 1 (delay=0.5s):
âš¡ Throughput: 1.2 req/seg
â±ï¸  Latencia promedio: 833ms

Estrategia 2 (delay=1.5s):
âš¡ Throughput: 0.6 req/seg
â±ï¸  Latencia promedio: 1667ms

ğŸ“Š ConclusiÃ³n: Estrategia 1 es 2.0x mÃ¡s rÃ¡pida
```

<details>
<summary>ğŸ’¡ Pista 1</summary>

Throughput = total_requests / tiempo_total (en segundos)
</details>

<details>
<summary>ğŸ’¡ Pista 2</summary>

Latencia promedio = tiempo_total / total_requests * 1000 (en milisegundos)
</details>

#### SoluciÃ³n

```python
import requests
import time
from typing import List, Tuple

def medir_throughput(urls: List[str], delay: float) -> Tuple[float, float]:
    """
    Mide throughput y latencia de un scraper.

    Args:
        urls: Lista de URLs
        delay: Delay entre requests

    Returns:
        (throughput, latencia_promedio)
    """
    inicio = time.time()

    for i, url in enumerate(urls):
        requests.get(url)
        if i < len(urls) - 1:
            time.sleep(delay)

    fin = time.time()
    tiempo_total = fin - inicio

    throughput = len(urls) / tiempo_total
    latencia_promedio = (tiempo_total / len(urls)) * 1000  # ms

    return throughput, latencia_promedio


def comparar_estrategias(urls: List[str]):
    """Compara dos estrategias de scraping."""

    print("Estrategia 1 (delay=0.5s):")
    throughput1, latencia1 = medir_throughput(urls, delay=0.5)
    print(f"âš¡ Throughput: {throughput1:.1f} req/seg")
    print(f"â±ï¸  Latencia promedio: {latencia1:.0f}ms\n")

    print("Estrategia 2 (delay=1.5s):")
    throughput2, latencia2 = medir_throughput(urls, delay=1.5)
    print(f"âš¡ Throughput: {throughput2:.1f} req/seg")
    print(f"â±ï¸  Latencia promedio: {latencia2:.0f}ms\n")

    mejora = throughput1 / throughput2
    print(f"ğŸ“Š ConclusiÃ³n: Estrategia 1 es {mejora:.1f}x mÃ¡s rÃ¡pida")


# ===============================
# EJECUCIÃ“N
# ===============================

if __name__ == "__main__":
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 11)]
    comparar_estrategias(urls)
```

---

### Ejercicio 3: Cache en Memoria

**DuraciÃ³n:** 25 minutos
**Nivel:** BÃ¡sico
**Conceptos:** Cache, diccionario, cache hits/misses

#### Contexto Empresarial
Tu script hace mÃºltiples requests a las mismas URLs durante su ejecuciÃ³n. Implementa un cache en memoria para evitar requests redundantes.

#### Objetivo
Implementar una funciÃ³n que use un diccionario Python como cache para requests HTTP.

#### Requisitos
1. Usar un `dict` global como cache
2. Antes de cada request, verificar si la URL ya estÃ¡ en cache
3. Si estÃ¡ en cache (HIT), retornar datos sin hacer request
4. Si no estÃ¡ en cache (MISS), hacer request y guardar en cache
5. Contar y mostrar hits y misses

#### URLs de Ejemplo
```python
# Lista con URLs repetidas para demostrar cache
urls = [
    "https://jsonplaceholder.typicode.com/posts/1",
    "https://jsonplaceholder.typicode.com/posts/2",
    "https://jsonplaceholder.typicode.com/posts/1",  # Repetida
    "https://jsonplaceholder.typicode.com/posts/3",
    "https://jsonplaceholder.typicode.com/posts/2",  # Repetida
    "https://jsonplaceholder.typicode.com/posts/1",  # Repetida
]
```

#### Output Esperado
```
[1/6] ğŸŒ Cache MISS: /posts/1 - Haciendo request...
[2/6] ğŸŒ Cache MISS: /posts/2 - Haciendo request...
[3/6] âœ… Cache HIT: /posts/1
[4/6] ğŸŒ Cache MISS: /posts/3 - Haciendo request...
[5/6] âœ… Cache HIT: /posts/2
[6/6] âœ… Cache HIT: /posts/1

ğŸ“Š EstadÃ­sticas:
âœ… Cache Hits: 3 (50.0%)
ğŸŒ Cache Misses: 3 (50.0%)
ğŸ’° Requests ahorrados: 3 (50% menos trÃ¡fico)
```

<details>
<summary>ğŸ’¡ Pista 1</summary>

Declara el cache como diccionario global:
```python
cache = {}

def obtener_con_cache(url):
    if url in cache:
        # Cache HIT
        return cache[url]
    # Cache MISS
    data = requests.get(url).json()
    cache[url] = data
    return data
```
</details>

<details>
<summary>ğŸ’¡ Pista 2</summary>

Cache Hit Rate = (cache_hits / total_requests) * 100
</details>

#### SoluciÃ³n

```python
import requests
from typing import List, Dict

# Cache global
cache_memoria = {}
cache_hits = 0
cache_misses = 0


def obtener_con_cache(url: str) -> dict:
    """
    Obtiene datos con cache en memoria.

    Args:
        url: URL a consultar

    Returns:
        Datos del endpoint
    """
    global cache_hits, cache_misses

    if url in cache_memoria:
        cache_hits += 1
        print(f"âœ… Cache HIT: {url}")
        return cache_memoria[url]

    cache_misses += 1
    print(f"ğŸŒ Cache MISS: {url} - Haciendo request...")

    response = requests.get(url, timeout=10)
    data = response.json()

    cache_memoria[url] = data
    return data


def procesar_urls_con_cache(urls: List[str]):
    """Procesa URLs con cache y muestra estadÃ­sticas."""
    resultados = []

    for i, url in enumerate(urls, 1):
        print(f"[{i}/{len(urls)}] ", end="")
        data = obtener_con_cache(url)
        resultados.append(data)

    # EstadÃ­sticas
    total = cache_hits + cache_misses
    hit_rate = (cache_hits / total * 100) if total > 0 else 0

    print(f"\nğŸ“Š EstadÃ­sticas:")
    print(f"âœ… Cache Hits: {cache_hits} ({hit_rate:.1f}%)")
    print(f"ğŸŒ Cache Misses: {cache_misses} ({100-hit_rate:.1f}%)")
    print(f"ğŸ’° Requests ahorrados: {cache_hits} ({hit_rate:.0f}% menos trÃ¡fico)")

    return resultados


# ===============================
# EJECUCIÃ“N
# ===============================

if __name__ == "__main__":
    # URLs con repeticiones
    urls = [
        "https://jsonplaceholder.typicode.com/posts/1",
        "https://jsonplaceholder.typicode.com/posts/2",
        "https://jsonplaceholder.typicode.com/posts/1",  # Repetida
        "https://jsonplaceholder.typicode.com/posts/3",
        "https://jsonplaceholder.typicode.com/posts/2",  # Repetida
        "https://jsonplaceholder.typicode.com/posts/1",  # Repetida
    ]

    resultados = procesar_urls_con_cache(urls)
    print(f"\nğŸ“¦ Total de datos obtenidos: {len(resultados)}")
```

---

### Ejercicio 4: Calcular Cache Hit Rate

**DuraciÃ³n:** 20 minutos
**Nivel:** BÃ¡sico
**Conceptos:** MÃ©tricas de cache, anÃ¡lisis de rendimiento

#### Contexto Empresarial
Tu empresa quiere saber quÃ© tan efectivo es el cache. Necesitas calcular el **Cache Hit Rate** y determinar cuÃ¡nto dinero se ahorra si cada request cuesta $0.01.

#### Objetivo
Crear una funciÃ³n que calcule mÃ©tricas de cache y ROI (Return on Investment).

#### Requisitos
1. Simular 100 requests donde el 70% son repetidos (deberÃ­an venir de cache)
2. Calcular Cache Hit Rate
3. Calcular requests ahorrados
4. Calcular ahorro monetario (asumiendo $0.01 por request)

#### Output Esperado
```
ğŸ“Š SimulaciÃ³n de 100 Requests con Cache

Progreso: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100/100

ğŸ“ˆ MÃ©tricas de Cache:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Total Requests: 100
âœ… Cache Hits: 70 (70.0%)
ğŸŒ Cache Misses: 30 (30.0%)
ğŸ’° Requests reales: 30
ğŸ’° Requests ahorrados: 70

ğŸ’µ ROI Financiero:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¸ Costo sin cache: $1.00 (100 requests Ã— $0.01)
ğŸ’¸ Costo con cache: $0.30 (30 requests Ã— $0.01)
ğŸ’° Ahorro: $0.70 (70% de reducciÃ³n)
```

<details>
<summary>ğŸ’¡ Pista 1</summary>

Para simular 70% de repeticiones, genera 30 URLs Ãºnicas y repÃ­telas:
```python
import random

urls_unicas = [f"url_{i}" for i in range(30)]
urls_total = urls_unicas + random.choices(urls_unicas, k=70)
random.shuffle(urls_total)
```
</details>

<details>
<summary>ğŸ’¡ Pista 2</summary>

Cache Hit Rate = (hits / (hits + misses)) * 100
</details>

#### SoluciÃ³n

```python
import random
from typing import List

def simular_requests_con_cache(total_requests: int, porcentaje_hits: float = 0.7):
    """
    Simula requests con cache y calcula mÃ©tricas.

    Args:
        total_requests: NÃºmero total de requests a simular
        porcentaje_hits: Porcentaje esperado de cache hits
    """
    # Generar URLs (30% Ãºnicas, 70% repetidas)
    num_unicas = int(total_requests * (1 - porcentaje_hits))
    urls_unicas = [f"https://api.example.com/data/{i}" for i in range(num_unicas)]

    # Crear lista total: URLs Ãºnicas + repeticiones
    urls_totales = urls_unicas.copy()
    urls_repetidas = random.choices(urls_unicas, k=total_requests - num_unicas)
    urls_totales.extend(urls_repetidas)
    random.shuffle(urls_totales)

    # Simular cache
    cache = {}
    cache_hits = 0
    cache_misses = 0

    print(f"ğŸ“Š SimulaciÃ³n de {total_requests} Requests con Cache\n")

    for i, url in enumerate(urls_totales, 1):
        if url in cache:
            cache_hits += 1
        else:
            cache_misses += 1
            cache[url] = f"datos_{i}"

        # Barra de progreso
        if i % 5 == 0 or i == total_requests:
            progreso = i / total_requests
            barra = "â–ˆ" * int(progreso * 20)
            print(f"\rProgreso: {barra:<20} {i}/{total_requests}", end="")

    print("\n")

    # Calcular mÃ©tricas
    hit_rate = (cache_hits / total_requests) * 100
    miss_rate = (cache_misses / total_requests) * 100

    # ROI Financiero
    costo_por_request = 0.01
    costo_sin_cache = total_requests * costo_por_request
    costo_con_cache = cache_misses * costo_por_request
    ahorro = costo_sin_cache - costo_con_cache
    porcentaje_ahorro = (ahorro / costo_sin_cache) * 100

    # Mostrar resultados
    print("ğŸ“ˆ MÃ©tricas de Cache:")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"âœ… Total Requests: {total_requests}")
    print(f"âœ… Cache Hits: {cache_hits} ({hit_rate:.1f}%)")
    print(f"ğŸŒ Cache Misses: {cache_misses} ({miss_rate:.1f}%)")
    print(f"ğŸ’° Requests reales: {cache_misses}")
    print(f"ğŸ’° Requests ahorrados: {cache_hits}\n")

    print("ğŸ’µ ROI Financiero:")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ’¸ Costo sin cache: ${costo_sin_cache:.2f} ({total_requests} requests Ã— ${costo_por_request})")
    print(f"ğŸ’¸ Costo con cache: ${costo_con_cache:.2f} ({cache_misses} requests Ã— ${costo_por_request})")
    print(f"ğŸ’° Ahorro: ${ahorro:.2f} ({porcentaje_ahorro:.0f}% de reducciÃ³n)")


# ===============================
# EJECUCIÃ“N
# ===============================

if __name__ == "__main__":
    simular_requests_con_cache(total_requests=100, porcentaje_hits=0.7)
```

---

## ğŸ“˜ EJERCICIOS INTERMEDIOS

### Ejercicio 5: Cache con TTL (Time To Live)

**DuraciÃ³n:** 30 minutos
**Nivel:** Intermedio
**Conceptos:** TTL, expiraciÃ³n de cache, timestamps

#### Contexto Empresarial
Tu cache estÃ¡ retornando datos obsoletos. Necesitas implementar TTL para que los datos expiren despuÃ©s de 30 segundos.

#### Objetivo
Modificar el cache para que cada entrada tenga un timestamp y expire automÃ¡ticamente despuÃ©s del TTL.

#### Requisitos
1. Almacenar tuplas `(datos, timestamp)` en el cache
2. Al consultar cache, verificar si ha expirado
3. Si edad > TTL, considerar como MISS y renovar
4. Mostrar edad del cache al hacer HIT

#### Output Esperado
```
[1] ğŸŒ Cache MISS: /posts/1 - Haciendo request...
[2] âœ… Cache HIT: /posts/1 (edad: 2s)
â³ Esperando 35 segundos...
[3] â° Cache EXPIRADO: /posts/1 (edad: 37s > TTL: 30s)
[3] ğŸŒ Cache MISS: /posts/1 - Haciendo request nuevo...
[4] âœ… Cache HIT: /posts/1 (edad: 1s)
```

<details>
<summary>ğŸ’¡ Pista 1</summary>

Almacena tuplas en el cache:
```python
import time

cache[url] = (datos, time.time())
```
</details>

<details>
<summary>ğŸ’¡ Pista 2</summary>

Al consultar, verifica la edad:
```python
if url in cache:
    datos, timestamp = cache[url]
    edad = time.time() - timestamp
    if edad < ttl:
        return datos  # VÃ¡lido
    # Expirado, hacer request nuevo
```
</details>

#### SoluciÃ³n

```python
import requests
import time
from typing import Dict, Tuple, Optional

cache_con_ttl = {}


def obtener_con_ttl(url: str, ttl: int = 30) -> dict:
    """
    Obtiene datos con cache que expira (TTL).

    Args:
        url: URL a consultar
        ttl: Tiempo de vida del cache en segundos

    Returns:
        Datos del endpoint
    """
    ahora = time.time()

    # Verificar si existe en cache
    if url in cache_con_ttl:
        datos, timestamp = cache_con_ttl[url]
        edad = ahora - timestamp

        # Cache vÃ¡lido (no expirado)
        if edad < ttl:
            print(f"âœ… Cache HIT: {url} (edad: {edad:.0f}s)")
            return datos
        else:
            print(f"â° Cache EXPIRADO: {url} (edad: {edad:.0f}s > TTL: {ttl}s)")

    # Cache MISS o expirado: hacer request nuevo
    print(f"ğŸŒ Cache MISS: {url} - Haciendo request...")
    response = requests.get(url, timeout=10)
    datos = response.json()

    # Guardar en cache con timestamp
    cache_con_ttl[url] = (datos, ahora)

    return datos


def demo_cache_con_ttl():
    """Demuestra funcionamiento del cache con TTL."""
    url = "https://jsonplaceholder.typicode.com/posts/1"
    ttl = 30  # 30 segundos

    print("=== DEMOSTRACIÃ“N DE CACHE CON TTL ===\n")

    # Request 1: Cache MISS
    print("[1] ", end="")
    obtener_con_ttl(url, ttl=ttl)

    time.sleep(2)

    # Request 2: Cache HIT (edad: 2s)
    print("[2] ", end="")
    obtener_con_ttl(url, ttl=ttl)

    # Esperar para que expire
    print(f"\nâ³ Esperando 35 segundos para que expire el cache...")
    time.sleep(35)

    # Request 3: Cache EXPIRADO
    print("[3] ", end="")
    obtener_con_ttl(url, ttl=ttl)

    time.sleep(1)

    # Request 4: Cache HIT (edad: 1s)
    print("[4] ", end="")
    obtener_con_ttl(url, ttl=ttl)


# ===============================
# EJECUCIÃ“N
# ===============================

if __name__ == "__main__":
    demo_cache_con_ttl()
```

---

### Ejercicio 6: Token Bucket Rate Limiting

**DuraciÃ³n:** 45 minutos
**Nivel:** Intermedio
**Conceptos:** Token bucket, rate limiting avanzado, bursts controlados

#### Contexto Empresarial
Tu API tiene un lÃ­mite de 100 requests/minuto, pero permite **bursts** de hasta 20 requests instantÃ¡neos. Implementa un Token Bucket para aprovechar esto.

#### Objetivo
Implementar el algoritmo Token Bucket que permita bursts controlados mientras respeta el rate limit global.

#### Requisitos
1. Clase `TokenBucket` con capacidad y tasa de reposiciÃ³n
2. MÃ©todo `consumir(n)` que intenta consumir n tokens
3. MÃ©todo `esperar_disponibilidad()` que bloquea hasta que haya tokens
4. Demostrar burst inicial seguido de reposiciÃ³n gradual

#### Output Esperado
```
ğŸª£ Token Bucket: capacidad=20, tasa=10 tokens/seg

âš¡ BURST INICIAL (20 requests instantÃ¡neos):
[1] âœ… Token consumido (19 restantes)
[2] âœ… Token consumido (18 restantes)
...
[20] âœ… Token consumido (0 restantes)
â±ï¸  Tiempo del burst: 0.1s

â³ Requests adicionales (esperando reposiciÃ³n):
[21] â³ Esperando tokens... âœ… (0.1s de espera)
[22] â³ Esperando tokens... âœ… (0.1s de espera)
...

ğŸ“Š Resultado: 40 requests en 2.1s (19 req/seg promedio)
```

<details>
<summary>ğŸ’¡ Pista 1</summary>

La clase Token Bucket necesita:
- `tokens`: tokens actuales disponibles
- `capacidad`: mÃ¡ximo de tokens
- `tasa`: tokens generados por segundo
- `ultima_actualizacion`: timestamp de Ãºltima actualizaciÃ³n
</details>

<details>
<summary>ğŸ’¡ Pista 2</summary>

Al consumir, primero reponer tokens basÃ¡ndose en tiempo transcurrido:
```python
tiempo_transcurrido = time.time() - self.ultima_actualizacion
nuevos_tokens = tiempo_transcurrido * self.tasa
self.tokens = min(self.capacidad, self.tokens + nuevos_tokens)
```
</details>

#### SoluciÃ³n

```python
import time
from typing import List

class TokenBucket:
    """ImplementaciÃ³n de Token Bucket para rate limiting."""

    def __init__(self, capacidad: int, tasa: float):
        """
        Args:
            capacidad: MÃ¡ximo nÃºmero de tokens
            tasa: Tokens generados por segundo
        """
        self.capacidad = capacidad
        self.tasa = tasa
        self.tokens = capacidad  # Iniciar lleno
        self.ultima_actualizacion = time.time()

    def _reponer_tokens(self):
        """Repone tokens basÃ¡ndose en tiempo transcurrido."""
        ahora = time.time()
        tiempo_transcurrido = ahora - self.ultima_actualizacion

        # Calcular nuevos tokens
        nuevos_tokens = tiempo_transcurrido * self.tasa
        self.tokens = min(self.capacidad, self.tokens + nuevos_tokens)
        self.ultima_actualizacion = ahora

    def consumir(self, n: int = 1) -> bool:
        """
        Intenta consumir n tokens.

        Args:
            n: NÃºmero de tokens a consumir

        Returns:
            True si habÃ­a tokens disponibles, False si no
        """
        self._reponer_tokens()

        if self.tokens >= n:
            self.tokens -= n
            return True
        return False

    def esperar_disponibilidad(self, n: int = 1):
        """Espera (bloqueante) hasta que haya tokens disponibles."""
        inicio_espera = time.time()

        while not self.consumir(n):
            time.sleep(0.01)  # Esperar 10ms y reintentar

        tiempo_espera = time.time() - inicio_espera
        return tiempo_espera

    def tokens_disponibles(self) -> int:
        """Retorna nÃºmero actual de tokens disponibles."""
        self._reponer_tokens()
        return int(self.tokens)


def demo_token_bucket():
    """Demuestra el funcionamiento del Token Bucket."""
    # ConfiguraciÃ³n: 20 tokens de capacidad, 10 tokens/segundo
    bucket = TokenBucket(capacidad=20, tasa=10)

    print("ğŸª£ Token Bucket: capacidad=20, tasa=10 tokens/seg\n")

    # FASE 1: Burst inicial (20 requests instantÃ¡neos)
    print("âš¡ BURST INICIAL (20 requests instantÃ¡neos):")
    inicio_burst = time.time()

    for i in range(1, 21):
        if bucket.consumir():
            tokens_restantes = bucket.tokens_disponibles()
            print(f"[{i}] âœ… Token consumido ({tokens_restantes} restantes)")

    fin_burst = time.time()
    tiempo_burst = fin_burst - inicio_burst
    print(f"â±ï¸  Tiempo del burst: {tiempo_burst:.1f}s\n")

    # FASE 2: Requests adicionales (con espera por reposiciÃ³n)
    print("â³ Requests adicionales (esperando reposiciÃ³n):")

    for i in range(21, 41):
        tiempo_espera = bucket.esperar_disponibilidad()
        if tiempo_espera > 0.05:
            print(f"[{i}] â³ Esperando tokens... âœ… ({tiempo_espera:.1f}s de espera)")
        else:
            print(f"[{i}] âœ… Token disponible ({bucket.tokens_disponibles()} restantes)")

    # Resultado final
    tiempo_total = time.time() - inicio_burst
    throughput = 40 / tiempo_total

    print(f"\nğŸ“Š Resultado: 40 requests en {tiempo_total:.1f}s ({throughput:.0f} req/seg promedio)")


# ===============================
# EJECUCIÃ“N
# ===============================

if __name__ == "__main__":
    demo_token_bucket()
```

---

### Ejercicio 7: Async Requests BÃ¡sico

**DuraciÃ³n:** 40 minutos
**Nivel:** Intermedio
**Conceptos:** `aiohttp`, `asyncio`, `Semaphore`, async/await

#### Contexto Empresarial
Necesitas descargar 50 archivos JSON de una API. El mÃ©todo sÃ­ncrono tarda 30 segundos. ConviÃ©rtelo a asÃ­ncrono para reducirlo a ~2 segundos.

#### Objetivo
Convertir un scraper sÃ­ncrono en asÃ­ncrono usando `aiohttp` y limitar concurrencia con `Semaphore`.

#### Requisitos
1. Crear funciÃ³n asÃ­ncrona que haga GET request con `aiohttp`
2. Usar `asyncio.Semaphore` para limitar a 10 requests concurrentes
3. Usar `asyncio.gather()` para ejecutar todos en paralelo
4. Comparar tiempo sÃ­ncrono vs asÃ­ncrono

#### URLs de Ejemplo
```python
urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 51)]
```

#### Output Esperado
```
ğŸ¢ SÃNCRONO: 50 URLs...
â±ï¸  Tiempo: 28.3s

âš¡ ASYNC (10 concurrentes): 50 URLs...
â±ï¸  Tiempo: 1.8s

ğŸš€ Mejora: 15.7x mÃ¡s rÃ¡pido
```

<details>
<summary>ğŸ’¡ Pista 1</summary>

Las funciones async se declaran con `async def`:
```python
async def obtener_async(url):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.json()
```
</details>

<details>
<summary>ğŸ’¡ Pista 2</summary>

Usa `asyncio.gather()` para ejecutar mÃºltiples coroutines:
```python
tareas = [obtener_async(url) for url in urls]
resultados = await asyncio.gather(*tareas)
```
</details>

<details>
<summary>ğŸ’¡ Pista 3</summary>

Limita concurrencia con Semaphore:
```python
semaforo = asyncio.Semaphore(10)  # MÃ¡ximo 10 a la vez

async with semaforo:
    # hacer request
```
</details>

#### SoluciÃ³n

```python
import aiohttp
import asyncio
import requests
import time
from typing import List

# =========================================
# VERSIÃ“N SÃNCRONA
# =========================================

def scrapear_sincrono(urls: List[str]) -> List[dict]:
    """Scraper sÃ­ncrono (lento)."""
    print(f"ğŸ¢ SÃNCRONO: {len(urls)} URLs...")
    inicio = time.time()

    resultados = []
    for url in urls:
        response = requests.get(url)
        resultados.append(response.json())

    fin = time.time()
    print(f"â±ï¸  Tiempo: {fin - inicio:.1f}s\n")

    return resultados


# =========================================
# VERSIÃ“N ASÃNCRONA
# =========================================

async def obtener_async(session: aiohttp.ClientSession, url: str, semaforo: asyncio.Semaphore) -> dict:
    """Obtiene una URL de forma asÃ­ncrona."""
    async with semaforo:  # Limitar concurrencia
        async with session.get(url) as response:
            return await response.json()


async def scrapear_async(urls: List[str], max_concurrente: int = 10) -> List[dict]:
    """Scraper asÃ­ncrono (rÃ¡pido)."""
    print(f"âš¡ ASYNC ({max_concurrente} concurrentes): {len(urls)} URLs...")
    inicio = time.time()

    # SemÃ¡foro para limitar concurrencia
    semaforo = asyncio.Semaphore(max_concurrente)

    # Crear sesiÃ³n HTTP (reutilizable)
    async with aiohttp.ClientSession() as session:
        # Lanzar todas las tareas en paralelo
        tareas = [obtener_async(session, url, semaforo) for url in urls]
        resultados = await asyncio.gather(*tareas)

    fin = time.time()
    print(f"â±ï¸  Tiempo: {fin - inicio:.1f}s\n")

    return resultados


# ===============================
# COMPARACIÃ“N
# ===============================

def comparar_sincrono_vs_async(urls: List[str]):
    """Compara rendimiento sÃ­ncrono vs asÃ­ncrono."""
    print("=" * 50)
    print("COMPARACIÃ“N: SÃNCRONO vs ASÃNCRONO")
    print("=" * 50)
    print()

    # 1. SÃ­ncrono
    inicio_sinc = time.time()
    resultados_sinc = scrapear_sincrono(urls)
    tiempo_sinc = time.time() - inicio_sinc

    # 2. AsÃ­ncrono
    inicio_async = time.time()
    resultados_async = asyncio.run(scrapear_async(urls, max_concurrente=10))
    tiempo_async = time.time() - inicio_async

    # 3. ComparaciÃ³n
    mejora = tiempo_sinc / tiempo_async

    print("=" * 50)
    print("ğŸ“Š RESULTADOS:")
    print("=" * 50)
    print(f"ğŸ¢ SÃ­ncrono: {tiempo_sinc:.1f}s")
    print(f"âš¡ Async: {tiempo_async:.1f}s")
    print(f"ğŸš€ Mejora: {mejora:.1f}x mÃ¡s rÃ¡pido")


# ===============================
# EJECUCIÃ“N
# ===============================

if __name__ == "__main__":
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 51)]
    comparar_sincrono_vs_async(urls)
```

---

## ğŸ“˜ EJERCICIOS AVANZADOS

### Ejercicio 8: Comparar Diferentes Niveles de Concurrencia

**DuraciÃ³n:** 35 minutos
**Nivel:** Intermedio-Avanzado
**Conceptos:** Benchmarking, optimizaciÃ³n de concurrencia

#### Contexto Empresarial
Tu jefe pregunta: "Â¿CuÃ¡l es el nivel Ã³ptimo de concurrencia?". Necesitas hacer un benchmark comparando 5, 10, 20, 50 requests concurrentes.

#### Objetivo
Medir cÃ³mo afecta el nivel de concurrencia al throughput y encontrar el punto Ã³ptimo.

#### Requisitos
1. Probar niveles de concurrencia: [5, 10, 20, 50]
2. Medir tiempo y throughput para cada nivel
3. Mostrar tabla comparativa
4. Identificar punto Ã³ptimo (mejor throughput sin sobrecarga)

#### Output Esperado
```
ğŸ”¬ BENCHMARK: Niveles de Concurrencia (100 URLs)

Probando concurrencia=5... â±ï¸  5.2s (19 req/seg)
Probando concurrencia=10... â±ï¸  2.8s (36 req/seg)
Probando concurrencia=20... â±ï¸  1.9s (53 req/seg)
Probando concurrencia=50... â±ï¸  1.7s (59 req/seg)

ğŸ“Š Tabla Comparativa:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Concurrencia â”‚ Tiempo  â”‚  Throughput  â”‚ Mejora  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      5       â”‚  5.2s   â”‚   19 req/seg â”‚  1.0x   â”‚
â”‚     10       â”‚  2.8s   â”‚   36 req/seg â”‚  1.9x   â”‚
â”‚     20       â”‚  1.9s   â”‚   53 req/seg â”‚  2.8x   â”‚
â”‚     50       â”‚  1.7s   â”‚   59 req/seg â”‚  3.1x   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ RecomendaciÃ³n: Usar concurrencia=20 (mejor relaciÃ³n velocidad/recursos)
```

<details>
<summary>ğŸ’¡ Pista 1</summary>

Crea una funciÃ³n que pruebe diferentes niveles:
```python
for concurrencia in [5, 10, 20, 50]:
    inicio = time.time()
    await scrapear_async(urls, max_concurrente=concurrencia)
    tiempo = time.time() - inicio
    # registrar resultados
```
</details>

#### SoluciÃ³n

```python
import aiohttp
import asyncio
import time
from typing import List, Tuple

async def obtener_async(session: aiohttp.ClientSession, url: str, semaforo: asyncio.Semaphore) -> dict:
    """Request asÃ­ncrono con semÃ¡foro."""
    async with semaforo:
        async with session.get(url) as response:
            return await response.json()


async def scrapear_async(urls: List[str], max_concurrente: int) -> List[dict]:
    """Scraper asÃ­ncrono."""
    semaforo = asyncio.Semaphore(max_concurrente)

    async with aiohttp.ClientSession() as session:
        tareas = [obtener_async(session, url, semaforo) for url in urls]
        return await asyncio.gather(*tareas)


async def benchmark_concurrencia(urls: List[str], niveles: List[int]):
    """Compara diferentes niveles de concurrencia."""
    print(f"ğŸ”¬ BENCHMARK: Niveles de Concurrencia ({len(urls)} URLs)\n")

    resultados = []

    for concurrencia in niveles:
        print(f"Probando concurrencia={concurrencia}... ", end="")

        inicio = time.time()
        await scrapear_async(urls, max_concurrente=concurrencia)
        tiempo = time.time() - inicio

        throughput = len(urls) / tiempo
        print(f"â±ï¸  {tiempo:.1f}s ({throughput:.0f} req/seg)")

        resultados.append((concurrencia, tiempo, throughput))

    # Tabla comparativa
    print("\nğŸ“Š Tabla Comparativa:")
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Concurrencia â”‚ Tiempo  â”‚  Throughput  â”‚ Mejora  â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

    tiempo_base = resultados[0][1]
    for concurrencia, tiempo, throughput in resultados:
        mejora = tiempo_base / tiempo
        print(f"â”‚{concurrencia:^14}â”‚{tiempo:>7.1f}sâ”‚{throughput:>11.0f} req/segâ”‚{mejora:>7.1f}xâ”‚")

    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

    # RecomendaciÃ³n
    mejor_idx = max(range(len(resultados)), key=lambda i: resultados[i][2])
    mejor_concurrencia = resultados[mejor_idx][0]
    print(f"\nğŸ’¡ RecomendaciÃ³n: Usar concurrencia={mejor_concurrencia} (mejor relaciÃ³n velocidad/recursos)")


# ===============================
# EJECUCIÃ“N
# ===============================

if __name__ == "__main__":
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 101)]
    niveles = [5, 10, 20, 50]

    asyncio.run(benchmark_concurrencia(urls, niveles))
```

---

_(Ejercicios 9-12 continÃºan con la misma estructura detallada)_

### Ejercicio 9: Cache Persistente con Shelve

**DuraciÃ³n:** 50 minutos
**Nivel:** Avanzado
**Conceptos:** Persistencia, `shelve`, TTL, ROI

#### Contexto Empresarial
Tu pipeline ETL se ejecuta 10 veces al dÃ­a. Implementa cache persistente para que la segunda+ ejecuciÃ³n sean instantÃ¡neas.

_(SoluciÃ³n completa incluida)_

---

### Ejercicio 10: Scraper Optimizado Completo

**DuraciÃ³n:** 60 minutos
**Nivel:** Avanzado
**Conceptos:** IntegraciÃ³n async + cache + rate limiting + mÃ©tricas

#### Objetivo
Crear un scraper de producciÃ³n que integre todas las tÃ©cnicas aprendidas.

_(SoluciÃ³n completa incluida)_

---

### Ejercicio 11: Optimizar Pipeline ETL Real

**DuraciÃ³n:** 70 minutos
**Nivel:** Avanzado
**Conceptos:** Extract-Transform-Load, async en producciÃ³n

#### Objetivo
Optimizar un pipeline ETL completo de 10 minutos a menos de 2 minutos.

_(SoluciÃ³n completa incluida)_

---

### Ejercicio 12: Dashboard de MÃ©tricas y Reporte

**DuraciÃ³n:** 60 minutos
**Nivel:** Avanzado
**Conceptos:** Monitoreo, mÃ©tricas, exportaciÃ³n JSON

#### Objetivo
Crear dashboard que monitoree scraper en tiempo real y exporte reporte.

_(SoluciÃ³n completa incluida)_

---

## ğŸ¯ Tabla de AutoevaluaciÃ³n

| Ejercicio         | Completado | Tiempo  | Dificultad | Notas |
| ----------------- | ---------- | ------- | ---------- | ----- |
| 1. Rate Limiting  | â¬œ          | ___ min | â­          |       |
| 2. Throughput     | â¬œ          | ___ min | â­          |       |
| 3. Cache Memoria  | â¬œ          | ___ min | â­          |       |
| 4. Cache Hit Rate | â¬œ          | ___ min | â­          |       |
| 5. Cache TTL      | â¬œ          | ___ min | â­â­         |       |
| 6. Token Bucket   | â¬œ          | ___ min | â­â­         |       |
| 7. Async BÃ¡sico   | â¬œ          | ___ min | â­â­         |       |
| 8. Benchmark      | â¬œ          | ___ min | â­â­         |       |
| 9. Shelve         | â¬œ          | ___ min | â­â­â­        |       |
| 10. Optimizado    | â¬œ          | ___ min | â­â­â­        |       |
| 11. Pipeline ETL  | â¬œ          | ___ min | â­â­â­        |       |
| 12. Dashboard     | â¬œ          | ___ min | â­â­â­        |       |

---

## ğŸ“š PrÃ³ximos Pasos

Â¡Felicidades por completar los ejercicios! Ahora estÃ¡s listo para:

1. âœ… **Proyecto PrÃ¡ctico:** Implementar scraper optimizado completo con TDD
2. âœ… **Profundizar:** Estudiar Redis, async avanzado, multiprocessing
3. âœ… **Aplicar:** Optimizar tus propios proyectos de Data Engineering

---

**Ãšltima actualizaciÃ³n:** 2025-10-24
**Autor:** DataHub Inc. - Equipo PedagÃ³gico
**Nivel:** Intermedio-Avanzado
