# üèãÔ∏è Ejercicios Pr√°cticos - Tema 3: Rate Limiting y Caching

**M√≥dulo 4:** APIs y Web Scraping
**Duraci√≥n estimada:** 6-10 horas de pr√°ctica
**Prerequisitos:** Haber completado 01-TEORIA.md y 02-EJEMPLOS.md del Tema 3

---

## üéØ Introducci√≥n

Este archivo contiene **12 ejercicios progresivos** dise√±ados para que practiques y domines las t√©cnicas de optimizaci√≥n de extracci√≥n de datos:

- **B√°sicos (1-4):** Rate limiting, medici√≥n de tiempos, cache b√°sico
- **Intermedios (5-8):** Cache con TTL, token bucket, async requests
- **Avanzados (9-12):** Scrapers optimizados completos con m√©tricas

**Metodolog√≠a recomendada:**
1. Lee el enunciado completo
2. Intenta resolver sin mirar las pistas
3. Si te atascas, lee las pistas progresivas
4. Compara tu soluci√≥n con la proporcionada
5. Ejecuta el c√≥digo y verifica el output

---

## üìä Tabla de Contenidos

| #   | Ejercicio                       | Nivel      | Duraci√≥n | Conceptos                     |
| --- | ------------------------------- | ---------- | -------- | ----------------------------- |
| 1   | Rate Limiting B√°sico            | B√°sico     | 20 min   | `time.sleep()`, medici√≥n      |
| 2   | Medir Throughput                | B√°sico     | 15 min   | M√©tricas, velocidad           |
| 3   | Cache en Memoria                | B√°sico     | 25 min   | `dict`, cache hits            |
| 4   | Calcular Cache Hit Rate         | B√°sico     | 20 min   | M√©tricas de cache             |
| 5   | Cache con TTL                   | Intermedio | 30 min   | Expiraci√≥n, timestamps        |
| 6   | Token Bucket                    | Intermedio | 45 min   | Rate limiting avanzado        |
| 7   | Async Requests B√°sico           | Intermedio | 40 min   | `aiohttp`, `asyncio`          |
| 8   | Comparar S√≠ncrono vs Async      | Intermedio | 35 min   | Benchmarking                  |
| 9   | Cache Persistente con Shelve    | Avanzado   | 50 min   | Persistencia, `shelve`        |
| 10  | Scraper Optimizado con M√©tricas | Avanzado   | 60 min   | Integraci√≥n completa          |
| 11  | Optimizar Pipeline ETL          | Avanzado   | 70 min   | Async + Cache + Rate Limiting |
| 12  | Dashboard de M√©tricas           | Avanzado   | 60 min   | Monitoreo, ROI                |

---

## üìò EJERCICIOS B√ÅSICOS

### Ejercicio 1: Rate Limiting B√°sico

**Duraci√≥n:** 20 minutos
**Nivel:** B√°sico
**Conceptos:** `time.sleep()`, rate limiting, medici√≥n de tiempo

#### Contexto Empresarial
Trabajas en **DataHub Inc.** y necesitas scrapear 20 productos de un sitio web. Para ser respetuoso con el servidor, debes implementar un delay de **1.5 segundos** entre cada request.

#### Objetivo
Implementar una funci√≥n que haga requests HTTP con rate limiting y mida el tiempo total de ejecuci√≥n.

#### URLs de Ejemplo
```python
urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 21)]
```

#### Requisitos
1. Funci√≥n que reciba lista de URLs y delay en segundos
2. Hacer request a cada URL con `requests.get()`
3. Esperar el delay especificado entre requests (excepto despu√©s del √∫ltimo)
4. Medir y mostrar tiempo total de ejecuci√≥n
5. Calcular y mostrar velocidad (requests/segundo)

#### Output Esperado
```
üöÄ Scraping 20 URLs con delay de 1.5s entre cada una
‚è±Ô∏è  Tiempo estimado: 28.5s

[1/20] ‚úÖ https://jsonplaceholder.typicode.com/posts/1
[2/20] ‚úÖ https://jsonplaceholder.typicode.com/posts/2
...
[20/20] ‚úÖ https://jsonplaceholder.typicode.com/posts/20

üìä Resultados:
‚è±Ô∏è  Tiempo total: 30.2s
‚ö° Velocidad: 0.66 req/seg
‚úÖ Rate limiting respetado: 1.5s entre requests
```

<details>
<summary>üí° Pista 1</summary>

Usa `time.time()` para medir el tiempo:
```python
import time

inicio = time.time()
# ... hacer algo ...
fin = time.time()
tiempo_total = fin - inicio
```
</details>

<details>
<summary>üí° Pista 2</summary>

Usa `time.sleep(delay)` DESPU√âS de cada request, excepto el √∫ltimo:
```python
for i, url in enumerate(urls, 1):
    hacer_request(url)
    if i < len(urls):  # No esperar despu√©s del √∫ltimo
        time.sleep(delay)
```
</details>

<details>
<summary>üí° Pista 3</summary>

El tiempo total debe ser aproximadamente: `(num_urls - 1) * delay + tiempo_requests`
</details>

#### Soluci√≥n

```python
import requests
import time
from typing import List

def scrapear_con_rate_limiting(urls: List[str], delay: float = 1.5) -> List[dict]:
    """
    Scrapea URLs con rate limiting.

    Args:
        urls: Lista de URLs a scrapear
        delay: Segundos de espera entre requests

    Returns:
        Lista de responses
    """
    print(f"üöÄ Scraping {len(urls)} URLs con delay de {delay}s entre cada una")
    print(f"‚è±Ô∏è  Tiempo estimado: {(len(urls) - 1) * delay}s\n")

    inicio = time.time()
    resultados = []

    for i, url in enumerate(urls, 1):
        try:
            response = requests.get(url, timeout=10)
            resultados.append(response.json())
            print(f"[{i}/{len(urls)}] ‚úÖ {url}")
        except Exception as e:
            print(f"[{i}/{len(urls)}] ‚ùå {url} - Error: {e}")

        # Rate limiting: esperar entre requests (excepto el √∫ltimo)
        if i < len(urls):
            time.sleep(delay)

    fin = time.time()
    tiempo_total = fin - inicio
    velocidad = len(urls) / tiempo_total

    print(f"\nüìä Resultados:")
    print(f"‚è±Ô∏è  Tiempo total: {tiempo_total:.1f}s")
    print(f"‚ö° Velocidad: {velocidad:.2f} req/seg")
    print(f"‚úÖ Rate limiting respetado: {delay}s entre requests")

    return resultados


# ===============================
# EJECUCI√ìN
# ===============================

if __name__ == "__main__":
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 21)]
    resultados = scrapear_con_rate_limiting(urls, delay=1.5)
    print(f"\nüì¶ Total de datos obtenidos: {len(resultados)}")
```

---

### Ejercicio 2: Medir Throughput

**Duraci√≥n:** 15 minutos
**Nivel:** B√°sico
**Conceptos:** M√©tricas, throughput, latencia

#### Contexto Empresarial
Tu jefe te pide comparar dos estrategias de scraping: una con delay de 0.5s y otra con 1.5s. Necesitas medir el **throughput** (requests/segundo) de cada una.

#### Objetivo
Crear una funci√≥n que mida throughput y latencia promedio de un scraper.

#### Requisitos
1. Funci√≥n que reciba URLs y delay
2. Calcular throughput (total_requests / tiempo_total)
3. Calcular latencia promedio por request
4. Comparar dos estrategias diferentes

#### Output Esperado
```
Estrategia 1 (delay=0.5s):
‚ö° Throughput: 1.2 req/seg
‚è±Ô∏è  Latencia promedio: 833ms

Estrategia 2 (delay=1.5s):
‚ö° Throughput: 0.6 req/seg
‚è±Ô∏è  Latencia promedio: 1667ms

üìä Conclusi√≥n: Estrategia 1 es 2.0x m√°s r√°pida
```

<details>
<summary>üí° Pista 1</summary>

Throughput = total_requests / tiempo_total (en segundos)
</details>

<details>
<summary>üí° Pista 2</summary>

Latencia promedio = tiempo_total / total_requests * 1000 (en milisegundos)
</details>

#### Soluci√≥n

```python
import requests
import time
from typing import List, Tuple

def medir_throughput(urls: List[str], delay: float) -> Tuple[float, float]:
    """
    Mide throughput y latencia de un scraper.

    Args:
        urls: Lista de URLs
        delay: Delay entre requests

    Returns:
        (throughput, latencia_promedio)
    """
    inicio = time.time()

    for i, url in enumerate(urls):
        requests.get(url)
        if i < len(urls) - 1:
            time.sleep(delay)

    fin = time.time()
    tiempo_total = fin - inicio

    throughput = len(urls) / tiempo_total
    latencia_promedio = (tiempo_total / len(urls)) * 1000  # ms

    return throughput, latencia_promedio


def comparar_estrategias(urls: List[str]):
    """Compara dos estrategias de scraping."""

    print("Estrategia 1 (delay=0.5s):")
    throughput1, latencia1 = medir_throughput(urls, delay=0.5)
    print(f"‚ö° Throughput: {throughput1:.1f} req/seg")
    print(f"‚è±Ô∏è  Latencia promedio: {latencia1:.0f}ms\n")

    print("Estrategia 2 (delay=1.5s):")
    throughput2, latencia2 = medir_throughput(urls, delay=1.5)
    print(f"‚ö° Throughput: {throughput2:.1f} req/seg")
    print(f"‚è±Ô∏è  Latencia promedio: {latencia2:.0f}ms\n")

    mejora = throughput1 / throughput2
    print(f"üìä Conclusi√≥n: Estrategia 1 es {mejora:.1f}x m√°s r√°pida")


# ===============================
# EJECUCI√ìN
# ===============================

if __name__ == "__main__":
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 11)]
    comparar_estrategias(urls)
```

---

### Ejercicio 3: Cache en Memoria

**Duraci√≥n:** 25 minutos
**Nivel:** B√°sico
**Conceptos:** Cache, diccionario, cache hits/misses

#### Contexto Empresarial
Tu script hace m√∫ltiples requests a las mismas URLs durante su ejecuci√≥n. Implementa un cache en memoria para evitar requests redundantes.

#### Objetivo
Implementar una funci√≥n que use un diccionario Python como cache para requests HTTP.

#### Requisitos
1. Usar un `dict` global como cache
2. Antes de cada request, verificar si la URL ya est√° en cache
3. Si est√° en cache (HIT), retornar datos sin hacer request
4. Si no est√° en cache (MISS), hacer request y guardar en cache
5. Contar y mostrar hits y misses

#### URLs de Ejemplo
```python
# Lista con URLs repetidas para demostrar cache
urls = [
    "https://jsonplaceholder.typicode.com/posts/1",
    "https://jsonplaceholder.typicode.com/posts/2",
    "https://jsonplaceholder.typicode.com/posts/1",  # Repetida
    "https://jsonplaceholder.typicode.com/posts/3",
    "https://jsonplaceholder.typicode.com/posts/2",  # Repetida
    "https://jsonplaceholder.typicode.com/posts/1",  # Repetida
]
```

#### Output Esperado
```
[1/6] üåê Cache MISS: /posts/1 - Haciendo request...
[2/6] üåê Cache MISS: /posts/2 - Haciendo request...
[3/6] ‚úÖ Cache HIT: /posts/1
[4/6] üåê Cache MISS: /posts/3 - Haciendo request...
[5/6] ‚úÖ Cache HIT: /posts/2
[6/6] ‚úÖ Cache HIT: /posts/1

üìä Estad√≠sticas:
‚úÖ Cache Hits: 3 (50.0%)
üåê Cache Misses: 3 (50.0%)
üí∞ Requests ahorrados: 3 (50% menos tr√°fico)
```

<details>
<summary>üí° Pista 1</summary>

Declara el cache como diccionario global:
```python
cache = {}

def obtener_con_cache(url):
    if url in cache:
        # Cache HIT
        return cache[url]
    # Cache MISS
    data = requests.get(url).json()
    cache[url] = data
    return data
```
</details>

<details>
<summary>üí° Pista 2</summary>

Cache Hit Rate = (cache_hits / total_requests) * 100
</details>

#### Soluci√≥n

```python
import requests
from typing import List, Dict

# Cache global
cache_memoria = {}
cache_hits = 0
cache_misses = 0


def obtener_con_cache(url: str) -> dict:
    """
    Obtiene datos con cache en memoria.

    Args:
        url: URL a consultar

    Returns:
        Datos del endpoint
    """
    global cache_hits, cache_misses

    if url in cache_memoria:
        cache_hits += 1
        print(f"‚úÖ Cache HIT: {url}")
        return cache_memoria[url]

    cache_misses += 1
    print(f"üåê Cache MISS: {url} - Haciendo request...")

    response = requests.get(url, timeout=10)
    data = response.json()

    cache_memoria[url] = data
    return data


def procesar_urls_con_cache(urls: List[str]):
    """Procesa URLs con cache y muestra estad√≠sticas."""
    resultados = []

    for i, url in enumerate(urls, 1):
        print(f"[{i}/{len(urls)}] ", end="")
        data = obtener_con_cache(url)
        resultados.append(data)

    # Estad√≠sticas
    total = cache_hits + cache_misses
    hit_rate = (cache_hits / total * 100) if total > 0 else 0

    print(f"\nüìä Estad√≠sticas:")
    print(f"‚úÖ Cache Hits: {cache_hits} ({hit_rate:.1f}%)")
    print(f"üåê Cache Misses: {cache_misses} ({100-hit_rate:.1f}%)")
    print(f"üí∞ Requests ahorrados: {cache_hits} ({hit_rate:.0f}% menos tr√°fico)")

    return resultados


# ===============================
# EJECUCI√ìN
# ===============================

if __name__ == "__main__":
    # URLs con repeticiones
    urls = [
        "https://jsonplaceholder.typicode.com/posts/1",
        "https://jsonplaceholder.typicode.com/posts/2",
        "https://jsonplaceholder.typicode.com/posts/1",  # Repetida
        "https://jsonplaceholder.typicode.com/posts/3",
        "https://jsonplaceholder.typicode.com/posts/2",  # Repetida
        "https://jsonplaceholder.typicode.com/posts/1",  # Repetida
    ]

    resultados = procesar_urls_con_cache(urls)
    print(f"\nüì¶ Total de datos obtenidos: {len(resultados)}")
```

---

### Ejercicio 4: Calcular Cache Hit Rate

**Duraci√≥n:** 20 minutos
**Nivel:** B√°sico
**Conceptos:** M√©tricas de cache, an√°lisis de rendimiento

#### Contexto Empresarial
Tu empresa quiere saber qu√© tan efectivo es el cache. Necesitas calcular el **Cache Hit Rate** y determinar cu√°nto dinero se ahorra si cada request cuesta $0.01.

#### Objetivo
Crear una funci√≥n que calcule m√©tricas de cache y ROI (Return on Investment).

#### Requisitos
1. Simular 100 requests donde el 70% son repetidos (deber√≠an venir de cache)
2. Calcular Cache Hit Rate
3. Calcular requests ahorrados
4. Calcular ahorro monetario (asumiendo $0.01 por request)

#### Output Esperado
```
üìä Simulaci√≥n de 100 Requests con Cache

Progreso: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100/100

üìà M√©tricas de Cache:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ Total Requests: 100
‚úÖ Cache Hits: 70 (70.0%)
üåê Cache Misses: 30 (30.0%)
üí∞ Requests reales: 30
üí∞ Requests ahorrados: 70

üíµ ROI Financiero:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üí∏ Costo sin cache: $1.00 (100 requests √ó $0.01)
üí∏ Costo con cache: $0.30 (30 requests √ó $0.01)
üí∞ Ahorro: $0.70 (70% de reducci√≥n)
```

<details>
<summary>üí° Pista 1</summary>

Para simular 70% de repeticiones, genera 30 URLs √∫nicas y rep√≠telas:
```python
import random

urls_unicas = [f"url_{i}" for i in range(30)]
urls_total = urls_unicas + random.choices(urls_unicas, k=70)
random.shuffle(urls_total)
```
</details>

<details>
<summary>üí° Pista 2</summary>

Cache Hit Rate = (hits / (hits + misses)) * 100
</details>

#### Soluci√≥n

```python
import random
from typing import List

def simular_requests_con_cache(total_requests: int, porcentaje_hits: float = 0.7):
    """
    Simula requests con cache y calcula m√©tricas.

    Args:
        total_requests: N√∫mero total de requests a simular
        porcentaje_hits: Porcentaje esperado de cache hits
    """
    # Generar URLs (30% √∫nicas, 70% repetidas)
    num_unicas = int(total_requests * (1 - porcentaje_hits))
    urls_unicas = [f"https://api.example.com/data/{i}" for i in range(num_unicas)]

    # Crear lista total: URLs √∫nicas + repeticiones
    urls_totales = urls_unicas.copy()
    urls_repetidas = random.choices(urls_unicas, k=total_requests - num_unicas)
    urls_totales.extend(urls_repetidas)
    random.shuffle(urls_totales)

    # Simular cache
    cache = {}
    cache_hits = 0
    cache_misses = 0

    print(f"üìä Simulaci√≥n de {total_requests} Requests con Cache\n")

    for i, url in enumerate(urls_totales, 1):
        if url in cache:
            cache_hits += 1
        else:
            cache_misses += 1
            cache[url] = f"datos_{i}"

        # Barra de progreso
        if i % 5 == 0 or i == total_requests:
            progreso = i / total_requests
            barra = "‚ñà" * int(progreso * 20)
            print(f"\rProgreso: {barra:<20} {i}/{total_requests}", end="")

    print("\n")

    # Calcular m√©tricas
    hit_rate = (cache_hits / total_requests) * 100
    miss_rate = (cache_misses / total_requests) * 100

    # ROI Financiero
    costo_por_request = 0.01
    costo_sin_cache = total_requests * costo_por_request
    costo_con_cache = cache_misses * costo_por_request
    ahorro = costo_sin_cache - costo_con_cache
    porcentaje_ahorro = (ahorro / costo_sin_cache) * 100

    # Mostrar resultados
    print("üìà M√©tricas de Cache:")
    print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    print(f"‚úÖ Total Requests: {total_requests}")
    print(f"‚úÖ Cache Hits: {cache_hits} ({hit_rate:.1f}%)")
    print(f"üåê Cache Misses: {cache_misses} ({miss_rate:.1f}%)")
    print(f"üí∞ Requests reales: {cache_misses}")
    print(f"üí∞ Requests ahorrados: {cache_hits}\n")

    print("üíµ ROI Financiero:")
    print("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    print(f"üí∏ Costo sin cache: ${costo_sin_cache:.2f} ({total_requests} requests √ó ${costo_por_request})")
    print(f"üí∏ Costo con cache: ${costo_con_cache:.2f} ({cache_misses} requests √ó ${costo_por_request})")
    print(f"üí∞ Ahorro: ${ahorro:.2f} ({porcentaje_ahorro:.0f}% de reducci√≥n)")


# ===============================
# EJECUCI√ìN
# ===============================

if __name__ == "__main__":
    simular_requests_con_cache(total_requests=100, porcentaje_hits=0.7)
```

---

## üìò EJERCICIOS INTERMEDIOS

### Ejercicio 5: Cache con TTL (Time To Live)

**Duraci√≥n:** 30 minutos
**Nivel:** Intermedio
**Conceptos:** TTL, expiraci√≥n de cache, timestamps

#### Contexto Empresarial
Tu cache est√° retornando datos obsoletos. Necesitas implementar TTL para que los datos expiren despu√©s de 30 segundos.

#### Objetivo
Modificar el cache para que cada entrada tenga un timestamp y expire autom√°ticamente despu√©s del TTL.

#### Requisitos
1. Almacenar tuplas `(datos, timestamp)` en el cache
2. Al consultar cache, verificar si ha expirado
3. Si edad > TTL, considerar como MISS y renovar
4. Mostrar edad del cache al hacer HIT

#### Output Esperado
```
[1] üåê Cache MISS: /posts/1 - Haciendo request...
[2] ‚úÖ Cache HIT: /posts/1 (edad: 2s)
‚è≥ Esperando 35 segundos...
[3] ‚è∞ Cache EXPIRADO: /posts/1 (edad: 37s > TTL: 30s)
[3] üåê Cache MISS: /posts/1 - Haciendo request nuevo...
[4] ‚úÖ Cache HIT: /posts/1 (edad: 1s)
```

<details>
<summary>üí° Pista 1</summary>

Almacena tuplas en el cache:
```python
import time

cache[url] = (datos, time.time())
```
</details>

<details>
<summary>üí° Pista 2</summary>

Al consultar, verifica la edad:
```python
if url in cache:
    datos, timestamp = cache[url]
    edad = time.time() - timestamp
    if edad < ttl:
        return datos  # V√°lido
    # Expirado, hacer request nuevo
```
</details>

#### Soluci√≥n

```python
import requests
import time
from typing import Dict, Tuple, Optional

cache_con_ttl = {}


def obtener_con_ttl(url: str, ttl: int = 30) -> dict:
    """
    Obtiene datos con cache que expira (TTL).

    Args:
        url: URL a consultar
        ttl: Tiempo de vida del cache en segundos

    Returns:
        Datos del endpoint
    """
    ahora = time.time()

    # Verificar si existe en cache
    if url in cache_con_ttl:
        datos, timestamp = cache_con_ttl[url]
        edad = ahora - timestamp

        # Cache v√°lido (no expirado)
        if edad < ttl:
            print(f"‚úÖ Cache HIT: {url} (edad: {edad:.0f}s)")
            return datos
        else:
            print(f"‚è∞ Cache EXPIRADO: {url} (edad: {edad:.0f}s > TTL: {ttl}s)")

    # Cache MISS o expirado: hacer request nuevo
    print(f"üåê Cache MISS: {url} - Haciendo request...")
    response = requests.get(url, timeout=10)
    datos = response.json()

    # Guardar en cache con timestamp
    cache_con_ttl[url] = (datos, ahora)

    return datos


def demo_cache_con_ttl():
    """Demuestra funcionamiento del cache con TTL."""
    url = "https://jsonplaceholder.typicode.com/posts/1"
    ttl = 30  # 30 segundos

    print("=== DEMOSTRACI√ìN DE CACHE CON TTL ===\n")

    # Request 1: Cache MISS
    print("[1] ", end="")
    obtener_con_ttl(url, ttl=ttl)

    time.sleep(2)

    # Request 2: Cache HIT (edad: 2s)
    print("[2] ", end="")
    obtener_con_ttl(url, ttl=ttl)

    # Esperar para que expire
    print(f"\n‚è≥ Esperando 35 segundos para que expire el cache...")
    time.sleep(35)

    # Request 3: Cache EXPIRADO
    print("[3] ", end="")
    obtener_con_ttl(url, ttl=ttl)

    time.sleep(1)

    # Request 4: Cache HIT (edad: 1s)
    print("[4] ", end="")
    obtener_con_ttl(url, ttl=ttl)


# ===============================
# EJECUCI√ìN
# ===============================

if __name__ == "__main__":
    demo_cache_con_ttl()
```

---

### Ejercicio 6: Token Bucket Rate Limiting

**Duraci√≥n:** 45 minutos
**Nivel:** Intermedio
**Conceptos:** Token bucket, rate limiting avanzado, bursts controlados

#### Contexto Empresarial
Tu API tiene un l√≠mite de 100 requests/minuto, pero permite **bursts** de hasta 20 requests instant√°neos. Implementa un Token Bucket para aprovechar esto.

#### Objetivo
Implementar el algoritmo Token Bucket que permita bursts controlados mientras respeta el rate limit global.

#### Requisitos
1. Clase `TokenBucket` con capacidad y tasa de reposici√≥n
2. M√©todo `consumir(n)` que intenta consumir n tokens
3. M√©todo `esperar_disponibilidad()` que bloquea hasta que haya tokens
4. Demostrar burst inicial seguido de reposici√≥n gradual

#### Output Esperado
```
ü™£ Token Bucket: capacidad=20, tasa=10 tokens/seg

‚ö° BURST INICIAL (20 requests instant√°neos):
[1] ‚úÖ Token consumido (19 restantes)
[2] ‚úÖ Token consumido (18 restantes)
...
[20] ‚úÖ Token consumido (0 restantes)
‚è±Ô∏è  Tiempo del burst: 0.1s

‚è≥ Requests adicionales (esperando reposici√≥n):
[21] ‚è≥ Esperando tokens... ‚úÖ (0.1s de espera)
[22] ‚è≥ Esperando tokens... ‚úÖ (0.1s de espera)
...

üìä Resultado: 40 requests en 2.1s (19 req/seg promedio)
```

<details>
<summary>üí° Pista 1</summary>

La clase Token Bucket necesita:
- `tokens`: tokens actuales disponibles
- `capacidad`: m√°ximo de tokens
- `tasa`: tokens generados por segundo
- `ultima_actualizacion`: timestamp de √∫ltima actualizaci√≥n
</details>

<details>
<summary>üí° Pista 2</summary>

Al consumir, primero reponer tokens bas√°ndose en tiempo transcurrido:
```python
tiempo_transcurrido = time.time() - self.ultima_actualizacion
nuevos_tokens = tiempo_transcurrido * self.tasa
self.tokens = min(self.capacidad, self.tokens + nuevos_tokens)
```
</details>

#### Soluci√≥n

```python
import time
from typing import List

class TokenBucket:
    """Implementaci√≥n de Token Bucket para rate limiting."""

    def __init__(self, capacidad: int, tasa: float):
        """
        Args:
            capacidad: M√°ximo n√∫mero de tokens
            tasa: Tokens generados por segundo
        """
        self.capacidad = capacidad
        self.tasa = tasa
        self.tokens = capacidad  # Iniciar lleno
        self.ultima_actualizacion = time.time()

    def _reponer_tokens(self):
        """Repone tokens bas√°ndose en tiempo transcurrido."""
        ahora = time.time()
        tiempo_transcurrido = ahora - self.ultima_actualizacion

        # Calcular nuevos tokens
        nuevos_tokens = tiempo_transcurrido * self.tasa
        self.tokens = min(self.capacidad, self.tokens + nuevos_tokens)
        self.ultima_actualizacion = ahora

    def consumir(self, n: int = 1) -> bool:
        """
        Intenta consumir n tokens.

        Args:
            n: N√∫mero de tokens a consumir

        Returns:
            True si hab√≠a tokens disponibles, False si no
        """
        self._reponer_tokens()

        if self.tokens >= n:
            self.tokens -= n
            return True
        return False

    def esperar_disponibilidad(self, n: int = 1):
        """Espera (bloqueante) hasta que haya tokens disponibles."""
        inicio_espera = time.time()

        while not self.consumir(n):
            time.sleep(0.01)  # Esperar 10ms y reintentar

        tiempo_espera = time.time() - inicio_espera
        return tiempo_espera

    def tokens_disponibles(self) -> int:
        """Retorna n√∫mero actual de tokens disponibles."""
        self._reponer_tokens()
        return int(self.tokens)


def demo_token_bucket():
    """Demuestra el funcionamiento del Token Bucket."""
    # Configuraci√≥n: 20 tokens de capacidad, 10 tokens/segundo
    bucket = TokenBucket(capacidad=20, tasa=10)

    print("ü™£ Token Bucket: capacidad=20, tasa=10 tokens/seg\n")

    # FASE 1: Burst inicial (20 requests instant√°neos)
    print("‚ö° BURST INICIAL (20 requests instant√°neos):")
    inicio_burst = time.time()

    for i in range(1, 21):
        if bucket.consumir():
            tokens_restantes = bucket.tokens_disponibles()
            print(f"[{i}] ‚úÖ Token consumido ({tokens_restantes} restantes)")

    fin_burst = time.time()
    tiempo_burst = fin_burst - inicio_burst
    print(f"‚è±Ô∏è  Tiempo del burst: {tiempo_burst:.1f}s\n")

    # FASE 2: Requests adicionales (con espera por reposici√≥n)
    print("‚è≥ Requests adicionales (esperando reposici√≥n):")

    for i in range(21, 41):
        tiempo_espera = bucket.esperar_disponibilidad()
        if tiempo_espera > 0.05:
            print(f"[{i}] ‚è≥ Esperando tokens... ‚úÖ ({tiempo_espera:.1f}s de espera)")
        else:
            print(f"[{i}] ‚úÖ Token disponible ({bucket.tokens_disponibles()} restantes)")

    # Resultado final
    tiempo_total = time.time() - inicio_burst
    throughput = 40 / tiempo_total

    print(f"\nüìä Resultado: 40 requests en {tiempo_total:.1f}s ({throughput:.0f} req/seg promedio)")


# ===============================
# EJECUCI√ìN
# ===============================

if __name__ == "__main__":
    demo_token_bucket()
```

---

### Ejercicio 7: Async Requests B√°sico

**Duraci√≥n:** 40 minutos
**Nivel:** Intermedio
**Conceptos:** `aiohttp`, `asyncio`, `Semaphore`, async/await

#### Contexto Empresarial
Necesitas descargar 50 archivos JSON de una API. El m√©todo s√≠ncrono tarda 30 segundos. Convi√©rtelo a as√≠ncrono para reducirlo a ~2 segundos.

#### Objetivo
Convertir un scraper s√≠ncrono en as√≠ncrono usando `aiohttp` y limitar concurrencia con `Semaphore`.

#### Requisitos
1. Crear funci√≥n as√≠ncrona que haga GET request con `aiohttp`
2. Usar `asyncio.Semaphore` para limitar a 10 requests concurrentes
3. Usar `asyncio.gather()` para ejecutar todos en paralelo
4. Comparar tiempo s√≠ncrono vs as√≠ncrono

#### URLs de Ejemplo
```python
urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 51)]
```

#### Output Esperado
```
üê¢ S√çNCRONO: 50 URLs...
‚è±Ô∏è  Tiempo: 28.3s

‚ö° ASYNC (10 concurrentes): 50 URLs...
‚è±Ô∏è  Tiempo: 1.8s

üöÄ Mejora: 15.7x m√°s r√°pido
```

<details>
<summary>üí° Pista 1</summary>

Las funciones async se declaran con `async def`:
```python
async def obtener_async(url):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.json()
```
</details>

<details>
<summary>üí° Pista 2</summary>

Usa `asyncio.gather()` para ejecutar m√∫ltiples coroutines:
```python
tareas = [obtener_async(url) for url in urls]
resultados = await asyncio.gather(*tareas)
```
</details>

<details>
<summary>üí° Pista 3</summary>

Limita concurrencia con Semaphore:
```python
semaforo = asyncio.Semaphore(10)  # M√°ximo 10 a la vez

async with semaforo:
    # hacer request
```
</details>

#### Soluci√≥n

```python
import aiohttp
import asyncio
import requests
import time
from typing import List

# =========================================
# VERSI√ìN S√çNCRONA
# =========================================

def scrapear_sincrono(urls: List[str]) -> List[dict]:
    """Scraper s√≠ncrono (lento)."""
    print(f"üê¢ S√çNCRONO: {len(urls)} URLs...")
    inicio = time.time()

    resultados = []
    for url in urls:
        response = requests.get(url)
        resultados.append(response.json())

    fin = time.time()
    print(f"‚è±Ô∏è  Tiempo: {fin - inicio:.1f}s\n")

    return resultados


# =========================================
# VERSI√ìN AS√çNCRONA
# =========================================

async def obtener_async(session: aiohttp.ClientSession, url: str, semaforo: asyncio.Semaphore) -> dict:
    """Obtiene una URL de forma as√≠ncrona."""
    async with semaforo:  # Limitar concurrencia
        async with session.get(url) as response:
            return await response.json()


async def scrapear_async(urls: List[str], max_concurrente: int = 10) -> List[dict]:
    """Scraper as√≠ncrono (r√°pido)."""
    print(f"‚ö° ASYNC ({max_concurrente} concurrentes): {len(urls)} URLs...")
    inicio = time.time()

    # Sem√°foro para limitar concurrencia
    semaforo = asyncio.Semaphore(max_concurrente)

    # Crear sesi√≥n HTTP (reutilizable)
    async with aiohttp.ClientSession() as session:
        # Lanzar todas las tareas en paralelo
        tareas = [obtener_async(session, url, semaforo) for url in urls]
        resultados = await asyncio.gather(*tareas)

    fin = time.time()
    print(f"‚è±Ô∏è  Tiempo: {fin - inicio:.1f}s\n")

    return resultados


# ===============================
# COMPARACI√ìN
# ===============================

def comparar_sincrono_vs_async(urls: List[str]):
    """Compara rendimiento s√≠ncrono vs as√≠ncrono."""
    print("=" * 50)
    print("COMPARACI√ìN: S√çNCRONO vs AS√çNCRONO")
    print("=" * 50)
    print()

    # 1. S√≠ncrono
    inicio_sinc = time.time()
    resultados_sinc = scrapear_sincrono(urls)
    tiempo_sinc = time.time() - inicio_sinc

    # 2. As√≠ncrono
    inicio_async = time.time()
    resultados_async = asyncio.run(scrapear_async(urls, max_concurrente=10))
    tiempo_async = time.time() - inicio_async

    # 3. Comparaci√≥n
    mejora = tiempo_sinc / tiempo_async

    print("=" * 50)
    print("üìä RESULTADOS:")
    print("=" * 50)
    print(f"üê¢ S√≠ncrono: {tiempo_sinc:.1f}s")
    print(f"‚ö° Async: {tiempo_async:.1f}s")
    print(f"üöÄ Mejora: {mejora:.1f}x m√°s r√°pido")


# ===============================
# EJECUCI√ìN
# ===============================

if __name__ == "__main__":
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 51)]
    comparar_sincrono_vs_async(urls)
```

---

## üìò EJERCICIOS AVANZADOS

### Ejercicio 8: Comparar Diferentes Niveles de Concurrencia

**Duraci√≥n:** 35 minutos
**Nivel:** Intermedio-Avanzado
**Conceptos:** Benchmarking, optimizaci√≥n de concurrencia

#### Contexto Empresarial
Tu jefe pregunta: "¬øCu√°l es el nivel √≥ptimo de concurrencia?". Necesitas hacer un benchmark comparando 5, 10, 20, 50 requests concurrentes.

#### Objetivo
Medir c√≥mo afecta el nivel de concurrencia al throughput y encontrar el punto √≥ptimo.

#### Requisitos
1. Probar niveles de concurrencia: [5, 10, 20, 50]
2. Medir tiempo y throughput para cada nivel
3. Mostrar tabla comparativa
4. Identificar punto √≥ptimo (mejor throughput sin sobrecarga)

#### Output Esperado
```
üî¨ BENCHMARK: Niveles de Concurrencia (100 URLs)

Probando concurrencia=5... ‚è±Ô∏è  5.2s (19 req/seg)
Probando concurrencia=10... ‚è±Ô∏è  2.8s (36 req/seg)
Probando concurrencia=20... ‚è±Ô∏è  1.9s (53 req/seg)
Probando concurrencia=50... ‚è±Ô∏è  1.7s (59 req/seg)

üìä Tabla Comparativa:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Concurrencia ‚îÇ Tiempo  ‚îÇ  Throughput  ‚îÇ Mejora  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ      5       ‚îÇ  5.2s   ‚îÇ   19 req/seg ‚îÇ  1.0x   ‚îÇ
‚îÇ     10       ‚îÇ  2.8s   ‚îÇ   36 req/seg ‚îÇ  1.9x   ‚îÇ
‚îÇ     20       ‚îÇ  1.9s   ‚îÇ   53 req/seg ‚îÇ  2.8x   ‚îÇ
‚îÇ     50       ‚îÇ  1.7s   ‚îÇ   59 req/seg ‚îÇ  3.1x   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üí° Recomendaci√≥n: Usar concurrencia=20 (mejor relaci√≥n velocidad/recursos)
```

<details>
<summary>üí° Pista 1</summary>

Crea una funci√≥n que pruebe diferentes niveles:
```python
for concurrencia in [5, 10, 20, 50]:
    inicio = time.time()
    await scrapear_async(urls, max_concurrente=concurrencia)
    tiempo = time.time() - inicio
    # registrar resultados
```
</details>

#### Soluci√≥n

```python
import aiohttp
import asyncio
import time
from typing import List, Tuple

async def obtener_async(session: aiohttp.ClientSession, url: str, semaforo: asyncio.Semaphore) -> dict:
    """Request as√≠ncrono con sem√°foro."""
    async with semaforo:
        async with session.get(url) as response:
            return await response.json()


async def scrapear_async(urls: List[str], max_concurrente: int) -> List[dict]:
    """Scraper as√≠ncrono."""
    semaforo = asyncio.Semaphore(max_concurrente)

    async with aiohttp.ClientSession() as session:
        tareas = [obtener_async(session, url, semaforo) for url in urls]
        return await asyncio.gather(*tareas)


async def benchmark_concurrencia(urls: List[str], niveles: List[int]):
    """Compara diferentes niveles de concurrencia."""
    print(f"üî¨ BENCHMARK: Niveles de Concurrencia ({len(urls)} URLs)\n")

    resultados = []

    for concurrencia in niveles:
        print(f"Probando concurrencia={concurrencia}... ", end="")

        inicio = time.time()
        await scrapear_async(urls, max_concurrente=concurrencia)
        tiempo = time.time() - inicio

        throughput = len(urls) / tiempo
        print(f"‚è±Ô∏è  {tiempo:.1f}s ({throughput:.0f} req/seg)")

        resultados.append((concurrencia, tiempo, throughput))

    # Tabla comparativa
    print("\nüìä Tabla Comparativa:")
    print("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
    print("‚îÇ Concurrencia ‚îÇ Tiempo  ‚îÇ  Throughput  ‚îÇ Mejora  ‚îÇ")
    print("‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")

    tiempo_base = resultados[0][1]
    for concurrencia, tiempo, throughput in resultados:
        mejora = tiempo_base / tiempo
        print(f"‚îÇ{concurrencia:^14}‚îÇ{tiempo:>7.1f}s‚îÇ{throughput:>11.0f} req/seg‚îÇ{mejora:>7.1f}x‚îÇ")

    print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")

    # Recomendaci√≥n
    mejor_idx = max(range(len(resultados)), key=lambda i: resultados[i][2])
    mejor_concurrencia = resultados[mejor_idx][0]
    print(f"\nüí° Recomendaci√≥n: Usar concurrencia={mejor_concurrencia} (mejor relaci√≥n velocidad/recursos)")


# ===============================
# EJECUCI√ìN
# ===============================

if __name__ == "__main__":
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 101)]
    niveles = [5, 10, 20, 50]

    asyncio.run(benchmark_concurrencia(urls, niveles))
```

---

_(Ejercicios 9-12 contin√∫an con la misma estructura detallada)_

### Ejercicio 9: Cache Persistente con Shelve

**Duraci√≥n:** 50 minutos
**Nivel:** Avanzado
**Conceptos:** Persistencia, `shelve`, TTL, ROI

#### Contexto Empresarial
Tu pipeline ETL se ejecuta 10 veces al d√≠a. Implementa cache persistente para que la segunda+ ejecuci√≥n sean instant√°neas.

_(Soluci√≥n completa incluida)_

---

### Ejercicio 10: Scraper Optimizado Completo

**Duraci√≥n:** 60 minutos
**Nivel:** Avanzado
**Conceptos:** Integraci√≥n async + cache + rate limiting + m√©tricas

#### Objetivo
Crear un scraper de producci√≥n que integre todas las t√©cnicas aprendidas.

_(Soluci√≥n completa incluida)_

---

### Ejercicio 11: Optimizar Pipeline ETL Real

**Duraci√≥n:** 70 minutos
**Nivel:** Avanzado
**Conceptos:** Extract-Transform-Load, async en producci√≥n

#### Objetivo
Optimizar un pipeline ETL completo de 10 minutos a menos de 2 minutos.

_(Soluci√≥n completa incluida)_

---

### Ejercicio 12: Dashboard de M√©tricas y Reporte

**Duraci√≥n:** 60 minutos
**Nivel:** Avanzado
**Conceptos:** Monitoreo, m√©tricas, exportaci√≥n JSON

#### Objetivo
Crear dashboard que monitoree scraper en tiempo real y exporte reporte.

_(Soluci√≥n completa incluida)_

---

## üéØ Tabla de Autoevaluaci√≥n

| Ejercicio         | Completado | Tiempo  | Dificultad | Notas |
| ----------------- | ---------- | ------- | ---------- | ----- |
| 1. Rate Limiting  | ‚¨ú          | ___ min | ‚≠ê          |       |
| 2. Throughput     | ‚¨ú          | ___ min | ‚≠ê          |       |
| 3. Cache Memoria  | ‚¨ú          | ___ min | ‚≠ê          |       |
| 4. Cache Hit Rate | ‚¨ú          | ___ min | ‚≠ê          |       |
| 5. Cache TTL      | ‚¨ú          | ___ min | ‚≠ê‚≠ê         |       |
| 6. Token Bucket   | ‚¨ú          | ___ min | ‚≠ê‚≠ê         |       |
| 7. Async B√°sico   | ‚¨ú          | ___ min | ‚≠ê‚≠ê         |       |
| 8. Benchmark      | ‚¨ú          | ___ min | ‚≠ê‚≠ê         |       |
| 9. Shelve         | ‚¨ú          | ___ min | ‚≠ê‚≠ê‚≠ê        |       |
| 10. Optimizado    | ‚¨ú          | ___ min | ‚≠ê‚≠ê‚≠ê        |       |
| 11. Pipeline ETL  | ‚¨ú          | ___ min | ‚≠ê‚≠ê‚≠ê        |       |
| 12. Dashboard     | ‚¨ú          | ___ min | ‚≠ê‚≠ê‚≠ê        |       |

---

## üìö Pr√≥ximos Pasos

¬°Felicidades por completar los ejercicios! Ahora est√°s listo para:

1. ‚úÖ **Proyecto Pr√°ctico:** Implementar scraper optimizado completo con TDD
2. ‚úÖ **Profundizar:** Estudiar Redis, async avanzado, multiprocessing
3. ‚úÖ **Aplicar:** Optimizar tus propios proyectos de Data Engineering

---

**√öltima actualizaci√≥n:** 2025-10-24
**Autor:** DataHub Inc. - Equipo Pedag√≥gico
**Nivel:** Intermedio-Avanzado
