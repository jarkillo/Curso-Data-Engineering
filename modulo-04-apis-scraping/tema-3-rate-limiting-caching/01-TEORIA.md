# üìò Tema 3: Rate Limiting y Caching - Optimizaci√≥n de Extracci√≥n de Datos

**M√≥dulo 4:** APIs y Web Scraping
**Duraci√≥n estimada:** 20-30 minutos de lectura
**Nivel:** Intermedio-Avanzado
**Prerequisitos:** Tema 1 (APIs REST), Tema 2 (Web Scraping)

---

## üéØ Introducci√≥n

Imagina que has construido un scraper que extrae informaci√≥n de 10,000 productos de un e-commerce. Con los conocimientos de los temas anteriores, podr√≠as tardar **3-4 horas** en completar la tarea. ¬øY si pudieras reducir ese tiempo a **menos de 10 minutos** sin violar ninguna regla √©tica?

Bienvenido al mundo de la **optimizaci√≥n de extracci√≥n de datos**.

### ¬øPor qu√© optimizar?

En Data Engineering, frecuentemente necesitas:

- **Extraer millones de registros** de APIs REST
- **Actualizar datos cada hora** desde m√∫ltiples fuentes
- **Scrapear cat√°logos completos** de sitios web
- **Procesar datos en tiempo real** sin sobrecargar servidores

Sin optimizaci√≥n, estos procesos pueden ser:
- ‚ùå **Lentos** (d√≠as en completar)
- ‚ùå **Costosos** (miles de requests innecesarias)
- ‚ùå **Bloqueados** (exceder rate limits)
- ‚ùå **Poco √©ticos** (sobrecargar servidores)

Con optimizaci√≥n, puedes lograr:
- ‚úÖ **10-100x m√°s r√°pido** (con async)
- ‚úÖ **90% menos requests** (con caching)
- ‚úÖ **0 bloqueos** (respetando rate limits)
- ‚úÖ **√âtico y sostenible**

---

## üìö Conceptos Fundamentales

### 1. Rate Limiting - Controlar la Velocidad de Requests

> **Analog√≠a:** Es como el l√≠mite de velocidad en una carretera. Puedes conducir, pero no a cualquier velocidad. El objetivo es mantener el tr√°fico fluido y seguro para todos.

#### ¬øQu√© es Rate Limiting?

**Rate limiting** es la pr√°ctica de limitar el n√∫mero de requests HTTP que puedes hacer en un per√≠odo de tiempo determinado. Puede ser:

**Impuesto por el servidor:**
- "M√°ximo 100 requests por minuto"
- Si excedes el l√≠mite ‚Üí recibes **429 Too Many Requests**
- Ejemplo: Twitter API permite 900 requests cada 15 minutos

**Auto-impuesto (cortes√≠a):**
- T√∫ decides limitar tu velocidad para no sobrecargar el servidor
- Ejemplo: Esperar 2 segundos entre cada request de scraping
- **Es lo √©tico cuando scrapeas sitios sin API**

#### ¬øPor qu√© existe?

**Desde el punto de vista del servidor:**
1. **Prevenir abuso:** Evitar que un solo cliente monopolice recursos
2. **Controlar costos:** Limitar uso de ancho de banda y procesamiento
3. **Seguridad:** Detectar y mitigar ataques DDoS
4. **Monetizaci√≥n:** APIs gratuitas con l√≠mites, premium sin l√≠mites

**Desde tu punto de vista (Data Engineer):**
1. **No ser bloqueado:** Evitar el error 429 y bloqueos de IP
2. **Cortes√≠a:** No sobrecargar servidores de terceros
3. **√âtica:** Scrapear responsablemente
4. **Sostenibilidad:** Mantener acceso a largo plazo

#### Estrategias de Rate Limiting

##### 1.1. Fixed Window (Ventana Fija)

**Concepto:** L√≠mite fijo de requests por intervalo de tiempo.

```
Ejemplo: 100 requests por minuto
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Minuto 1   ‚îÇ ‚îÇ  Minuto 2   ‚îÇ ‚îÇ  Minuto 3   ‚îÇ
‚îÇ  100 reqs   ‚îÇ ‚îÇ  100 reqs   ‚îÇ ‚îÇ  100 reqs   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚úÖ             ‚úÖ             ‚úÖ
```

**Ventajas:**
- ‚úÖ Simple de implementar
- ‚úÖ F√°cil de entender

**Desventajas:**
- ‚ùå Puede permitir "burst" de requests al inicio del per√≠odo
- ‚ùå Ejemplo: 100 requests a las 12:00:59, otros 100 a las 12:01:00 (200 requests en 2 segundos)

**Caso de uso:** APIs que usan este m√©todo (GitHub, Stripe)

##### 1.2. Sliding Window (Ventana Deslizante)

**Concepto:** L√≠mite de requests en los **√∫ltimos** N segundos.

```
L√≠mite: 100 requests en 60 segundos

Tiempo actual: 12:00:30
Cuenta requests desde: 12:00:30 - 60s = 11:59:30

Tiempo actual: 12:00:35
Cuenta requests desde: 12:00:35 - 60s = 11:59:35
```

**Ventajas:**
- ‚úÖ M√°s justo, evita bursts
- ‚úÖ Distribuci√≥n m√°s uniforme

**Desventajas:**
- ‚ùå M√°s complejo de implementar
- ‚ùå Requiere rastrear timestamp de cada request

**Caso de uso:** APIs m√°s sofisticadas (AWS)

##### 1.3. Token Bucket (Cubeta de Tokens)

**Concepto:** Tienes una "cubeta" con tokens. Cada request consume 1 token. La cubeta se rellena a una velocidad constante.

```
Capacidad de cubeta: 100 tokens
Velocidad de relleno: 10 tokens/segundo

Inicio:     [‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ... 100 tokens]
Request 1:  [‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ... 99 tokens]  ‚úÖ
Request 2:  [‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ... 98 tokens]  ‚úÖ
...
Request 100:[‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ... 0 tokens]   ‚úÖ
Request 101:[]                          ‚ùå Esperar 0.1 seg
+0.1 seg:   [‚óè]                         (1 token generado)
Request 101:[‚óè]                         ‚úÖ
```

**Ventajas:**
- ‚úÖ Permite bursts controlados (√∫til para cargas variables)
- ‚úÖ Flexible y adaptable
- ‚úÖ Usado por sistemas distribuidos (Redis, AWS Lambda)

**Desventajas:**
- ‚ùå M√°s complejo de implementar

**Caso de uso:** Sistemas de alto rendimiento, microservicios

#### Implementaci√≥n B√°sica en Python

**Fixed Window simple con `time.sleep()`:**

```python
import time

def rate_limited_scraper(urls: list, delay_seconds: float = 1.0):
    """Scraper con rate limiting b√°sico."""
    resultados = []

    for url in urls:
        # Hacer request
        data = hacer_request(url)
        resultados.append(data)

        # Esperar delay antes del siguiente request
        time.sleep(delay_seconds)

    return resultados

# Uso: 100 URLs con 2 segundos entre cada una
# Tiempo total: 100 * 2 = 200 segundos = 3.3 minutos
resultados = rate_limited_scraper(urls, delay_seconds=2.0)
```

**Token Bucket avanzado:**

```python
import time

class TokenBucket:
    def __init__(self, capacidad: int, tasa_reposicion: float):
        """
        capacidad: M√°ximo n√∫mero de tokens
        tasa_reposicion: Tokens generados por segundo
        """
        self.capacidad = capacidad
        self.tasa = tasa_reposicion
        self.tokens = capacidad
        self.ultima_actualizacion = time.time()

    def consumir(self, tokens: int = 1) -> bool:
        """Intenta consumir tokens. Retorna True si hay disponibles."""
        ahora = time.time()

        # Generar nuevos tokens seg√∫n tiempo transcurrido
        tiempo_transcurrido = ahora - self.ultima_actualizacion
        nuevos_tokens = tiempo_transcurrido * self.tasa
        self.tokens = min(self.capacidad, self.tokens + nuevos_tokens)
        self.ultima_actualizacion = ahora

        # Consumir tokens si hay disponibles
        if self.tokens >= tokens:
            self.tokens -= tokens
            return True
        return False

    def esperar_disponibilidad(self, tokens: int = 1):
        """Espera hasta que haya tokens disponibles."""
        while not self.consumir(tokens):
            time.sleep(0.1)

# Uso: 100 requests/minuto
bucket = TokenBucket(capacidad=100, tasa_reposicion=100/60)  # 1.67 tokens/seg

for url in urls:
    bucket.esperar_disponibilidad()
    hacer_request(url)
```

---

### 2. Caching - Almacenar Respuestas para Reutilizar

> **Analog√≠a:** Es como tomar apuntes en clase. La primera vez, escuchas la explicaci√≥n (request HTTP lento). Las siguientes veces, solo consultas tus apuntes (cache r√°pido). No necesitas volver a preguntar al profesor cada vez.

#### ¬øQu√© es Caching?

**Caching** es la pr√°ctica de **almacenar respuestas de requests HTTP** para reutilizarlas sin hacer el request nuevamente.

**Escenario sin cache:**
```
Request 1 ‚Üí API ‚Üí Response (500ms)
Request 2 ‚Üí API ‚Üí Response (500ms)  [MISMO dato que Request 1]
Request 3 ‚Üí API ‚Üí Response (500ms)  [MISMO dato que Request 1]

Tiempo total: 1,500ms
Requests realizadas: 3
```

**Escenario con cache:**
```
Request 1 ‚Üí API ‚Üí Response (500ms) ‚Üí [Guardar en cache]
Request 2 ‚Üí Cache ‚Üí Response (1ms)   [Leer de cache]
Request 3 ‚Üí Cache ‚Üí Response (1ms)   [Leer de cache]

Tiempo total: 502ms (3x m√°s r√°pido)
Requests realizadas: 1 (66% menos tr√°fico)
```

#### ¬øCu√°ndo usar Cache?

**‚úÖ Casos ideales para caching:**

1. **Datos que cambian poco**
   - Cat√°logo de productos (actualiza 1 vez al d√≠a)
   - Datos hist√≥ricos (no cambian nunca)
   - Metadatos de configuraci√≥n

2. **Requests costosos**
   - APIs de pago ($0.01 por request)
   - APIs lentas (>5 segundos por response)
   - Scraping de sitios lentos

3. **Requests repetidos**
   - M√∫ltiples an√°lisis sobre los mismos datos
   - Re-ejecuciones de scripts durante desarrollo
   - Pipelines que se ejecutan frecuentemente

**‚ùå Casos donde NO usar cache:**

1. **Datos en tiempo real**
   - Cotizaciones de bolsa (cambian cada segundo)
   - Clima actual (cambia cada minuto)
   - Estado de pedidos (cambia constantemente)

2. **Datos personalizados**
   - Dependen del usuario autenticado
   - Dependen de par√°metros √∫nicos

3. **Requests √∫nicos**
   - Solo se hace una vez (no vale la pena el overhead)

#### Tipos de Cache

##### 2.1. Cache en Memoria (RAM)

**Almacenamiento:** Diccionario Python en memoria

**Ventajas:**
- ‚úÖ **Ultra r√°pido** (acceso en nanosegundos)
- ‚úÖ **Simple de implementar** (solo un `dict`)

**Desventajas:**
- ‚ùå **Se pierde al reiniciar** el programa
- ‚ùå **Limitado por RAM** disponible

**Caso de uso:** Cache temporal durante ejecuci√≥n de un script

**Implementaci√≥n:**

```python
# Cache simple con diccionario
cache_memoria = {}

def obtener_con_cache(url: str) -> dict:
    """Obtiene datos de API con cache en memoria."""
    if url in cache_memoria:
        print(f"‚úÖ Cache HIT: {url}")
        return cache_memoria[url]

    print(f"üåê Cache MISS: {url} - Haciendo request...")
    response = requests.get(url)
    data = response.json()

    cache_memoria[url] = data
    return data

# Uso
data1 = obtener_con_cache("https://api.example.com/users/1")  # Request real
data2 = obtener_con_cache("https://api.example.com/users/1")  # Desde cache (instant√°neo)
```

##### 2.2. Cache en Disco (Persistente)

**Almacenamiento:** Archivos en disco (JSON, pickle, shelve, SQLite)

**Ventajas:**
- ‚úÖ **Persiste entre ejecuciones** del programa
- ‚úÖ **No limitado por RAM** (usa disco)
- ‚úÖ **Compartible** entre procesos

**Desventajas:**
- ‚ùå **M√°s lento que memoria** (acceso en milisegundos)
- ‚ùå **Requiere gesti√≥n** de archivos

**Caso de uso:** Cache para datos de APIs costosas que cambian lentamente

**Implementaci√≥n con `shelve`:**

```python
import shelve

def obtener_con_cache_disco(url: str) -> dict:
    """Obtiene datos con cache persistente en disco."""
    with shelve.open("cache_api.db") as cache:
        if url in cache:
            print(f"‚úÖ Cache HIT: {url}")
            return cache[url]

        print(f"üåê Cache MISS: {url} - Haciendo request...")
        response = requests.get(url)
        data = response.json()

        cache[url] = data
        return data

# Uso
data = obtener_con_cache_disco("https://api.example.com/products")
# Segunda ejecuci√≥n (incluso despu√©s de reiniciar Python) ‚Üí desde cache
```

##### 2.3. Cache Distribuido (Redis, Memcached)

**Almacenamiento:** Servidor de cache dedicado (Redis)

**Ventajas:**
- ‚úÖ **Ultra r√°pido** (Redis en RAM)
- ‚úÖ **Compartido** entre m√∫ltiples aplicaciones
- ‚úÖ **TTL autom√°tico** (expiraci√≥n)
- ‚úÖ **Alta disponibilidad**

**Desventajas:**
- ‚ùå **Requiere servidor Redis** corriendo
- ‚ùå **M√°s complejo** de configurar

**Caso de uso:** Sistemas distribuidos, microservicios, producci√≥n

**Implementaci√≥n con Redis:**

```python
import redis
import json

# Conectar a Redis
r = redis.Redis(host='localhost', port=6379, decode_responses=True)

def obtener_con_redis(url: str, ttl: int = 3600) -> dict:
    """
    Obtiene datos con cache en Redis.

    Args:
        url: URL de la API
        ttl: Tiempo de vida en segundos (3600 = 1 hora)
    """
    # Intentar obtener de Redis
    cached = r.get(url)
    if cached:
        print(f"‚úÖ Cache HIT: {url}")
        return json.loads(cached)

    # Request real si no est√° en cache
    print(f"üåê Cache MISS: {url}")
    response = requests.get(url)
    data = response.json()

    # Guardar en Redis con TTL
    r.setex(url, ttl, json.dumps(data))
    return data

# Uso
data = obtener_con_redis("https://api.example.com/data", ttl=7200)  # Cache 2 horas
```

#### TTL (Time To Live) - Tiempo de Expiraci√≥n

**Concepto:** El cache no debe durar para siempre. Despu√©s de cierto tiempo, los datos se consideran "obsoletos" y se hace un request fresco.

```python
import time

cache_con_ttl = {}

def obtener_con_ttl(url: str, ttl: int = 60) -> dict:
    """
    Cache con expiraci√≥n autom√°tica.

    Args:
        url: URL de la API
        ttl: Tiempo de vida en segundos
    """
    ahora = time.time()

    # Verificar si existe y no ha expirado
    if url in cache_con_ttl:
        datos, timestamp = cache_con_ttl[url]
        if ahora - timestamp < ttl:
            print(f"‚úÖ Cache v√°lido (edad: {int(ahora - timestamp)}s)")
            return datos
        else:
            print(f"‚è∞ Cache expirado (edad: {int(ahora - timestamp)}s > {ttl}s)")

    # Request fresco
    response = requests.get(url)
    data = response.json()
    cache_con_ttl[url] = (data, ahora)
    return data

# Uso
data1 = obtener_con_ttl("https://api.example.com/weather", ttl=300)  # Cache 5 min
time.sleep(60)
data2 = obtener_con_ttl("https://api.example.com/weather", ttl=300)  # A√∫n v√°lido
time.sleep(300)
data3 = obtener_con_ttl("https://api.example.com/weather", ttl=300)  # Expir√≥, request nuevo
```

**Gu√≠a de TTL seg√∫n tipo de dato:**

| Tipo de Dato        | TTL Recomendado  | Ejemplo                       |
| ------------------- | ---------------- | ----------------------------- |
| **Est√°tico**        | 1 semana - 1 mes | Pa√≠ses, c√≥digos postales      |
| **Semi-est√°tico**   | 1 d√≠a - 1 semana | Cat√°logo de productos         |
| **Din√°mico**        | 1 hora - 1 d√≠a   | Precios, inventario           |
| **Tiempo real**     | 1-5 minutos      | Clima, tr√°fico                |
| **Ultra real-time** | No cachear       | Cotizaciones de bolsa en vivo |

---

### 3. Async Requests - Concurrencia para M√°xima Velocidad

> **Analog√≠a:** Imagina que necesitas lavar 10 platos. M√©todo **s√≠ncrono**: lavas uno, esperas que se seque, lavas el siguiente (10 minutos total). M√©todo **as√≠ncrono**: pones todos a lavar a la vez en el lavavajillas (2 minutos total). Mucho m√°s eficiente cuando las tareas esperan (I/O).

#### ¬øQu√© es Async/Await?

**Async programming** permite hacer **m√∫ltiples requests HTTP en paralelo** sin bloquear el programa mientras espera respuestas.

**Problema con requests s√≠ncronos:**

```python
import requests
import time

# S√≠ncrono: Hace 1 request, espera, hace el siguiente
urls = ["https://api.example.com/1", "https://api.example.com/2", ...]  # 100 URLs

inicio = time.time()
resultados = []
for url in urls:
    response = requests.get(url)  # Espera 0.5 seg por cada uno
    resultados.append(response.json())

fin = time.time()
print(f"Tiempo: {fin - inicio}s")  # ~50 segundos (100 * 0.5s)
```

**Soluci√≥n con async:**

```python
import aiohttp
import asyncio
import time

async def obtener_async(session, url):
    """Hace un request as√≠ncrono."""
    async with session.get(url) as response:
        return await response.json()

async def obtener_todos(urls):
    """Obtiene todas las URLs en paralelo."""
    async with aiohttp.ClientSession() as session:
        # Lanza todos los requests EN PARALELO
        tareas = [obtener_async(session, url) for url in urls]
        return await asyncio.gather(*tareas)

# Uso
urls = ["https://api.example.com/1", ...] * 100  # 100 URLs

inicio = time.time()
resultados = asyncio.run(obtener_todos(urls))
fin = time.time()

print(f"Tiempo: {fin - inicio}s")  # ~2-3 segundos (¬°20x m√°s r√°pido!)
```

#### ¬øCu√°ndo usar Async?

**‚úÖ Casos ideales:**

1. **Muchos requests a la vez** (>10)
2. **I/O bound** (mayor parte del tiempo esperando respuestas de red)
3. **APIs r√°pidas** (responden en <1 segundo)
4. **Scraping masivo** con rate limiting (ej: 100 requests/seg)

**‚ùå Cuando NO usarlo:**

1. **Pocos requests** (<5) - overhead no vale la pena
2. **CPU bound** (procesamiento intensivo local)
3. **Rate limiting estricto** (1 request/seg) - no hay beneficio de paralelismo

#### Combinando Async + Rate Limiting + Cache

**El combo perfecto para scraping masivo:**

```python
import aiohttp
import asyncio
from typing import List, Dict
import time

class ScraperOptimizado:
    def __init__(self, max_concurrente: int = 10):
        self.cache = {}
        self.max_concurrente = max_concurrente
        self.semaforo = asyncio.Semaphore(max_concurrente)

    async def obtener_con_cache(self, session, url: str) -> dict:
        """Request as√≠ncrono con cache y rate limiting."""
        # 1. Verificar cache
        if url in self.cache:
            print(f"‚úÖ Cache HIT: {url}")
            return self.cache[url]

        # 2. Rate limiting con sem√°foro (m√°ximo N requests concurrentes)
        async with self.semaforo:
            print(f"üåê Request: {url}")
            async with session.get(url) as response:
                data = await response.json()

                # 3. Guardar en cache
                self.cache[url] = data

                # 4. Cortes√≠a: peque√±o delay
                await asyncio.sleep(0.1)

                return data

    async def scrapear_masivo(self, urls: List[str]) -> List[dict]:
        """Scrape masivo optimizado."""
        async with aiohttp.ClientSession() as session:
            tareas = [self.obtener_con_cache(session, url) for url in urls]
            return await asyncio.gather(*tareas)

# Uso
scraper = ScraperOptimizado(max_concurrente=20)
urls = [f"https://api.example.com/products/{i}" for i in range(1000)]

inicio = time.time()
resultados = asyncio.run(scraper.scrapear_masivo(urls))
fin = time.time()

print(f"üéâ {len(resultados)} productos en {fin-inicio:.1f}s")
print(f"‚ö° Velocidad: {len(resultados)/(fin-inicio):.1f} productos/seg")
```

---

### 4. Monitoreo y M√©tricas

> **Analog√≠a:** Es como el veloc√≠metro de un auto. Sin √©l, no sabes si vas a 50 km/h o 150 km/h. Con m√©tricas, puedes **medir, optimizar y mejorar**.

#### M√©tricas Clave

**1. Throughput (Rendimiento)**
- **Qu√© mide:** Requests por segundo (req/seg)
- **F√≥rmula:** `throughput = total_requests / tiempo_total`
- **Objetivo:** Maximizar (m√°s req/seg = m√°s r√°pido)

```python
inicio = time.time()
hacer_1000_requests()
fin = time.time()

throughput = 1000 / (fin - inicio)
print(f"Throughput: {throughput:.1f} req/seg")
```

**2. Latencia (Latency)**
- **Qu√© mide:** Tiempo promedio por request (milisegundos)
- **F√≥rmula:** `latencia = tiempo_total / total_requests`
- **Objetivo:** Minimizar (menos ms = m√°s r√°pido)

```python
latencias = []
for url in urls:
    inicio = time.time()
    hacer_request(url)
    fin = time.time()
    latencias.append((fin - inicio) * 1000)  # ms

print(f"Latencia promedio: {sum(latencias)/len(latencias):.0f}ms")
```

**3. Cache Hit Rate (Tasa de Aciertos)**
- **Qu√© mide:** % de requests servidos desde cache
- **F√≥rmula:** `hit_rate = cache_hits / total_requests * 100`
- **Objetivo:** Maximizar (90%+ es excelente)

```python
cache_hits = 0
cache_misses = 0

# ... despu√©s de N requests ...

hit_rate = cache_hits / (cache_hits + cache_misses) * 100
print(f"Cache Hit Rate: {hit_rate:.1f}%")
print(f"Requests ahorrados: {cache_hits}")
```

**4. Error Rate (Tasa de Errores)**
- **Qu√© mide:** % de requests que fallaron
- **F√≥rmula:** `error_rate = errores / total_requests * 100`
- **Objetivo:** Minimizar (<1% es bueno, <0.1% es excelente)

```python
errores = 0
exitos = 0

# ... despu√©s de N requests ...

error_rate = errores / (errores + exitos) * 100
print(f"Error Rate: {error_rate:.2f}%")
```

#### Dashboard de M√©tricas en Vivo

```python
import time
from dataclasses import dataclass
from typing import Optional

@dataclass
class Metricas:
    """Contenedor de m√©tricas del scraper."""
    total_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    errores: int = 0
    inicio: float = 0.0

    def registrar_cache_hit(self):
        self.total_requests += 1
        self.cache_hits += 1

    def registrar_cache_miss(self):
        self.total_requests += 1
        self.cache_misses += 1

    def registrar_error(self):
        self.errores += 1

    def reporte(self) -> str:
        """Genera reporte de m√©tricas."""
        tiempo = time.time() - self.inicio
        throughput = self.total_requests / tiempo if tiempo > 0 else 0
        hit_rate = self.cache_hits / self.total_requests * 100 if self.total_requests > 0 else 0
        error_rate = self.errores / self.total_requests * 100 if self.total_requests > 0 else 0

        return f"""
        üìä M√©tricas del Scraper
        ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
        ‚è±Ô∏è  Tiempo: {tiempo:.1f}s
        üìù Requests: {self.total_requests}
        ‚ö° Throughput: {throughput:.1f} req/seg
        ‚úÖ Cache Hits: {self.cache_hits} ({hit_rate:.1f}%)
        üåê Cache Misses: {self.cache_misses}
        ‚ùå Errores: {self.errores} ({error_rate:.2f}%)
        """

# Uso
metricas = Metricas(inicio=time.time())

for url in urls:
    if url in cache:
        metricas.registrar_cache_hit()
    else:
        try:
            hacer_request(url)
            metricas.registrar_cache_miss()
        except:
            metricas.registrar_error()

print(metricas.reporte())
```

---

## ‚öñÔ∏è Comparaci√≥n: Antes vs Despu√©s de Optimizar

### Caso de Estudio: Scraping de 1,000 Productos

**Escenario:** Necesitas extraer datos de 1,000 productos de una API. Cada request tarda ~500ms.

| T√©cnica                        | Requests Reales | Tiempo        | Throughput  | Mejora               |
| ------------------------------ | --------------- | ------------- | ----------- | -------------------- |
| **S√≠ncrono sin cache**         | 1,000           | 500s (~8 min) | 2 req/seg   | Baseline             |
| **+ Cache (50% hit rate)**     | 500             | 250s (~4 min) | 4 req/seg   | 2x m√°s r√°pido        |
| **+ Rate limiting (cortes√≠a)** | 500             | 300s (~5 min) | 3.3 req/seg | √âtico ‚úÖ              |
| **+ Async (20 concurrentes)**  | 500             | 15s           | 66 req/seg  | **33x m√°s r√°pido** üöÄ |

**Resultado final:** De **8 minutos** a **15 segundos** (33x mejora) siendo **√©tico** y **respetando rate limits**.

---

## üéØ Aplicaciones en Data Engineering

### 1. ETL Pipelines Optimizados

```python
# Pipeline ETL con optimizaci√≥n completa
async def pipeline_etl_optimizado():
    """Extrae, transforma y carga datos optimizadamente."""

    # EXTRACT (async + cache)
    urls = generar_urls(n=10000)
    scraper = ScraperOptimizado(max_concurrente=50)
    datos_raw = await scraper.scrapear_masivo(urls)

    # TRANSFORM (procesamiento local)
    datos_limpios = [limpiar(d) for d in datos_raw]

    # LOAD (batch insert)
    guardar_en_bd(datos_limpios, batch_size=1000)

    print("‚úÖ ETL completado en tiempo r√©cord")
```

### 2. Actualizaci√≥n Incremental

```python
# Solo actualizar datos que cambiaron (con cache inteligente)
def actualizar_incremental():
    """Actualiza solo datos modificados desde √∫ltima ejecuci√≥n."""
    ultima_ejecucion = obtener_timestamp_ultima_ejecucion()

    # Request solo endpoints que pueden haber cambiado
    urls_a_actualizar = []
    for producto in productos_en_bd:
        if producto.ultima_actualizacion < ultima_ejecucion:
            urls_a_actualizar.append(producto.url_api)

    # El resto viene de cache
    datos_actualizados = obtener_con_cache(urls_a_actualizar)
    actualizar_bd(datos_actualizados)
```

### 3. Monitoreo en Tiempo Real

```python
# Dashboard de m√©tricas para monitorear scrapers de producci√≥n
def monitorear_scraper_produccion():
    """Env√≠a m√©tricas a sistema de monitoreo (ej: Grafana)."""
    metricas = obtener_metricas_actuales()

    enviar_metrica("scraper.throughput", metricas.throughput)
    enviar_metrica("scraper.cache_hit_rate", metricas.hit_rate)
    enviar_metrica("scraper.error_rate", metricas.error_rate)

    # Alertas
    if metricas.error_rate > 5.0:
        enviar_alerta("‚ö†Ô∏è Error rate alto en scraper")
```

---

## üö® Errores Comunes y C√≥mo Evitarlos

### Error 1: "Async sin Rate Limiting = Ban Inmediato"

‚ùå **Mal:**
```python
# Lanza 1,000 requests en 1 segundo ‚Üí Ban instant√°neo
async def scrapear_todo_rapido(urls):
    async with aiohttp.ClientSession() as session:
        tareas = [session.get(url) for url in urls]  # ¬°1,000 requests paralelos!
        return await asyncio.gather(*tareas)
```

‚úÖ **Bien:**
```python
# Limita a 20 requests concurrentes
async def scrapear_responsable(urls):
    semaforo = asyncio.Semaphore(20)  # M√°ximo 20 a la vez

    async def obtener_limitado(session, url):
        async with semaforo:
            await asyncio.sleep(0.1)  # 100ms de delay
            async with session.get(url) as response:
                return await response.json()

    async with aiohttp.ClientSession() as session:
        tareas = [obtener_limitado(session, url) for url in urls]
        return await asyncio.gather(*tareas)
```

### Error 2: "Cache sin Expiraci√≥n = Datos Obsoletos"

‚ùå **Mal:**
```python
cache = {}
# Cache dura para siempre ‚Üí datos obsoletos despu√©s de semanas
```

‚úÖ **Bien:**
```python
# Cache con TTL de 24 horas
cache = {}
def obtener_con_ttl(url, ttl=86400):  # 86400 seg = 24 horas
    if url in cache:
        datos, timestamp = cache[url]
        if time.time() - timestamp < ttl:
            return datos

    datos = requests.get(url).json()
    cache[url] = (datos, time.time())
    return datos
```

### Error 3: "No Medir = No Saber Si Est√°s Optimizando"

‚ùå **Mal:**
```python
# No sabes si el c√≥digo es lento o r√°pido
scrapear_datos(urls)
```

‚úÖ **Bien:**
```python
inicio = time.time()
resultados = scrapear_datos(urls)
fin = time.time()

print(f"‚è±Ô∏è  Tiempo: {fin-inicio:.1f}s")
print(f"‚ö° Throughput: {len(urls)/(fin-inicio):.1f} req/seg")
```

### Error 4: "Rate Limiting Demasiado Agresivo"

‚ùå **Mal:**
```python
# 10 segundos entre cada request = 10 horas para 3,600 productos
for url in urls:
    hacer_request(url)
    time.sleep(10)  # Demasiado lento
```

‚úÖ **Bien:**
```python
# Usar async + sem√°foro para ser r√°pido Y respetuoso
# 20 requests concurrentes + 0.1s delay = r√°pido pero cort√©s
```

### Error 5: "Cache en Memoria sin L√≠mite = Memory Leak"

‚ùå **Mal:**
```python
cache = {}  # Crece infinitamente hasta agotar RAM
for i in range(1000000):
    cache[f"url_{i}"] = "datos..."  # ¬°Puede consumir 10+ GB RAM!
```

‚úÖ **Bien:**
```python
from functools import lru_cache

@lru_cache(maxsize=1000)  # M√°ximo 1,000 entradas
def obtener_dato(url):
    return requests.get(url).json()

# O usar cache en disco con shelve/SQLite
```

---

## ‚úÖ Buenas Pr√°cticas

### 1. Principio del "Buen Vecino"

Cuando scrapeas sitios web (no APIs oficiales):
- ‚úÖ Limita a **1-2 requests por segundo** m√°ximo
- ‚úÖ Identif√≠cate con **User-Agent descriptivo**
- ‚úÖ Respeta **robots.txt** siempre
- ‚úÖ Scrape en **horarios de baja carga** (madrugada)
- ‚úÖ Implementa **exponential backoff** ante errores

### 2. Estrategia de Cache Inteligente

```python
# TTL seg√∫n volatilidad del dato
TTL_CONFIG = {
    "productos": 86400,      # 24 horas
    "precios": 3600,         # 1 hora
    "inventario": 300,       # 5 minutos
    "paises": 2592000,       # 30 d√≠as (datos est√°ticos)
}

def obtener_con_cache_inteligente(endpoint: str, recurso_id: int):
    url = f"https://api.example.com/{endpoint}/{recurso_id}"
    ttl = TTL_CONFIG.get(endpoint, 3600)  # Default 1 hora
    return obtener_con_ttl(url, ttl=ttl)
```

### 3. Logging Completo de Optimizaciones

```python
import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def scrapear_con_logs(urls):
    logger.info(f"üöÄ Iniciando scraping de {len(urls)} URLs")

    inicio = time.time()
    metricas = Metricas(inicio=inicio)

    for url in urls:
        if url in cache:
            logger.debug(f"‚úÖ Cache HIT: {url}")
            metricas.registrar_cache_hit()
        else:
            logger.debug(f"üåê Request: {url}")
            metricas.registrar_cache_miss()

    fin = time.time()
    logger.info(f"‚úÖ Completado en {fin-inicio:.1f}s")
    logger.info(metricas.reporte())
```

### 4. Fail-Safe: Degradaci√≥n Elegante

```python
async def scrapear_con_fallback(urls):
    """Si async falla, fallback a s√≠ncrono."""
    try:
        # Intentar async (m√°s r√°pido)
        return await scrapear_async(urls)
    except Exception as e:
        logger.warning(f"Async fall√≥: {e}. Fallback a s√≠ncrono.")
        # Fallback a s√≠ncrono (m√°s lento pero funciona)
        return scrapear_sincrono(urls)
```

---

## üìö Checklist de Aprendizaje

Despu√©s de completar este tema, deber√≠as poder:

### Nivel B√°sico ‚úÖ
- [ ] Explicar qu√© es rate limiting y por qu√© existe
- [ ] Implementar rate limiting b√°sico con `time.sleep()`
- [ ] Crear un cache en memoria con un `dict`
- [ ] Calcular throughput (requests/segundo)
- [ ] Medir tiempo de ejecuci√≥n de un scraper

### Nivel Intermedio ‚úÖ
- [ ] Implementar token bucket rate limiting
- [ ] Crear cache con TTL (expiraci√≥n autom√°tica)
- [ ] Usar `shelve` para cache persistente
- [ ] Medir cache hit rate
- [ ] Comparar rendimiento s√≠ncrono vs as√≠ncrono

### Nivel Avanzado ‚úÖ
- [ ] Implementar scraper as√≠ncrono con `aiohttp` y `asyncio`
- [ ] Combinar async + rate limiting + cache
- [ ] Integrar Redis para cache distribuido
- [ ] Crear dashboard de m√©tricas en vivo
- [ ] Optimizar pipeline ETL completo (Extract ‚Üí Transform ‚Üí Load)
- [ ] Calcular ROI de optimizaci√≥n (tiempo/costo ahorrado)

---

## üéØ Conclusi√≥n

La optimizaci√≥n de extracci√≥n de datos es una **habilidad cr√≠tica** en Data Engineering. Con las t√©cnicas de este tema, puedes:

‚úÖ **Acelerar pipelines 10-100x** (de horas a minutos)
‚úÖ **Reducir costos 90%** (menos requests = menos $$$)
‚úÖ **Ser √©tico y sostenible** (respetar rate limits)
‚úÖ **Escalar a millones de requests** (con async y cache)
‚úÖ **Monitorear y mejorar continuamente** (con m√©tricas)

**Recuerda:**
> "La optimizaci√≥n prematura es la ra√≠z de todo mal" - Donald Knuth

Pero tambi√©n:
> "La optimizaci√≥n informada con m√©tricas es la ra√≠z de todo bien" - T√∫, ahora üòâ

**Pr√≥ximos pasos:**
1. Practica con los **ejemplos** del siguiente archivo
2. Resuelve los **12 ejercicios** progresivos
3. Implementa el **proyecto pr√°ctico** (scraper optimizado TDD)

---

**Palabras clave:** Rate limiting, token bucket, fixed window, sliding window, caching, TTL, cache invalidation, async, await, aiohttp, asyncio, concurrencia, throughput, latencia, m√©tricas, optimizaci√≥n, I/O bound

**√öltima actualizaci√≥n:** 2025-10-24
**Tiempo de lectura:** 25-30 minutos
**Nivel:** Intermedio-Avanzado
