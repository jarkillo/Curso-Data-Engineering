# ğŸ“ Ejemplos Trabajados - Tema 3: Rate Limiting y Caching

**MÃ³dulo 4:** APIs y Web Scraping
**DuraciÃ³n estimada:** 45-60 minutos
**Prerequisitos:** Haber leÃ­do 01-TEORIA.md del Tema 3

---

## ğŸ¯ IntroducciÃ³n

En este archivo encontrarÃ¡s **4 ejemplos completos y ejecutables** que demuestran cÃ³mo optimizar scrapers y clientes de APIs mediante:

1. **Rate limiting bÃ¡sico** (cortesÃ­a y evitar bans)
2. **Cache persistente** (ahorrar requests costosos)
3. **Async requests** (velocidad 20x)
4. **Scraper optimizado completo** (combo de todas las tÃ©cnicas)

Todos los ejemplos son **cÃ³digo real** que puedes copiar, pegar y ejecutar directamente en Python 3.8+.

---

## ğŸ“¦ Dependencias

Antes de empezar, instala las dependencias necesarias:

```bash
pip install requests aiohttp
```

---

## ğŸ“˜ Ejemplo 1: Rate Limiting BÃ¡sico con `time.sleep()`

### ğŸ¯ Objetivo
Aprender a implementar rate limiting simple para evitar sobrecargar servidores y respetar lÃ­mites de APIs.

### ğŸ¢ Contexto Empresarial
Trabajas en **DataHub Inc.** y necesitas scrapear 100 URLs de un sitio web que no tiene API. Para ser respetuoso con el servidor, decides limitar tus requests a **1 request cada 2 segundos** (mÃ¡ximo 30 requests/minuto).

### ğŸ“Š Datos de Ejemplo
```python
# 100 URLs ficticias para scrapear
urls = [f"https://example.com/productos/{i}" for i in range(1, 101)]
```

### ğŸ’» CÃ³digo Completo

```python
import requests
import time
from typing import List, Dict

def scrapear_con_rate_limiting(
    urls: List[str],
    delay_seconds: float = 2.0
) -> List[Dict]:
    """
    Scrapea mÃºltiples URLs con rate limiting.

    Args:
        urls: Lista de URLs a scrapear
        delay_seconds: Segundos a esperar entre cada request

    Returns:
        Lista de datos scrapeados
    """
    resultados = []
    inicio_total = time.time()

    print(f"ğŸš€ Iniciando scraping de {len(urls)} URLs")
    print(f"â±ï¸  Rate limit: 1 request cada {delay_seconds}s")
    print(f"â±ï¸  Tiempo estimado: {len(urls) * delay_seconds / 60:.1f} minutos\n")

    for i, url in enumerate(urls, 1):
        # Medir tiempo del request
        inicio_request = time.time()

        try:
            # Hacer request HTTP
            response = requests.get(url, timeout=10)
            response.raise_for_status()

            # Simular extracciÃ³n de datos (en realidad parsearÃ­as HTML)
            datos = {
                "url": url,
                "status_code": response.status_code,
                "content_length": len(response.content),
                "timestamp": time.time()
            }

            resultados.append(datos)

            fin_request = time.time()
            tiempo_request = fin_request - inicio_request

            # Log de progreso
            print(f"âœ… [{i}/{len(urls)}] {url} - {tiempo_request*1000:.0f}ms")

        except requests.RequestException as e:
            print(f"âŒ [{i}/{len(urls)}] {url} - Error: {e}")

        # RATE LIMITING: Esperar antes del siguiente request
        # (excepto en el Ãºltimo)
        if i < len(urls):
            time.sleep(delay_seconds)

    fin_total = time.time()
    tiempo_total = fin_total - inicio_total

    # Resumen
    print(f"\nğŸ“Š Resumen:")
    print(f"âœ… Completados: {len(resultados)}/{len(urls)}")
    print(f"â±ï¸  Tiempo total: {tiempo_total:.1f}s ({tiempo_total/60:.1f} min)")
    print(f"âš¡ Velocidad: {len(resultados)/tiempo_total:.2f} URLs/seg")

    return resultados


# ===============================
# EJECUCIÃ“N DEL EJEMPLO
# ===============================

if __name__ == "__main__":
    # URLs de ejemplo (usando JSONPlaceholder, API pÃºblica)
    urls_ejemplo = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 21)]

    # Scrapear con rate limiting de 1 segundo
    resultados = scrapear_con_rate_limiting(urls_ejemplo, delay_seconds=1.0)

    # Mostrar muestra de resultados
    print(f"\nğŸ“„ Muestra de datos scrapeados:")
    for dato in resultados[:3]:
        print(f"  - {dato['url']}: {dato['content_length']} bytes")
```

### ğŸ“¤ Output Esperado

```
ğŸš€ Iniciando scraping de 20 URLs
â±ï¸  Rate limit: 1 request cada 1.0s
â±ï¸  Tiempo estimado: 0.3 minutos

âœ… [1/20] https://jsonplaceholder.typicode.com/posts/1 - 234ms
âœ… [2/20] https://jsonplaceholder.typicode.com/posts/2 - 187ms
âœ… [3/20] https://jsonplaceholder.typicode.com/posts/3 - 195ms
...
âœ… [20/20] https://jsonplaceholder.typicode.com/posts/20 - 203ms

ğŸ“Š Resumen:
âœ… Completados: 20/20
â±ï¸  Tiempo total: 23.1s (0.4 min)
âš¡ Velocidad: 0.87 URLs/seg

ğŸ“„ Muestra de datos scrapeados:
  - https://jsonplaceholder.typicode.com/posts/1: 292 bytes
  - https://jsonplaceholder.typicode.com/posts/2: 277 bytes
  - https://jsonplaceholder.typicode.com/posts/3: 285 bytes
```

### ğŸ” Conceptos Clave

1. **`time.sleep(delay_seconds)`**: Pausa la ejecuciÃ³n durante N segundos
2. **Rate limiting respetuoso**: 1-2 requests/segundo es considerado cortÃ©s
3. **MediciÃ³n de tiempo**: Usar `time.time()` para calcular duraciones
4. **Progreso visual**: Logs que muestran el avance del scraping
5. **Manejo de errores**: Try/except para que un error no detenga todo

### ğŸ“ Lecciones Aprendidas

- âœ… Rate limiting simple pero efectivo con `time.sleep()`
- âœ… 20 URLs en ~23 segundos (1 req/seg incluyendo tiempo de request)
- âœ… Ã‰tico y respetuoso con el servidor
- âŒ **Lento** para grandes volÃºmenes (100 URLs = 3+ minutos)
- ğŸ’¡ **Mejora posible:** Usar async para paralelizar (Ejemplo 3)

---

## ğŸ“˜ Ejemplo 2: Cache Persistente con `shelve`

### ğŸ¯ Objetivo
Implementar un sistema de cache persistente que ahorre requests costosos a APIs de pago.

### ğŸ¢ Contexto Empresarial
Tu empresa paga **$0.01 por cada request** a una API de datos financieros. El pipeline ETL se ejecuta 10 veces al dÃ­a durante desarrollo. Sin cache, cada ejecuciÃ³n cuesta $10 (1,000 requests Ã— $0.01). Con cache, solo la primera ejecuciÃ³n hace requests reales.

**Ahorro:** $90/dÃ­a en desarrollo ($27,000/aÃ±o) ğŸ’°

### ğŸ“Š Datos de Ejemplo
```python
# Simular API costosa que retorna cotizaciones de acciones
def api_costosa(simbolo: str) -> dict:
    """Simula una API de pago ($0.01 por request)."""
    time.sleep(0.5)  # Simular latencia
    return {
        "simbolo": simbolo,
        "precio": round(random.uniform(10, 500), 2),
        "timestamp": time.time()
    }
```

### ğŸ’» CÃ³digo Completo

```python
import shelve
import time
import random
from typing import Optional, Dict

class CacheAPI:
    """Cliente de API con cache persistente en disco."""

    def __init__(self, archivo_cache: str = "cache_api.db", ttl: int = 3600):
        """
        Args:
            archivo_cache: Nombre del archivo de cache (shelve)
            ttl: Tiempo de vida del cache en segundos (default: 1 hora)
        """
        self.archivo_cache = archivo_cache
        self.ttl = ttl
        self.hits = 0
        self.misses = 0

    def obtener_dato(self, clave: str, funcion_api) -> dict:
        """
        Obtiene datos de API con cache.

        Args:
            clave: Identificador Ãºnico del dato (ej: sÃ­mbolo de acciÃ³n)
            funcion_api: FunciÃ³n a llamar si no hay cache

        Returns:
            Datos del API (desde cache o request real)
        """
        with shelve.open(self.archivo_cache) as cache:
            # Verificar si existe en cache y no ha expirado
            if clave in cache:
                datos_cache, timestamp = cache[clave]
                edad = time.time() - timestamp

                # Cache vÃ¡lido (no expirado)
                if edad < self.ttl:
                    self.hits += 1
                    print(f"âœ… Cache HIT: {clave} (edad: {edad:.0f}s)")
                    return datos_cache
                else:
                    print(f"â° Cache EXPIRADO: {clave} (edad: {edad:.0f}s > TTL: {self.ttl}s)")

            # Cache MISS: hacer request real
            self.misses += 1
            print(f"ğŸŒ Cache MISS: {clave} - Haciendo request...")
            datos = funcion_api(clave)

            # Guardar en cache con timestamp
            cache[clave] = (datos, time.time())

            return datos

    def estadisticas(self) -> str:
        """Retorna estadÃ­sticas de uso del cache."""
        total = self.hits + self.misses
        hit_rate = self.hits / total * 100 if total > 0 else 0

        return f"""
ğŸ“Š EstadÃ­sticas de Cache:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Cache Hits: {self.hits}
ğŸŒ Cache Misses: {self.misses}
ğŸ“ˆ Hit Rate: {hit_rate:.1f}%
ğŸ’° Requests ahorrados: {self.hits} Ã— $0.01 = ${self.hits * 0.01:.2f}
        """


def obtener_cotizacion_accion(simbolo: str) -> dict:
    """Simula API costosa de cotizaciones ($0.01/request)."""
    print(f"   ğŸ’¸ Cobrando $0.01...")
    time.sleep(0.5)  # Simular latencia de API
    return {
        "simbolo": simbolo,
        "precio": round(random.uniform(10, 500), 2),
        "volumen": random.randint(1000, 100000),
        "timestamp": time.time()
    }


# ===============================
# EJECUCIÃ“N DEL EJEMPLO
# ===============================

if __name__ == "__main__":
    # Lista de acciones a consultar
    acciones = ["AAPL", "GOOGL", "MSFT", "AMZN", "TSLA"]

    # Crear cliente con cache (TTL: 30 segundos para demo)
    cliente = CacheAPI(archivo_cache="cache_finanzas.db", ttl=30)

    print("=" * 50)
    print("PRIMERA EJECUCIÃ“N (Sin cache)")
    print("=" * 50)
    for accion in acciones:
        dato = cliente.obtener_dato(accion, obtener_cotizacion_accion)
        print(f"  â†’ {accion}: ${dato['precio']:.2f}\n")

    print(cliente.estadisticas())

    # Simular segunda ejecuciÃ³n (deberÃ­a usar cache)
    print("\n" + "=" * 50)
    print("SEGUNDA EJECUCIÃ“N (Con cache - instantÃ¡neo)")
    print("=" * 50)

    # Resetear contadores para segunda ejecuciÃ³n
    cliente_2da = CacheAPI(archivo_cache="cache_finanzas.db", ttl=30)

    for accion in acciones:
        dato = cliente_2da.obtener_dato(accion, obtener_cotizacion_accion)
        print(f"  â†’ {accion}: ${dato['precio']:.2f}\n")

    print(cliente_2da.estadisticas())

    # Simular expiraciÃ³n del cache
    print("\n" + "=" * 50)
    print("TERCERA EJECUCIÃ“N (DespuÃ©s de 35 segundos)")
    print("=" * 50)
    print("â³ Esperando 35 segundos para que expire el cache...")
    time.sleep(35)

    cliente_3ra = CacheAPI(archivo_cache="cache_finanzas.db", ttl=30)

    for accion in acciones[:2]:  # Solo 2 para no hacer muy largo el ejemplo
        dato = cliente_3ra.obtener_dato(accion, obtener_cotizacion_accion)
        print(f"  â†’ {accion}: ${dato['precio']:.2f}\n")

    print(cliente_3ra.estadisticas())
```

### ğŸ“¤ Output Esperado

```
==================================================
PRIMERA EJECUCIÃ“N (Sin cache)
==================================================
ğŸŒ Cache MISS: AAPL - Haciendo request...
   ğŸ’¸ Cobrando $0.01...
  â†’ AAPL: $145.23

ğŸŒ Cache MISS: GOOGL - Haciendo request...
   ğŸ’¸ Cobrando $0.01...
  â†’ GOOGL: $102.45

... (3 mÃ¡s)

ğŸ“Š EstadÃ­sticas de Cache:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Cache Hits: 0
ğŸŒ Cache Misses: 5
ğŸ“ˆ Hit Rate: 0.0%
ğŸ’° Requests ahorrados: 0 Ã— $0.01 = $0.00

==================================================
SEGUNDA EJECUCIÃ“N (Con cache - instantÃ¡neo)
==================================================
âœ… Cache HIT: AAPL (edad: 3s)
  â†’ AAPL: $145.23

âœ… Cache HIT: GOOGL (edad: 3s)
  â†’ GOOGL: $102.45

... (3 mÃ¡s)

ğŸ“Š EstadÃ­sticas de Cache:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Cache Hits: 5
ğŸŒ Cache Misses: 0
ğŸ“ˆ Hit Rate: 100.0%
ğŸ’° Requests ahorrados: 5 Ã— $0.01 = $0.05

==================================================
TERCERA EJECUCIÃ“N (DespuÃ©s de 35 segundos)
==================================================
â³ Esperando 35 segundos para que expire el cache...
â° Cache EXPIRADO: AAPL (edad: 38s > TTL: 30s)
ğŸŒ Cache MISS: AAPL - Haciendo request...
   ğŸ’¸ Cobrando $0.01...
  â†’ AAPL: $147.89
```

### ğŸ” Conceptos Clave

1. **`shelve`**: Base de datos simple tipo diccionario persistente
2. **TTL (Time To Live)**: Cache expira automÃ¡ticamente despuÃ©s de N segundos
3. **Cache Hit Rate**: MÃ©trica clave (100% = todos los requests desde cache)
4. **Persistencia**: El cache sobrevive entre ejecuciones del programa
5. **ROI**: Calcular ahorro monetario del cache ($0.05 en este ejemplo)

### ğŸ“ Lecciones Aprendidas

- âœ… Cache reduce costos de APIs de pago dramÃ¡ticamente
- âœ… `shelve` es perfecto para cache simple persistente
- âœ… TTL evita datos obsoletos
- âœ… Segunda ejecuciÃ³n es **instantÃ¡nea** (sin esperar 0.5s por request)
- ğŸ’¡ **En producciÃ³n:** Usar Redis para cache distribuido

---

## ğŸ“˜ Ejemplo 3: Async Requests con `aiohttp` (20x MÃ¡s RÃ¡pido)

### ğŸ¯ Objetivo
Demostrar el poder de la programaciÃ³n asÃ­ncrona para hacer mÃºltiples requests HTTP en paralelo.

### ğŸ¢ Contexto Empresarial
Necesitas actualizar el catÃ¡logo completo de **1,000 productos** consultando una API. Cada request tarda ~0.5 segundos.

- **SÃ­ncrono:** 1,000 Ã— 0.5s = 500s = **8.3 minutos** â°
- **Async (20 concurrentes):** 1,000 Ã· 20 Ã— 0.5s = 25s = **25 segundos** âš¡ (20x mÃ¡s rÃ¡pido)

### ğŸ’» CÃ³digo Completo

```python
import aiohttp
import asyncio
import time
from typing import List, Dict

# =========================================
# VERSIÃ“N SÃNCRONA (Para comparaciÃ³n)
# =========================================

import requests

def scrapear_sincrono(urls: List[str]) -> List[Dict]:
    """VersiÃ³n sÃ­ncrona: hace requests UNO POR UNO."""
    print(f"ğŸ¢ SÃNCRONO: Scrapeando {len(urls)} URLs...")
    inicio = time.time()

    resultados = []
    for i, url in enumerate(urls, 1):
        response = requests.get(url)
        datos = response.json()
        resultados.append(datos)

        if i % 10 == 0:
            print(f"   Progreso: {i}/{len(urls)}")

    fin = time.time()
    print(f"âœ… Completado en {fin-inicio:.1f}s\n")

    return resultados


# =========================================
# VERSIÃ“N ASÃNCRONA (RÃ¡pida)
# =========================================

async def obtener_url_async(session: aiohttp.ClientSession, url: str, semaforo: asyncio.Semaphore) -> dict:
    """
    Hace un request asÃ­ncrono con rate limiting.

    Args:
        session: SesiÃ³n HTTP reutilizable
        url: URL a consultar
        semaforo: Limita requests concurrentes
    """
    async with semaforo:  # Limita a N requests concurrentes
        async with session.get(url) as response:
            return await response.json()


async def scrapear_async(urls: List[str], max_concurrente: int = 20) -> List[Dict]:
    """
    VersiÃ³n asÃ­ncrona: hace MÃšLTIPLES requests EN PARALELO.

    Args:
        urls: Lista de URLs
        max_concurrente: MÃ¡ximo nÃºmero de requests simultÃ¡neos
    """
    print(f"âš¡ ASYNC: Scrapeando {len(urls)} URLs ({max_concurrente} concurrentes)...")
    inicio = time.time()

    # SemÃ¡foro para limitar concurrencia
    semaforo = asyncio.Semaphore(max_concurrente)

    # Crear sesiÃ³n HTTP reutilizable
    async with aiohttp.ClientSession() as session:
        # Lanzar todos los requests EN PARALELO
        tareas = [obtener_url_async(session, url, semaforo) for url in urls]
        resultados = await asyncio.gather(*tareas)

    fin = time.time()
    print(f"âœ… Completado en {fin-inicio:.1f}s\n")

    return resultados


# ===============================
# COMPARACIÃ“N LADO A LADO
# ===============================

def comparar_sincrono_vs_async(n_urls: int = 50):
    """Compara rendimiento sÃ­ncrono vs asÃ­ncrono."""
    # URLs de ejemplo (API pÃºblica)
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, n_urls + 1)]

    print("=" * 60)
    print(f"COMPARACIÃ“N: Scraping de {n_urls} URLs")
    print("=" * 60)
    print()

    # 1. VersiÃ³n sÃ­ncrona
    inicio_sinc = time.time()
    resultados_sinc = scrapear_sincrono(urls)
    tiempo_sinc = time.time() - inicio_sinc

    # 2. VersiÃ³n asÃ­ncrona
    inicio_async = time.time()
    resultados_async = asyncio.run(scrapear_async(urls, max_concurrente=20))
    tiempo_async = time.time() - inicio_async

    # 3. ComparaciÃ³n
    mejora = tiempo_sinc / tiempo_async

    print("=" * 60)
    print("ğŸ“Š RESULTADOS:")
    print("=" * 60)
    print(f"ğŸ¢ SÃ­ncrono:  {tiempo_sinc:.1f}s")
    print(f"âš¡ Async:     {tiempo_async:.1f}s")
    print(f"ğŸš€ Mejora:    {mejora:.1f}x mÃ¡s rÃ¡pido")
    print(f"â±ï¸  Ahorro:    {tiempo_sinc - tiempo_async:.1f}s ({(1 - tiempo_async/tiempo_sinc)*100:.0f}% menos tiempo)")
    print("=" * 60)


# ===============================
# EJECUCIÃ“N DEL EJEMPLO
# ===============================

if __name__ == "__main__":
    # Comparar con 50 URLs (ajusta segÃºn tu conexiÃ³n)
    comparar_sincrono_vs_async(n_urls=50)

    # Extra: Demostrar escalabilidad
    print("\n\n" + "=" * 60)
    print("ğŸ”¬ EXPERIMENTO: Â¿CuÃ¡nto mejora con mÃ¡s URLs?")
    print("=" * 60)
    print()

    for n in [10, 25, 50, 100]:
        urls = [f"https://jsonplaceholder.typicode.com/posts/{i % 100 + 1}" for i in range(n)]

        # SÃ­ncrono
        inicio = time.time()
        _ = scrapear_sincrono(urls)
        tiempo_sinc = time.time() - inicio

        # Async
        inicio = time.time()
        _ = asyncio.run(scrapear_async(urls, max_concurrente=20))
        tiempo_async = time.time() - inicio

        mejora = tiempo_sinc / tiempo_async
        print(f"ğŸ“Š {n} URLs: SÃ­ncrono {tiempo_sinc:.1f}s | Async {tiempo_async:.1f}s | Mejora: {mejora:.1f}x")
```

### ğŸ“¤ Output Esperado

```
============================================================
COMPARACIÃ“N: Scraping de 50 URLs
============================================================

ğŸ¢ SÃNCRONO: Scrapeando 50 URLs...
   Progreso: 10/50
   Progreso: 20/50
   Progreso: 30/50
   Progreso: 40/50
   Progreso: 50/50
âœ… Completado en 27.3s

âš¡ ASYNC: Scrapeando 50 URLs (20 concurrentes)...
âœ… Completado en 1.4s

============================================================
ğŸ“Š RESULTADOS:
============================================================
ğŸ¢ SÃ­ncrono:  27.3s
âš¡ Async:     1.4s
ğŸš€ Mejora:    19.5x mÃ¡s rÃ¡pido
â±ï¸  Ahorro:    25.9s (95% menos tiempo)
============================================================


============================================================
ğŸ”¬ EXPERIMENTO: Â¿CuÃ¡nto mejora con mÃ¡s URLs?
============================================================

ğŸ“Š 10 URLs: SÃ­ncrono 5.1s | Async 0.6s | Mejora: 8.5x
ğŸ“Š 25 URLs: SÃ­ncrono 13.2s | Async 1.0s | Mejora: 13.2x
ğŸ“Š 50 URLs: SÃ­ncrono 27.8s | Async 1.5s | Mejora: 18.5x
ğŸ“Š 100 URLs: SÃ­ncrono 54.3s | Async 2.9s | Mejora: 18.7x
```

### ğŸ” Conceptos Clave

1. **`async`/`await`**: Palabras clave para funciones asÃ­ncronas
2. **`aiohttp`**: LibrerÃ­a HTTP asÃ­ncrona (alternativa async a `requests`)
3. **`asyncio.Semaphore`**: Limita cuÃ¡ntas tareas corren simultÃ¡neamente
4. **`asyncio.gather()`**: Ejecuta mÃºltiples coroutines en paralelo
5. **Concurrencia vs Paralelismo**: Async = concurrencia (1 thread, muchas tareas esperando I/O)

### ğŸ“ Lecciones Aprendidas

- âœ… Async puede ser **10-20x mÃ¡s rÃ¡pido** para I/O-bound (requests HTTP)
- âœ… Crucial usar `Semaphore` para limitar concurrencia (evitar sobrecarga)
- âœ… MÃ¡s URLs = mayor mejora relativa (asymptotically mejor)
- âŒ **Cuidado:** Async sin rate limiting = posible ban
- ğŸ’¡ **Mejor prÃ¡ctica:** Combinar async + semÃ¡foro + delay mÃ­nimo

---

## ğŸ“˜ Ejemplo 4: Scraper Optimizado Completo (Async + Cache + Rate Limiting + MÃ©tricas)

### ğŸ¯ Objetivo
Integrar **todas** las tÃ©cnicas de optimizaciÃ³n en un scraper de producciÃ³n completo.

### ğŸ¢ Contexto Empresarial
Necesitas scrapear 500 productos de una API para un dashboard que se actualiza cada hora. Cada request cuesta $0.01.

**Requisitos:**
- âœ… MÃ¡ximo 50 requests/minuto (cortesÃ­a)
- âœ… Cachear datos por 1 hora (evitar requests innecesarios)
- âœ… MÃ¡xima velocidad posible (dentro de lÃ­mites)
- âœ… MÃ©tricas detalladas (throughput, cache hit rate, costos)

### ğŸ’» CÃ³digo Completo

```python
import aiohttp
import asyncio
import time
import shelve
from typing import List, Dict, Optional
from dataclasses import dataclass, field

@dataclass
class Metricas:
    """Contenedor de mÃ©tricas del scraper."""
    total_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    errores: int = 0
    inicio: float = field(default_factory=time.time)

    @property
    def tiempo_transcurrido(self) -> float:
        return time.time() - self.inicio

    @property
    def throughput(self) -> float:
        """Requests por segundo."""
        return self.total_requests / self.tiempo_transcurrido if self.tiempo_transcurrido > 0 else 0

    @property
    def cache_hit_rate(self) -> float:
        """Porcentaje de hits en cache."""
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total * 100 if total > 0 else 0

    @property
    def error_rate(self) -> float:
        """Porcentaje de errores."""
        return self.errores / self.total_requests * 100 if self.total_requests > 0 else 0

    @property
    def costo_estimado(self) -> float:
        """Costo en $ (asumiendo $0.01 por request real)."""
        return self.cache_misses * 0.01

    @property
    def ahorro_estimado(self) -> float:
        """Ahorro en $ gracias al cache."""
        return self.cache_hits * 0.01

    def reporte(self) -> str:
        """Genera reporte detallado."""
        return f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ“Š MÃ‰TRICAS DEL SCRAPER OPTIMIZADO              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ â±ï¸  Tiempo total:        {self.tiempo_transcurrido:>6.1f}s                      â•‘
â•‘ ğŸ“ Total requests:       {self.total_requests:>6}                          â•‘
â•‘ âš¡ Throughput:           {self.throughput:>6.1f} req/seg                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ âœ… Cache HITS:           {self.cache_hits:>6} ({self.cache_hit_rate:>5.1f}%)                â•‘
â•‘ ğŸŒ Cache MISSES:         {self.cache_misses:>6}                          â•‘
â•‘ âŒ Errores:              {self.errores:>6} ({self.error_rate:>5.2f}%)                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ’° Costo (requests):     ${self.costo_estimado:>6.2f}                        â•‘
â•‘ ğŸ’µ Ahorro (cache):       ${self.ahorro_estimado:>6.2f}                        â•‘
â•‘ ğŸ“Š ROI del cache:        {self.ahorro_estimado/(self.ahorro_estimado+self.costo_estimado)*100 if (self.ahorro_estimado+self.costo_estimado) > 0 else 0:>5.1f}%                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """


class ScraperOptimizado:
    """Scraper de producciÃ³n con async, cache, rate limiting y mÃ©tricas."""

    def __init__(
        self,
        archivo_cache: str = "cache_scraper.db",
        ttl: int = 3600,
        max_concurrente: int = 20,
        delay_minimo: float = 0.1
    ):
        """
        Args:
            archivo_cache: Archivo del cache persistente
            ttl: Tiempo de vida del cache (segundos)
            max_concurrente: MÃ¡ximo requests simultÃ¡neos
            delay_minimo: Delay mÃ­nimo entre requests (segundos)
        """
        self.archivo_cache = archivo_cache
        self.ttl = ttl
        self.max_concurrente = max_concurrente
        self.delay_minimo = delay_minimo
        self.metricas = Metricas()

    def _verificar_cache(self, clave: str) -> Optional[dict]:
        """Verifica si existe en cache y es vÃ¡lido."""
        with shelve.open(self.archivo_cache) as cache:
            if clave in cache:
                datos, timestamp = cache[clave]
                edad = time.time() - timestamp

                if edad < self.ttl:
                    self.metricas.total_requests += 1
                    self.metricas.cache_hits += 1
                    return datos

        return None

    def _guardar_en_cache(self, clave: str, datos: dict):
        """Guarda datos en cache."""
        with shelve.open(self.archivo_cache) as cache:
            cache[clave] = (datos, time.time())

    async def _obtener_con_cache(
        self,
        session: aiohttp.ClientSession,
        url: str,
        semaforo: asyncio.Semaphore
    ) -> dict:
        """Obtiene datos con cache, async y rate limiting."""
        # 1. Verificar cache
        datos_cache = self._verificar_cache(url)
        if datos_cache is not None:
            return datos_cache

        # 2. Request real con rate limiting
        async with semaforo:
            try:
                # Delay mÃ­nimo (cortesÃ­a)
                await asyncio.sleep(self.delay_minimo)

                # Request HTTP
                async with session.get(url, timeout=10) as response:
                    response.raise_for_status()
                    datos = await response.json()

                    # Guardar en cache
                    self._guardar_en_cache(url, datos)

                    # Actualizar mÃ©tricas
                    self.metricas.total_requests += 1
                    self.metricas.cache_misses += 1

                    return datos

            except Exception as e:
                self.metricas.errores += 1
                print(f"âŒ Error en {url}: {e}")
                return {"error": str(e), "url": url}

    async def scrapear(self, urls: List[str]) -> List[dict]:
        """
        Scrape masivo optimizado.

        Args:
            urls: Lista de URLs a scrapear

        Returns:
            Lista de datos scrapeados
        """
        print(f"ğŸš€ Iniciando scraper optimizado")
        print(f"ğŸ“ URLs: {len(urls)}")
        print(f"âš¡ Concurrencia: {self.max_concurrente}")
        print(f"â±ï¸  Delay mÃ­nimo: {self.delay_minimo}s")
        print(f"ğŸ’¾ Cache TTL: {self.ttl}s ({self.ttl/3600:.1f}h)")
        print()

        # SemÃ¡foro para limitar concurrencia
        semaforo = asyncio.Semaphore(self.max_concurrente)

        # SesiÃ³n HTTP reutilizable
        async with aiohttp.ClientSession() as session:
            tareas = [
                self._obtener_con_cache(session, url, semaforo)
                for url in urls
            ]
            resultados = await asyncio.gather(*tareas)

        return resultados


# ===============================
# DEMOSTRACIÃ“N COMPLETA
# ===============================

async def demo_completa():
    """Demuestra el scraper optimizado con mÃºltiples ejecuciones."""
    # URLs de ejemplo (API pÃºblica)
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 101)]

    # Crear scraper
    scraper = ScraperOptimizado(
        archivo_cache="demo_cache.db",
        ttl=60,  # Cache de 1 minuto (para demo)
        max_concurrente=20,
        delay_minimo=0.05  # 50ms delay
    )

    print("=" * 70)
    print("PRIMERA EJECUCIÃ“N (Sin cache - todos los requests son reales)")
    print("=" * 70)
    print()

    resultados_1 = await scraper.scrapear(urls)
    print(scraper.metricas.reporte())

    # Segunda ejecuciÃ³n (deberÃ­a usar cache al 100%)
    print("\n" + "=" * 70)
    print("SEGUNDA EJECUCIÃ“N (Con cache - deberÃ­a ser instantÃ¡neo)")
    print("=" * 70)
    print()

    scraper_2 = ScraperOptimizado(
        archivo_cache="demo_cache.db",
        ttl=60,
        max_concurrente=20,
        delay_minimo=0.05
    )

    resultados_2 = await scraper_2.scrapear(urls)
    print(scraper_2.metricas.reporte())

    # ComparaciÃ³n
    mejora = scraper.metricas.tiempo_transcurrido / scraper_2.metricas.tiempo_transcurrido

    print("\n" + "=" * 70)
    print("ğŸ“Š COMPARACIÃ“N:")
    print("=" * 70)
    print(f"1ra ejecuciÃ³n: {scraper.metricas.tiempo_transcurrido:.1f}s (sin cache)")
    print(f"2da ejecuciÃ³n: {scraper_2.metricas.tiempo_transcurrido:.1f}s (100% cache)")
    print(f"Mejora: {mejora:.0f}x mÃ¡s rÃ¡pido")
    print(f"Ahorro acumulado: ${scraper_2.metricas.ahorro_estimado:.2f}")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(demo_completa())
```

### ğŸ“¤ Output Esperado

```
======================================================================
PRIMERA EJECUCIÃ“N (Sin cache - todos los requests son reales)
======================================================================

ğŸš€ Iniciando scraper optimizado
ğŸ“ URLs: 100
âš¡ Concurrencia: 20
â±ï¸  Delay mÃ­nimo: 0.05s
ğŸ’¾ Cache TTL: 60s (0.0h)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ“Š MÃ‰TRICAS DEL SCRAPER OPTIMIZADO              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ â±ï¸  Tiempo total:           8.3s                         â•‘
â•‘ ğŸ“ Total requests:          100                          â•‘
â•‘ âš¡ Throughput:             12.0 req/seg                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ âœ… Cache HITS:                0 (  0.0%)                  â•‘
â•‘ ğŸŒ Cache MISSES:            100                          â•‘
â•‘ âŒ Errores:                   0 ( 0.00%)                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ’° Costo (requests):      $ 1.00                         â•‘
â•‘ ğŸ’µ Ahorro (cache):        $ 0.00                         â•‘
â•‘ ğŸ“Š ROI del cache:          0.0%                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

======================================================================
SEGUNDA EJECUCIÃ“N (Con cache - deberÃ­a ser instantÃ¡neo)
======================================================================

ğŸš€ Iniciando scraper optimizado
ğŸ“ URLs: 100
âš¡ Concurrencia: 20
â±ï¸  Delay mÃ­nimo: 0.05s
ğŸ’¾ Cache TTL: 60s (0.0h)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ“Š MÃ‰TRICAS DEL SCRAPER OPTIMIZADO              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ â±ï¸  Tiempo total:           0.2s                         â•‘
â•‘ ğŸ“ Total requests:          100                          â•‘
â•‘ âš¡ Throughput:           500.0 req/seg                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ âœ… Cache HITS:              100 (100.0%)                  â•‘
â•‘ ğŸŒ Cache MISSES:              0                          â•‘
â•‘ âŒ Errores:                   0 ( 0.00%)                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ’° Costo (requests):      $ 0.00                         â•‘
â•‘ ğŸ’µ Ahorro (cache):        $ 1.00                         â•‘
â•‘ ğŸ“Š ROI del cache:        100.0%                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

======================================================================
ğŸ“Š COMPARACIÃ“N:
======================================================================
1ra ejecuciÃ³n: 8.3s (sin cache)
2da ejecuciÃ³n: 0.2s (100% cache)
Mejora: 42x mÃ¡s rÃ¡pido
Ahorro acumulado: $1.00
======================================================================
```

### ğŸ” Conceptos Clave Integrados

1. **Async + Semaphore**: MÃ¡xima velocidad con control de concurrencia
2. **Cache persistente**: Ahorra requests entre ejecuciones
3. **TTL**: Datos siempre frescos (expiraciÃ³n automÃ¡tica)
4. **MÃ©tricas completas**: Throughput, hit rate, costos, ROI
5. **Manejo de errores**: No detiene el scraping completo si falla 1 URL

### ğŸ“ Lecciones Aprendidas

- âœ… **Primera ejecuciÃ³n:** 8.3s, $1.00 de costo
- âœ… **Segunda ejecuciÃ³n:** 0.2s, $0 de costo (100% cache)
- âœ… **Mejora:** 42x mÃ¡s rÃ¡pido en segunda ejecuciÃ³n
- âœ… **ROI del cache:** 100% (todo el costo evitado)
- âœ… **Throughput:** De 12 req/seg (sin cache) a 500 req/seg (con cache)
- ğŸ’¡ **ProducciÃ³n:** Este patrÃ³n es ideal para pipelines ETL que corren cada hora

---

## ğŸ¯ Resumen y PrÃ³ximos Pasos

### Lo que aprendiste:

| Ejemplo | TÃ©cnica              | Mejora             | Caso de Uso     |
| ------- | -------------------- | ------------------ | --------------- |
| **1**   | Rate limiting bÃ¡sico | Ã‰tico âœ…            | Scraping cortÃ©s |
| **2**   | Cache persistente    | 90% menos requests | APIs de pago    |
| **3**   | Async requests       | 20x mÃ¡s rÃ¡pido     | Volumen masivo  |
| **4**   | Combo completo       | 42x + 100% ROI     | ProducciÃ³n      |

### PrÃ³ximos pasos:

1. âœ… **Ejercicios:** Resuelve los 12 ejercicios del archivo `03-EJERCICIOS.md`
2. âœ… **Proyecto prÃ¡ctico:** Implementa un scraper optimizado completo con TDD
3. âœ… **Experimenta:** Prueba diferentes configuraciones (TTL, concurrencia, delay)
4. âœ… **Mide:** Siempre mide throughput, cache hit rate y costos

---

**Ãšltima actualizaciÃ³n:** 2025-10-24
**Tiempo total:** 45-60 minutos de lectura/prÃ¡ctica
**Nivel:** Intermedio-Avanzado
