# üìù Ejemplos Trabajados - Tema 3: Rate Limiting y Caching

**M√≥dulo 4:** APIs y Web Scraping
**Duraci√≥n estimada:** 45-60 minutos
**Prerequisitos:** Haber le√≠do 01-TEORIA.md del Tema 3

---

## üéØ Introducci√≥n

En este archivo encontrar√°s **4 ejemplos completos y ejecutables** que demuestran c√≥mo optimizar scrapers y clientes de APIs mediante:

1. **Rate limiting b√°sico** (cortes√≠a y evitar bans)
2. **Cache persistente** (ahorrar requests costosos)
3. **Async requests** (velocidad 20x)
4. **Scraper optimizado completo** (combo de todas las t√©cnicas)

Todos los ejemplos son **c√≥digo real** que puedes copiar, pegar y ejecutar directamente en Python 3.8+.

---

## üì¶ Dependencias

Antes de empezar, instala las dependencias necesarias:

```bash
pip install requests aiohttp
```

---

## üìò Ejemplo 1: Rate Limiting B√°sico con `time.sleep()`

### üéØ Objetivo
Aprender a implementar rate limiting simple para evitar sobrecargar servidores y respetar l√≠mites de APIs.

### üè¢ Contexto Empresarial
Trabajas en **DataHub Inc.** y necesitas scrapear 100 URLs de un sitio web que no tiene API. Para ser respetuoso con el servidor, decides limitar tus requests a **1 request cada 2 segundos** (m√°ximo 30 requests/minuto).

### üìä Datos de Ejemplo
```python
# 100 URLs ficticias para scrapear
urls = [f"https://example.com/productos/{i}" for i in range(1, 101)]
```

### üíª C√≥digo Completo

```python
import requests
import time
from typing import List, Dict

def scrapear_con_rate_limiting(
    urls: List[str],
    delay_seconds: float = 2.0
) -> List[Dict]:
    """
    Scrapea m√∫ltiples URLs con rate limiting.

    Args:
        urls: Lista de URLs a scrapear
        delay_seconds: Segundos a esperar entre cada request

    Returns:
        Lista de datos scrapeados
    """
    resultados = []
    inicio_total = time.time()

    print(f"üöÄ Iniciando scraping de {len(urls)} URLs")
    print(f"‚è±Ô∏è  Rate limit: 1 request cada {delay_seconds}s")
    print(f"‚è±Ô∏è  Tiempo estimado: {len(urls) * delay_seconds / 60:.1f} minutos\n")

    for i, url in enumerate(urls, 1):
        # Medir tiempo del request
        inicio_request = time.time()

        try:
            # Hacer request HTTP
            response = requests.get(url, timeout=10)
            response.raise_for_status()

            # Simular extracci√≥n de datos (en realidad parsear√≠as HTML)
            datos = {
                "url": url,
                "status_code": response.status_code,
                "content_length": len(response.content),
                "timestamp": time.time()
            }

            resultados.append(datos)

            fin_request = time.time()
            tiempo_request = fin_request - inicio_request

            # Log de progreso
            print(f"‚úÖ [{i}/{len(urls)}] {url} - {tiempo_request*1000:.0f}ms")

        except requests.RequestException as e:
            print(f"‚ùå [{i}/{len(urls)}] {url} - Error: {e}")

        # RATE LIMITING: Esperar antes del siguiente request
        # (excepto en el √∫ltimo)
        if i < len(urls):
            time.sleep(delay_seconds)

    fin_total = time.time()
    tiempo_total = fin_total - inicio_total

    # Resumen
    print(f"\nüìä Resumen:")
    print(f"‚úÖ Completados: {len(resultados)}/{len(urls)}")
    print(f"‚è±Ô∏è  Tiempo total: {tiempo_total:.1f}s ({tiempo_total/60:.1f} min)")
    print(f"‚ö° Velocidad: {len(resultados)/tiempo_total:.2f} URLs/seg")

    return resultados


# ===============================
# EJECUCI√ìN DEL EJEMPLO
# ===============================

if __name__ == "__main__":
    # URLs de ejemplo (usando JSONPlaceholder, API p√∫blica)
    urls_ejemplo = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 21)]

    # Scrapear con rate limiting de 1 segundo
    resultados = scrapear_con_rate_limiting(urls_ejemplo, delay_seconds=1.0)

    # Mostrar muestra de resultados
    print(f"\nüìÑ Muestra de datos scrapeados:")
    for dato in resultados[:3]:
        print(f"  - {dato['url']}: {dato['content_length']} bytes")
```

### üì§ Output Esperado

```
üöÄ Iniciando scraping de 20 URLs
‚è±Ô∏è  Rate limit: 1 request cada 1.0s
‚è±Ô∏è  Tiempo estimado: 0.3 minutos

‚úÖ [1/20] https://jsonplaceholder.typicode.com/posts/1 - 234ms
‚úÖ [2/20] https://jsonplaceholder.typicode.com/posts/2 - 187ms
‚úÖ [3/20] https://jsonplaceholder.typicode.com/posts/3 - 195ms
...
‚úÖ [20/20] https://jsonplaceholder.typicode.com/posts/20 - 203ms

üìä Resumen:
‚úÖ Completados: 20/20
‚è±Ô∏è  Tiempo total: 23.1s (0.4 min)
‚ö° Velocidad: 0.87 URLs/seg

üìÑ Muestra de datos scrapeados:
  - https://jsonplaceholder.typicode.com/posts/1: 292 bytes
  - https://jsonplaceholder.typicode.com/posts/2: 277 bytes
  - https://jsonplaceholder.typicode.com/posts/3: 285 bytes
```

### üîç Conceptos Clave

1. **`time.sleep(delay_seconds)`**: Pausa la ejecuci√≥n durante N segundos
2. **Rate limiting respetuoso**: 1-2 requests/segundo es considerado cort√©s
3. **Medici√≥n de tiempo**: Usar `time.time()` para calcular duraciones
4. **Progreso visual**: Logs que muestran el avance del scraping
5. **Manejo de errores**: Try/except para que un error no detenga todo

### üéì Lecciones Aprendidas

- ‚úÖ Rate limiting simple pero efectivo con `time.sleep()`
- ‚úÖ 20 URLs en ~23 segundos (1 req/seg incluyendo tiempo de request)
- ‚úÖ √âtico y respetuoso con el servidor
- ‚ùå **Lento** para grandes vol√∫menes (100 URLs = 3+ minutos)
- üí° **Mejora posible:** Usar async para paralelizar (Ejemplo 3)

---

## üìò Ejemplo 2: Cache Persistente con `shelve`

### üéØ Objetivo
Implementar un sistema de cache persistente que ahorre requests costosos a APIs de pago.

### üè¢ Contexto Empresarial
Tu empresa paga **$0.01 por cada request** a una API de datos financieros. El pipeline ETL se ejecuta 10 veces al d√≠a durante desarrollo. Sin cache, cada ejecuci√≥n cuesta $10 (1,000 requests √ó $0.01). Con cache, solo la primera ejecuci√≥n hace requests reales.

**Ahorro:** $90/d√≠a en desarrollo ($27,000/a√±o) üí∞

### üìä Datos de Ejemplo
```python
# Simular API costosa que retorna cotizaciones de acciones
def api_costosa(simbolo: str) -> dict:
    """Simula una API de pago ($0.01 por request)."""
    time.sleep(0.5)  # Simular latencia
    return {
        "simbolo": simbolo,
        "precio": round(random.uniform(10, 500), 2),
        "timestamp": time.time()
    }
```

### üíª C√≥digo Completo

```python
import shelve
import time
import random
from typing import Optional, Dict

class CacheAPI:
    """Cliente de API con cache persistente en disco."""

    def __init__(self, archivo_cache: str = "cache_api.db", ttl: int = 3600):
        """
        Args:
            archivo_cache: Nombre del archivo de cache (shelve)
            ttl: Tiempo de vida del cache en segundos (default: 1 hora)
        """
        self.archivo_cache = archivo_cache
        self.ttl = ttl
        self.hits = 0
        self.misses = 0

    def obtener_dato(self, clave: str, funcion_api) -> dict:
        """
        Obtiene datos de API con cache.

        Args:
            clave: Identificador √∫nico del dato (ej: s√≠mbolo de acci√≥n)
            funcion_api: Funci√≥n a llamar si no hay cache

        Returns:
            Datos del API (desde cache o request real)
        """
        with shelve.open(self.archivo_cache) as cache:
            # Verificar si existe en cache y no ha expirado
            if clave in cache:
                datos_cache, timestamp = cache[clave]
                edad = time.time() - timestamp

                # Cache v√°lido (no expirado)
                if edad < self.ttl:
                    self.hits += 1
                    print(f"‚úÖ Cache HIT: {clave} (edad: {edad:.0f}s)")
                    return datos_cache
                else:
                    print(f"‚è∞ Cache EXPIRADO: {clave} (edad: {edad:.0f}s > TTL: {self.ttl}s)")

            # Cache MISS: hacer request real
            self.misses += 1
            print(f"üåê Cache MISS: {clave} - Haciendo request...")
            datos = funcion_api(clave)

            # Guardar en cache con timestamp
            cache[clave] = (datos, time.time())

            return datos

    def estadisticas(self) -> str:
        """Retorna estad√≠sticas de uso del cache."""
        total = self.hits + self.misses
        hit_rate = self.hits / total * 100 if total > 0 else 0

        return f"""
üìä Estad√≠sticas de Cache:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ Cache Hits: {self.hits}
üåê Cache Misses: {self.misses}
üìà Hit Rate: {hit_rate:.1f}%
üí∞ Requests ahorrados: {self.hits} √ó $0.01 = ${self.hits * 0.01:.2f}
        """


def obtener_cotizacion_accion(simbolo: str) -> dict:
    """Simula API costosa de cotizaciones ($0.01/request)."""
    print(f"   üí∏ Cobrando $0.01...")
    time.sleep(0.5)  # Simular latencia de API
    return {
        "simbolo": simbolo,
        "precio": round(random.uniform(10, 500), 2),
        "volumen": random.randint(1000, 100000),
        "timestamp": time.time()
    }


# ===============================
# EJECUCI√ìN DEL EJEMPLO
# ===============================

if __name__ == "__main__":
    # Lista de acciones a consultar
    acciones = ["AAPL", "GOOGL", "MSFT", "AMZN", "TSLA"]

    # Crear cliente con cache (TTL: 30 segundos para demo)
    cliente = CacheAPI(archivo_cache="cache_finanzas.db", ttl=30)

    print("=" * 50)
    print("PRIMERA EJECUCI√ìN (Sin cache)")
    print("=" * 50)
    for accion in acciones:
        dato = cliente.obtener_dato(accion, obtener_cotizacion_accion)
        print(f"  ‚Üí {accion}: ${dato['precio']:.2f}\n")

    print(cliente.estadisticas())

    # Simular segunda ejecuci√≥n (deber√≠a usar cache)
    print("\n" + "=" * 50)
    print("SEGUNDA EJECUCI√ìN (Con cache - instant√°neo)")
    print("=" * 50)

    # Resetear contadores para segunda ejecuci√≥n
    cliente_2da = CacheAPI(archivo_cache="cache_finanzas.db", ttl=30)

    for accion in acciones:
        dato = cliente_2da.obtener_dato(accion, obtener_cotizacion_accion)
        print(f"  ‚Üí {accion}: ${dato['precio']:.2f}\n")

    print(cliente_2da.estadisticas())

    # Simular expiraci√≥n del cache
    print("\n" + "=" * 50)
    print("TERCERA EJECUCI√ìN (Despu√©s de 35 segundos)")
    print("=" * 50)
    print("‚è≥ Esperando 35 segundos para que expire el cache...")
    time.sleep(35)

    cliente_3ra = CacheAPI(archivo_cache="cache_finanzas.db", ttl=30)

    for accion in acciones[:2]:  # Solo 2 para no hacer muy largo el ejemplo
        dato = cliente_3ra.obtener_dato(accion, obtener_cotizacion_accion)
        print(f"  ‚Üí {accion}: ${dato['precio']:.2f}\n")

    print(cliente_3ra.estadisticas())
```

### üì§ Output Esperado

```
==================================================
PRIMERA EJECUCI√ìN (Sin cache)
==================================================
üåê Cache MISS: AAPL - Haciendo request...
   üí∏ Cobrando $0.01...
  ‚Üí AAPL: $145.23

üåê Cache MISS: GOOGL - Haciendo request...
   üí∏ Cobrando $0.01...
  ‚Üí GOOGL: $102.45

... (3 m√°s)

üìä Estad√≠sticas de Cache:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ Cache Hits: 0
üåê Cache Misses: 5
üìà Hit Rate: 0.0%
üí∞ Requests ahorrados: 0 √ó $0.01 = $0.00

==================================================
SEGUNDA EJECUCI√ìN (Con cache - instant√°neo)
==================================================
‚úÖ Cache HIT: AAPL (edad: 3s)
  ‚Üí AAPL: $145.23

‚úÖ Cache HIT: GOOGL (edad: 3s)
  ‚Üí GOOGL: $102.45

... (3 m√°s)

üìä Estad√≠sticas de Cache:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ Cache Hits: 5
üåê Cache Misses: 0
üìà Hit Rate: 100.0%
üí∞ Requests ahorrados: 5 √ó $0.01 = $0.05

==================================================
TERCERA EJECUCI√ìN (Despu√©s de 35 segundos)
==================================================
‚è≥ Esperando 35 segundos para que expire el cache...
‚è∞ Cache EXPIRADO: AAPL (edad: 38s > TTL: 30s)
üåê Cache MISS: AAPL - Haciendo request...
   üí∏ Cobrando $0.01...
  ‚Üí AAPL: $147.89
```

### üîç Conceptos Clave

1. **`shelve`**: Base de datos simple tipo diccionario persistente
2. **TTL (Time To Live)**: Cache expira autom√°ticamente despu√©s de N segundos
3. **Cache Hit Rate**: M√©trica clave (100% = todos los requests desde cache)
4. **Persistencia**: El cache sobrevive entre ejecuciones del programa
5. **ROI**: Calcular ahorro monetario del cache ($0.05 en este ejemplo)

### üéì Lecciones Aprendidas

- ‚úÖ Cache reduce costos de APIs de pago dram√°ticamente
- ‚úÖ `shelve` es perfecto para cache simple persistente
- ‚úÖ TTL evita datos obsoletos
- ‚úÖ Segunda ejecuci√≥n es **instant√°nea** (sin esperar 0.5s por request)
- üí° **En producci√≥n:** Usar Redis para cache distribuido

---

## üìò Ejemplo 3: Async Requests con `aiohttp` (20x M√°s R√°pido)

### üéØ Objetivo
Demostrar el poder de la programaci√≥n as√≠ncrona para hacer m√∫ltiples requests HTTP en paralelo.

### üè¢ Contexto Empresarial
Necesitas actualizar el cat√°logo completo de **1,000 productos** consultando una API. Cada request tarda ~0.5 segundos.

- **S√≠ncrono:** 1,000 √ó 0.5s = 500s = **8.3 minutos** ‚è∞
- **Async (20 concurrentes):** 1,000 √∑ 20 √ó 0.5s = 25s = **25 segundos** ‚ö° (20x m√°s r√°pido)

### üíª C√≥digo Completo

```python
import aiohttp
import asyncio
import time
from typing import List, Dict

# =========================================
# VERSI√ìN S√çNCRONA (Para comparaci√≥n)
# =========================================

import requests

def scrapear_sincrono(urls: List[str]) -> List[Dict]:
    """Versi√≥n s√≠ncrona: hace requests UNO POR UNO."""
    print(f"üê¢ S√çNCRONO: Scrapeando {len(urls)} URLs...")
    inicio = time.time()

    resultados = []
    for i, url in enumerate(urls, 1):
        response = requests.get(url)
        datos = response.json()
        resultados.append(datos)

        if i % 10 == 0:
            print(f"   Progreso: {i}/{len(urls)}")

    fin = time.time()
    print(f"‚úÖ Completado en {fin-inicio:.1f}s\n")

    return resultados


# =========================================
# VERSI√ìN AS√çNCRONA (R√°pida)
# =========================================

async def obtener_url_async(session: aiohttp.ClientSession, url: str, semaforo: asyncio.Semaphore) -> dict:
    """
    Hace un request as√≠ncrono con rate limiting.

    Args:
        session: Sesi√≥n HTTP reutilizable
        url: URL a consultar
        semaforo: Limita requests concurrentes
    """
    async with semaforo:  # Limita a N requests concurrentes
        async with session.get(url) as response:
            return await response.json()


async def scrapear_async(urls: List[str], max_concurrente: int = 20) -> List[Dict]:
    """
    Versi√≥n as√≠ncrona: hace M√öLTIPLES requests EN PARALELO.

    Args:
        urls: Lista de URLs
        max_concurrente: M√°ximo n√∫mero de requests simult√°neos
    """
    print(f"‚ö° ASYNC: Scrapeando {len(urls)} URLs ({max_concurrente} concurrentes)...")
    inicio = time.time()

    # Sem√°foro para limitar concurrencia
    semaforo = asyncio.Semaphore(max_concurrente)

    # Crear sesi√≥n HTTP reutilizable
    async with aiohttp.ClientSession() as session:
        # Lanzar todos los requests EN PARALELO
        tareas = [obtener_url_async(session, url, semaforo) for url in urls]
        resultados = await asyncio.gather(*tareas)

    fin = time.time()
    print(f"‚úÖ Completado en {fin-inicio:.1f}s\n")

    return resultados


# ===============================
# COMPARACI√ìN LADO A LADO
# ===============================

def comparar_sincrono_vs_async(n_urls: int = 50):
    """Compara rendimiento s√≠ncrono vs as√≠ncrono."""
    # URLs de ejemplo (API p√∫blica)
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, n_urls + 1)]

    print("=" * 60)
    print(f"COMPARACI√ìN: Scraping de {n_urls} URLs")
    print("=" * 60)
    print()

    # 1. Versi√≥n s√≠ncrona
    inicio_sinc = time.time()
    resultados_sinc = scrapear_sincrono(urls)
    tiempo_sinc = time.time() - inicio_sinc

    # 2. Versi√≥n as√≠ncrona
    inicio_async = time.time()
    resultados_async = asyncio.run(scrapear_async(urls, max_concurrente=20))
    tiempo_async = time.time() - inicio_async

    # 3. Comparaci√≥n
    mejora = tiempo_sinc / tiempo_async

    print("=" * 60)
    print("üìä RESULTADOS:")
    print("=" * 60)
    print(f"üê¢ S√≠ncrono:  {tiempo_sinc:.1f}s")
    print(f"‚ö° Async:     {tiempo_async:.1f}s")
    print(f"üöÄ Mejora:    {mejora:.1f}x m√°s r√°pido")
    print(f"‚è±Ô∏è  Ahorro:    {tiempo_sinc - tiempo_async:.1f}s ({(1 - tiempo_async/tiempo_sinc)*100:.0f}% menos tiempo)")
    print("=" * 60)


# ===============================
# EJECUCI√ìN DEL EJEMPLO
# ===============================

if __name__ == "__main__":
    # Comparar con 50 URLs (ajusta seg√∫n tu conexi√≥n)
    comparar_sincrono_vs_async(n_urls=50)

    # Extra: Demostrar escalabilidad
    print("\n\n" + "=" * 60)
    print("üî¨ EXPERIMENTO: ¬øCu√°nto mejora con m√°s URLs?")
    print("=" * 60)
    print()

    for n in [10, 25, 50, 100]:
        urls = [f"https://jsonplaceholder.typicode.com/posts/{i % 100 + 1}" for i in range(n)]

        # S√≠ncrono
        inicio = time.time()
        _ = scrapear_sincrono(urls)
        tiempo_sinc = time.time() - inicio

        # Async
        inicio = time.time()
        _ = asyncio.run(scrapear_async(urls, max_concurrente=20))
        tiempo_async = time.time() - inicio

        mejora = tiempo_sinc / tiempo_async
        print(f"üìä {n} URLs: S√≠ncrono {tiempo_sinc:.1f}s | Async {tiempo_async:.1f}s | Mejora: {mejora:.1f}x")
```

### üì§ Output Esperado

```
============================================================
COMPARACI√ìN: Scraping de 50 URLs
============================================================

üê¢ S√çNCRONO: Scrapeando 50 URLs...
   Progreso: 10/50
   Progreso: 20/50
   Progreso: 30/50
   Progreso: 40/50
   Progreso: 50/50
‚úÖ Completado en 27.3s

‚ö° ASYNC: Scrapeando 50 URLs (20 concurrentes)...
‚úÖ Completado en 1.4s

============================================================
üìä RESULTADOS:
============================================================
üê¢ S√≠ncrono:  27.3s
‚ö° Async:     1.4s
üöÄ Mejora:    19.5x m√°s r√°pido
‚è±Ô∏è  Ahorro:    25.9s (95% menos tiempo)
============================================================


============================================================
üî¨ EXPERIMENTO: ¬øCu√°nto mejora con m√°s URLs?
============================================================

üìä 10 URLs: S√≠ncrono 5.1s | Async 0.6s | Mejora: 8.5x
üìä 25 URLs: S√≠ncrono 13.2s | Async 1.0s | Mejora: 13.2x
üìä 50 URLs: S√≠ncrono 27.8s | Async 1.5s | Mejora: 18.5x
üìä 100 URLs: S√≠ncrono 54.3s | Async 2.9s | Mejora: 18.7x
```

### üîç Conceptos Clave

1. **`async`/`await`**: Palabras clave para funciones as√≠ncronas
2. **`aiohttp`**: Librer√≠a HTTP as√≠ncrona (alternativa async a `requests`)
3. **`asyncio.Semaphore`**: Limita cu√°ntas tareas corren simult√°neamente
4. **`asyncio.gather()`**: Ejecuta m√∫ltiples coroutines en paralelo
5. **Concurrencia vs Paralelismo**: Async = concurrencia (1 thread, muchas tareas esperando I/O)

### üéì Lecciones Aprendidas

- ‚úÖ Async puede ser **10-20x m√°s r√°pido** para I/O-bound (requests HTTP)
- ‚úÖ Crucial usar `Semaphore` para limitar concurrencia (evitar sobrecarga)
- ‚úÖ M√°s URLs = mayor mejora relativa (asymptotically mejor)
- ‚ùå **Cuidado:** Async sin rate limiting = posible ban
- üí° **Mejor pr√°ctica:** Combinar async + sem√°foro + delay m√≠nimo

---

## üìò Ejemplo 4: Scraper Optimizado Completo (Async + Cache + Rate Limiting + M√©tricas)

### üéØ Objetivo
Integrar **todas** las t√©cnicas de optimizaci√≥n en un scraper de producci√≥n completo.

### üè¢ Contexto Empresarial
Necesitas scrapear 500 productos de una API para un dashboard que se actualiza cada hora. Cada request cuesta $0.01.

**Requisitos:**
- ‚úÖ M√°ximo 50 requests/minuto (cortes√≠a)
- ‚úÖ Cachear datos por 1 hora (evitar requests innecesarios)
- ‚úÖ M√°xima velocidad posible (dentro de l√≠mites)
- ‚úÖ M√©tricas detalladas (throughput, cache hit rate, costos)

### üíª C√≥digo Completo

```python
import aiohttp
import asyncio
import time
import shelve
from typing import List, Dict, Optional
from dataclasses import dataclass, field

@dataclass
class Metricas:
    """Contenedor de m√©tricas del scraper."""
    total_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    errores: int = 0
    inicio: float = field(default_factory=time.time)

    @property
    def tiempo_transcurrido(self) -> float:
        return time.time() - self.inicio

    @property
    def throughput(self) -> float:
        """Requests por segundo."""
        return self.total_requests / self.tiempo_transcurrido if self.tiempo_transcurrido > 0 else 0

    @property
    def cache_hit_rate(self) -> float:
        """Porcentaje de hits en cache."""
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total * 100 if total > 0 else 0

    @property
    def error_rate(self) -> float:
        """Porcentaje de errores."""
        return self.errores / self.total_requests * 100 if self.total_requests > 0 else 0

    @property
    def costo_estimado(self) -> float:
        """Costo en $ (asumiendo $0.01 por request real)."""
        return self.cache_misses * 0.01

    @property
    def ahorro_estimado(self) -> float:
        """Ahorro en $ gracias al cache."""
        return self.cache_hits * 0.01

    def reporte(self) -> str:
        """Genera reporte detallado."""
        return f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë          üìä M√âTRICAS DEL SCRAPER OPTIMIZADO              ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë ‚è±Ô∏è  Tiempo total:        {self.tiempo_transcurrido:>6.1f}s                      ‚ïë
‚ïë üìù Total requests:       {self.total_requests:>6}                          ‚ïë
‚ïë ‚ö° Throughput:           {self.throughput:>6.1f} req/seg                  ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë ‚úÖ Cache HITS:           {self.cache_hits:>6} ({self.cache_hit_rate:>5.1f}%)                ‚ïë
‚ïë üåê Cache MISSES:         {self.cache_misses:>6}                          ‚ïë
‚ïë ‚ùå Errores:              {self.errores:>6} ({self.error_rate:>5.2f}%)                 ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë üí∞ Costo (requests):     ${self.costo_estimado:>6.2f}                        ‚ïë
‚ïë üíµ Ahorro (cache):       ${self.ahorro_estimado:>6.2f}                        ‚ïë
‚ïë üìä ROI del cache:        {self.ahorro_estimado/(self.ahorro_estimado+self.costo_estimado)*100 if (self.ahorro_estimado+self.costo_estimado) > 0 else 0:>5.1f}%                         ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
        """


class ScraperOptimizado:
    """Scraper de producci√≥n con async, cache, rate limiting y m√©tricas."""

    def __init__(
        self,
        archivo_cache: str = "cache_scraper.db",
        ttl: int = 3600,
        max_concurrente: int = 20,
        delay_minimo: float = 0.1
    ):
        """
        Args:
            archivo_cache: Archivo del cache persistente
            ttl: Tiempo de vida del cache (segundos)
            max_concurrente: M√°ximo requests simult√°neos
            delay_minimo: Delay m√≠nimo entre requests (segundos)
        """
        self.archivo_cache = archivo_cache
        self.ttl = ttl
        self.max_concurrente = max_concurrente
        self.delay_minimo = delay_minimo
        self.metricas = Metricas()

    def _verificar_cache(self, clave: str) -> Optional[dict]:
        """Verifica si existe en cache y es v√°lido."""
        with shelve.open(self.archivo_cache) as cache:
            if clave in cache:
                datos, timestamp = cache[clave]
                edad = time.time() - timestamp

                if edad < self.ttl:
                    self.metricas.total_requests += 1
                    self.metricas.cache_hits += 1
                    return datos

        return None

    def _guardar_en_cache(self, clave: str, datos: dict):
        """Guarda datos en cache."""
        with shelve.open(self.archivo_cache) as cache:
            cache[clave] = (datos, time.time())

    async def _obtener_con_cache(
        self,
        session: aiohttp.ClientSession,
        url: str,
        semaforo: asyncio.Semaphore
    ) -> dict:
        """Obtiene datos con cache, async y rate limiting."""
        # 1. Verificar cache
        datos_cache = self._verificar_cache(url)
        if datos_cache is not None:
            return datos_cache

        # 2. Request real con rate limiting
        async with semaforo:
            try:
                # Delay m√≠nimo (cortes√≠a)
                await asyncio.sleep(self.delay_minimo)

                # Request HTTP
                async with session.get(url, timeout=10) as response:
                    response.raise_for_status()
                    datos = await response.json()

                    # Guardar en cache
                    self._guardar_en_cache(url, datos)

                    # Actualizar m√©tricas
                    self.metricas.total_requests += 1
                    self.metricas.cache_misses += 1

                    return datos

            except Exception as e:
                self.metricas.errores += 1
                print(f"‚ùå Error en {url}: {e}")
                return {"error": str(e), "url": url}

    async def scrapear(self, urls: List[str]) -> List[dict]:
        """
        Scrape masivo optimizado.

        Args:
            urls: Lista de URLs a scrapear

        Returns:
            Lista de datos scrapeados
        """
        print(f"üöÄ Iniciando scraper optimizado")
        print(f"üìù URLs: {len(urls)}")
        print(f"‚ö° Concurrencia: {self.max_concurrente}")
        print(f"‚è±Ô∏è  Delay m√≠nimo: {self.delay_minimo}s")
        print(f"üíæ Cache TTL: {self.ttl}s ({self.ttl/3600:.1f}h)")
        print()

        # Sem√°foro para limitar concurrencia
        semaforo = asyncio.Semaphore(self.max_concurrente)

        # Sesi√≥n HTTP reutilizable
        async with aiohttp.ClientSession() as session:
            tareas = [
                self._obtener_con_cache(session, url, semaforo)
                for url in urls
            ]
            resultados = await asyncio.gather(*tareas)

        return resultados


# ===============================
# DEMOSTRACI√ìN COMPLETA
# ===============================

async def demo_completa():
    """Demuestra el scraper optimizado con m√∫ltiples ejecuciones."""
    # URLs de ejemplo (API p√∫blica)
    urls = [f"https://jsonplaceholder.typicode.com/posts/{i}" for i in range(1, 101)]

    # Crear scraper
    scraper = ScraperOptimizado(
        archivo_cache="demo_cache.db",
        ttl=60,  # Cache de 1 minuto (para demo)
        max_concurrente=20,
        delay_minimo=0.05  # 50ms delay
    )

    print("=" * 70)
    print("PRIMERA EJECUCI√ìN (Sin cache - todos los requests son reales)")
    print("=" * 70)
    print()

    resultados_1 = await scraper.scrapear(urls)
    print(scraper.metricas.reporte())

    # Segunda ejecuci√≥n (deber√≠a usar cache al 100%)
    print("\n" + "=" * 70)
    print("SEGUNDA EJECUCI√ìN (Con cache - deber√≠a ser instant√°neo)")
    print("=" * 70)
    print()

    scraper_2 = ScraperOptimizado(
        archivo_cache="demo_cache.db",
        ttl=60,
        max_concurrente=20,
        delay_minimo=0.05
    )

    resultados_2 = await scraper_2.scrapear(urls)
    print(scraper_2.metricas.reporte())

    # Comparaci√≥n
    mejora = scraper.metricas.tiempo_transcurrido / scraper_2.metricas.tiempo_transcurrido

    print("\n" + "=" * 70)
    print("üìä COMPARACI√ìN:")
    print("=" * 70)
    print(f"1ra ejecuci√≥n: {scraper.metricas.tiempo_transcurrido:.1f}s (sin cache)")
    print(f"2da ejecuci√≥n: {scraper_2.metricas.tiempo_transcurrido:.1f}s (100% cache)")
    print(f"Mejora: {mejora:.0f}x m√°s r√°pido")
    print(f"Ahorro acumulado: ${scraper_2.metricas.ahorro_estimado:.2f}")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(demo_completa())
```

### üì§ Output Esperado

```
======================================================================
PRIMERA EJECUCI√ìN (Sin cache - todos los requests son reales)
======================================================================

üöÄ Iniciando scraper optimizado
üìù URLs: 100
‚ö° Concurrencia: 20
‚è±Ô∏è  Delay m√≠nimo: 0.05s
üíæ Cache TTL: 60s (0.0h)

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë          üìä M√âTRICAS DEL SCRAPER OPTIMIZADO              ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë ‚è±Ô∏è  Tiempo total:           8.3s                         ‚ïë
‚ïë üìù Total requests:          100                          ‚ïë
‚ïë ‚ö° Throughput:             12.0 req/seg                   ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë ‚úÖ Cache HITS:                0 (  0.0%)                  ‚ïë
‚ïë üåê Cache MISSES:            100                          ‚ïë
‚ïë ‚ùå Errores:                   0 ( 0.00%)                  ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë üí∞ Costo (requests):      $ 1.00                         ‚ïë
‚ïë üíµ Ahorro (cache):        $ 0.00                         ‚ïë
‚ïë üìä ROI del cache:          0.0%                          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

======================================================================
SEGUNDA EJECUCI√ìN (Con cache - deber√≠a ser instant√°neo)
======================================================================

üöÄ Iniciando scraper optimizado
üìù URLs: 100
‚ö° Concurrencia: 20
‚è±Ô∏è  Delay m√≠nimo: 0.05s
üíæ Cache TTL: 60s (0.0h)

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë          üìä M√âTRICAS DEL SCRAPER OPTIMIZADO              ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë ‚è±Ô∏è  Tiempo total:           0.2s                         ‚ïë
‚ïë üìù Total requests:          100                          ‚ïë
‚ïë ‚ö° Throughput:           500.0 req/seg                    ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë ‚úÖ Cache HITS:              100 (100.0%)                  ‚ïë
‚ïë üåê Cache MISSES:              0                          ‚ïë
‚ïë ‚ùå Errores:                   0 ( 0.00%)                  ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë üí∞ Costo (requests):      $ 0.00                         ‚ïë
‚ïë üíµ Ahorro (cache):        $ 1.00                         ‚ïë
‚ïë üìä ROI del cache:        100.0%                          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

======================================================================
üìä COMPARACI√ìN:
======================================================================
1ra ejecuci√≥n: 8.3s (sin cache)
2da ejecuci√≥n: 0.2s (100% cache)
Mejora: 42x m√°s r√°pido
Ahorro acumulado: $1.00
======================================================================
```

### üîç Conceptos Clave Integrados

1. **Async + Semaphore**: M√°xima velocidad con control de concurrencia
2. **Cache persistente**: Ahorra requests entre ejecuciones
3. **TTL**: Datos siempre frescos (expiraci√≥n autom√°tica)
4. **M√©tricas completas**: Throughput, hit rate, costos, ROI
5. **Manejo de errores**: No detiene el scraping completo si falla 1 URL

### üéì Lecciones Aprendidas

- ‚úÖ **Primera ejecuci√≥n:** 8.3s, $1.00 de costo
- ‚úÖ **Segunda ejecuci√≥n:** 0.2s, $0 de costo (100% cache)
- ‚úÖ **Mejora:** 42x m√°s r√°pido en segunda ejecuci√≥n
- ‚úÖ **ROI del cache:** 100% (todo el costo evitado)
- ‚úÖ **Throughput:** De 12 req/seg (sin cache) a 500 req/seg (con cache)
- üí° **Producci√≥n:** Este patr√≥n es ideal para pipelines ETL que corren cada hora

---

## üéØ Resumen y Pr√≥ximos Pasos

### Lo que aprendiste:

| Ejemplo | T√©cnica              | Mejora             | Caso de Uso     |
| ------- | -------------------- | ------------------ | --------------- |
| **1**   | Rate limiting b√°sico | √âtico ‚úÖ            | Scraping cort√©s |
| **2**   | Cache persistente    | 90% menos requests | APIs de pago    |
| **3**   | Async requests       | 20x m√°s r√°pido     | Volumen masivo  |
| **4**   | Combo completo       | 42x + 100% ROI     | Producci√≥n      |

### Pr√≥ximos pasos:

1. ‚úÖ **Ejercicios:** Resuelve los 12 ejercicios del archivo `03-EJERCICIOS.md`
2. ‚úÖ **Proyecto pr√°ctico:** Implementa un scraper optimizado completo con TDD
3. ‚úÖ **Experimenta:** Prueba diferentes configuraciones (TTL, concurrencia, delay)
4. ‚úÖ **Mide:** Siempre mide throughput, cache hit rate y costos

---

**√öltima actualizaci√≥n:** 2025-10-24
**Tiempo total:** 45-60 minutos de lectura/pr√°ctica
**Nivel:** Intermedio-Avanzado
