# Tema 2: GCP para Data Engineering

## ðŸ“‹ InformaciÃ³n del Tema

- **DuraciÃ³n estimada:** 3-4 semanas
- **Nivel:** Intermedio-Avanzado
- **Prerrequisitos:** Tema 1 (AWS) completado, conocimientos de SQL
- **Proyecto prÃ¡ctico:** Pipeline de datos en tiempo real con GCP

---

## ðŸŽ¯ Objetivos de Aprendizaje

Al completar este tema, serÃ¡s capaz de:

- âœ… Almacenar y gestionar datos en **Cloud Storage**
- âœ… Ejecutar queries analÃ­ticas SQL con **BigQuery**
- âœ… Procesar datos batch y streaming con **Dataflow** (Apache Beam)
- âœ… Implementar ingesta en tiempo real con **Pub/Sub**
- âœ… Orquestar workflows con **Cloud Composer** (Apache Airflow managed)
- âœ… Calcular costos y optimizar uso de GCP
- âœ… Aplicar mejores prÃ¡cticas de seguridad en GCP

---

## ðŸ“š Contenido

### 1. IntroducciÃ³n a GCP para Data Engineering
### 2. Cloud Storage: El Data Lake de GCP
### 3. BigQuery: Data Warehouse Serverless
### 4. Dataflow: Procesamiento con Apache Beam
### 5. Pub/Sub: MensajerÃ­a para Streaming
### 6. Cloud Composer: OrquestaciÃ³n Managed

---

## 1. IntroducciÃ³n a GCP para Data Engineering

### Â¿QuÃ© es Google Cloud Platform (GCP)?

**Google Cloud Platform** es la plataforma cloud de Google que ofrece servicios de computaciÃ³n, almacenamiento, bases de datos, machine learning y mÃ¡s.

**Â¿Por quÃ© GCP para Data Engineering?**

GCP destaca especialmente en:
- **BigQuery**: El data warehouse mÃ¡s rÃ¡pido y escalable del mercado
- **IntegraciÃ³n nativa**: Todos los servicios estÃ¡n diseÃ±ados para trabajar juntos
- **Serverless por defecto**: Menos gestiÃ³n de infraestructura
- **Machine Learning**: IntegraciÃ³n profunda con TensorFlow y Vertex AI
- **Costos competitivos**: Descuentos automÃ¡ticos por uso sostenido

### ComparaciÃ³n GCP vs AWS

| CaracterÃ­stica | GCP | AWS |
|----------------|-----|-----|
| **Data Warehouse** | BigQuery (serverless) | Redshift (con servidores) |
| **Procesamiento** | Dataflow (managed Beam) | Glue + EMR |
| **Object Storage** | Cloud Storage | S3 |
| **Streaming** | Pub/Sub | Kinesis |
| **OrquestaciÃ³n** | Cloud Composer (Airflow) | MWAA (Airflow) |
| **FilosofÃ­a** | Serverless-first | MÃ¡s opciones, mÃ¡s complejidad |

**AnalogÃ­a:** Si AWS es como un supermercado gigante con 1000 productos, GCP es como un restaurante gourmet con 50 platos perfectamente ejecutados.

---

## 2. Cloud Storage: El Data Lake de GCP

### Â¿QuÃ© es Cloud Storage?

**Cloud Storage** es el servicio de almacenamiento de objetos de GCP, equivalente a Amazon S3.

### AnalogÃ­a: Cloud Storage como un almacÃ©n gigante

Imagina Cloud Storage como un **almacÃ©n de Amazon**:
- **Buckets**: Son como los almacenes fÃ­sicos (uno en Madrid, otro en Barcelona)
- **Objetos**: Son las cajas que guardas (archivos CSV, JSON, Parquet)
- **Clases de almacenamiento**: Como diferentes secciones del almacÃ©n:
  - **Standard**: Caja en estanterÃ­a de acceso rÃ¡pido (0.01â‚¬/GB/mes mÃ¡s operaciones)
  - **Nearline**: Caja en sÃ³tano nivel 1 (0.004â‚¬/GB/mes, acces cada 30 dÃ­as)
  - **Coldline**: Caja en sÃ³tano nivel 2 (0.002â‚¬/GB/mes, acceso cada 90 dÃ­as)
  - **Archive**: Caja en almacÃ©n externo (0.0005â‚¬/GB/mes, acceso cada 365 dÃ­as)

### Conceptos Clave

#### 1. Buckets (Cubos)

```python
# Crear un bucket
from google.cloud import storage

client = storage.Client()
bucket = client.create_bucket("cloudapi-data-lake-prod", location="europe-west1")

# Configurar clase de almacenamiento
bucket.storage_class = "STANDARD"
bucket.patch()
```

**Naming rules:**
- Globalmente Ãºnico (como dominios web)
- Solo minÃºsculas, nÃºmeros, guiones
- Entre 3-63 caracteres

#### 2. Objetos (Objects)

```python
# Subir archivo
bucket = client.bucket("cloudapi-data-lake-prod")
blob = bucket.blob("data/raw/2025/01/15/logs.csv")
blob.upload_from_filename("local_logs.csv")

# Descargar archivo
blob.download_to_filename("downloaded_logs.csv")
```

#### 3. Lifecycle Policies

**Automatiza la gestiÃ³n de datos:**

```json
{
  "rule": [
    {
      "action": {"type": "SetStorageClass", "storageClass": "NEARLINE"},
      "condition": {"age": 30}
    },
    {
      "action": {"type": "SetStorageClass", "storageClass": "COLDLINE"},
      "condition": {"age": 90}
    },
    {
      "action": {"type": "Delete"},
      "condition": {"age": 365}
    }
  ]
}
```

**TraducciÃ³n:**
- DespuÃ©s de 30 dÃ­as â†’ Mover a Nearline (mÃ¡s barato)
- DespuÃ©s de 90 dÃ­as â†’ Mover a Coldline (aÃºn mÃ¡s barato)
- DespuÃ©s de 365 dÃ­as â†’ Eliminar (liberar espacio)

### Casos de Uso Reales

**1. Data Lake para RestaurantData Co.**

```
gs://restaurantdata-datalake/
â”œâ”€â”€ raw/                    # Datos sin procesar (Standard)
â”‚   â”œâ”€â”€ 2025/01/15/
â”‚   â”‚   â””â”€â”€ ventas.csv
â”‚   â””â”€â”€ 2025/01/16/
â”œâ”€â”€ processed/              # Datos procesados (Standard)
â”‚   â””â”€â”€ 2025/01/15/
â”‚       â””â”€â”€ ventas.parquet
â””â”€â”€ archive/                # Datos histÃ³ricos (Archive)
    â””â”€â”€ 2024/
```

**2. OptimizaciÃ³n de costos**

Dato real: **10 TB de datos histÃ³ricos**

| Estrategia | Costo mensual |
|------------|---------------|
| Todo en Standard | 10,000 GB Ã— 0.01â‚¬ = **100â‚¬** |
| 1 TB Standard + 9 TB Archive | (1000Ã—0.01) + (9000Ã—0.0005) = **14.50â‚¬** |
| **Ahorro:** | **85.50â‚¬/mes = 1,026â‚¬/aÃ±o** |

---

## 3. BigQuery: Data Warehouse Serverless

### Â¿QuÃ© es BigQuery?

**BigQuery** es el data warehouse completamente serverless de GCP. Es el servicio estrella para analytics.

### AnalogÃ­a: BigQuery como una biblioteca mÃ¡gica

Imagina una **biblioteca infinita**:
- **No necesitas estanterÃ­as**: No gestionas servidores (serverless)
- **BÃºsqueda instantÃ¡nea**: Queries sobre petabytes en segundos
- **Pagas por leer**: Solo pagas por los datos escaneados, no por almacenamiento de infraestructura
- **SeparaciÃ³n storage/compute**: Almacenamiento barato, compute solo cuando lo usas

### Conceptos Clave

#### 1. Datasets y Tablas

```sql
-- Crear dataset
CREATE SCHEMA IF NOT EXISTS `cloudapi_analytics`
OPTIONS(
  location="europe-west1",
  description="Analytics para CloudAPI Systems"
);

-- Crear tabla
CREATE TABLE `cloudapi_analytics.logs_api` (
  timestamp TIMESTAMP,
  endpoint STRING,
  method STRING,
  status_code INT64,
  response_time_ms FLOAT64,
  user_id STRING,
  ip_address STRING
)
PARTITION BY DATE(timestamp)
CLUSTER BY endpoint, status_code;
```

#### 2. Particionamiento (Partitioning)

**Â¿Por quÃ© particionar?**

Reduce costos escaneando solo las particiones necesarias.

**Ejemplo sin particionamiento:**
```sql
-- Escanea 100 GB (todo el histÃ³rico)
SELECT * FROM logs_api
WHERE timestamp >= '2025-01-15'
  AND timestamp < '2025-01-16';

-- Costo: 100 GB Ã— $5/TB = $0.50
```

**Ejemplo con particionamiento por fecha:**
```sql
-- Escanea solo 500 MB (una particiÃ³n)
SELECT * FROM logs_api
WHERE timestamp >= '2025-01-15'
  AND timestamp < '2025-01-16';

-- Costo: 0.5 GB Ã— $5/TB = $0.0025 (200x mÃ¡s barato!)
```

#### 3. Clustering

Organiza datos dentro de cada particiÃ³n para queries aÃºn mÃ¡s rÃ¡pidas.

```sql
CREATE TABLE logs_api
PARTITION BY DATE(timestamp)
CLUSTER BY endpoint, status_code;  -- Agrupa por estos campos
```

**Beneficio:** Queries filtradas por `endpoint` escanean menos datos.

#### 4. Queries SQL

BigQuery usa **SQL estÃ¡ndar** con extensiones potentes:

```sql
-- Calcular percentiles de response time
SELECT
  endpoint,
  APPROX_QUANTILES(response_time_ms, 100)[OFFSET(50)] AS p50,
  APPROX_QUANTILES(response_time_ms, 100)[OFFSET(95)] AS p95,
  APPROX_QUANTILES(response_time_ms, 100)[OFFSET(99)] AS p99,
  COUNT(*) AS total_requests
FROM `cloudapi_analytics.logs_api`
WHERE DATE(timestamp) = '2025-01-15'
GROUP BY endpoint
ORDER BY p99 DESC;
```

### Costos de BigQuery

**2 tipos de costos:**

1. **Almacenamiento** (muy barato):
   - Active storage: $0.02/GB/mes
   - Long-term storage (>90 dÃ­as sin modificar): $0.01/GB/mes

2. **Queries** (pagas por datos escaneados):
   - On-demand: $5 por TB escaneado
   - Flat-rate: $2,000/mes por 100 slots (para uso intensivo)

**Ejemplo real de CloudAPI Systems:**

```
Datos: 1 TB de logs
Queries diarias: 10 queries escaneando 10 GB cada una

Costo mensual:
- Storage: 1000 GB Ã— $0.02 = $20
- Queries: (10 queries Ã— 10 GB Ã— 30 dÃ­as) / 1000 GB Ã— $5 = $15
- TOTAL: $35/mes

Con particionamiento (escanea solo 1 GB por query):
- Storage: $20
- Queries: (10 Ã— 1 GB Ã— 30) / 1000 Ã— $5 = $1.50
- TOTAL: $21.50/mes (38% ahorro)
```

### Optimizaciones Clave

#### 1. Solo SELECT columnas necesarias

```sql
-- âŒ MAL: Escanea toda la tabla
SELECT * FROM logs_api;

-- âœ… BIEN: Escanea solo 3 columnas
SELECT endpoint, status_code, response_time_ms
FROM logs_api;
```

#### 2. Usa LIMIT con precauciÃ³n

```sql
-- âŒ MAL: LIMIT no reduce datos escaneados
SELECT * FROM logs_api LIMIT 100;  -- Escanea toda la tabla

-- âœ… BIEN: Filtra primero
SELECT * FROM logs_api
WHERE DATE(timestamp) = CURRENT_DATE()
LIMIT 100;
```

#### 3. Materializa resultados frecuentes

```sql
-- Crear tabla materializada (se actualiza automÃ¡ticamente)
CREATE MATERIALIZED VIEW daily_metrics AS
SELECT
  DATE(timestamp) AS dia,
  endpoint,
  COUNT(*) AS requests,
  AVG(response_time_ms) AS avg_response_time
FROM logs_api
GROUP BY dia, endpoint;
```

---

## 4. Dataflow: Procesamiento con Apache Beam

### Â¿QuÃ© es Dataflow?

**Dataflow** es el servicio managed de GCP para ejecutar pipelines de **Apache Beam**.

### AnalogÃ­a: Dataflow como una fÃ¡brica automatizada

Imagina una **fÃ¡brica de Toyota** (procesamiento batch) que tambiÃ©n puede funcionar **24/7** (streaming):
- **Apache Beam**: El plano de la fÃ¡brica (tu cÃ³digo Python)
- **Dataflow**: La fÃ¡brica fÃ­sica que ejecuta el plano (infraestructura)
- **Workers**: Los trabajadores (mÃ¡quinas que GCP gestiona automÃ¡ticamente)
- **Autoscaling**: Contratar/despedir trabajadores segÃºn la carga

### Conceptos Clave

#### 1. Pipeline BÃ¡sico de Apache Beam

```python
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

# Definir opciones para Dataflow
options = PipelineOptions(
    project='mi-proyecto-gcp',
    job_name='procesar-logs-api',
    temp_location='gs://mi-bucket/temp',
    region='europe-west1',
    runner='DataflowRunner'  # Ejecutar en Dataflow (no local)
)

# Definir pipeline
with beam.Pipeline(options=options) as pipeline:
    (
        pipeline
        | 'Leer de GCS' >> beam.io.ReadFromText('gs://logs-raw/*.csv')
        | 'Parsear CSV' >> beam.Map(parse_csv_line)
        | 'Filtrar vÃ¡lidos' >> beam.Filter(lambda x: x['status_code'] < 500)
        | 'Calcular mÃ©tricas' >> beam.CombinePerKey(calcular_promedio)
        | 'Escribir a BigQuery' >> beam.io.WriteToBigQuery(
              table='cloudapi_analytics.metricas_diarias',
              schema='endpoint:STRING,avg_response_time:FLOAT'
          )
    )
```

#### 2. Transformaciones Comunes

**Map**: 1 input â†’ 1 output
```python
| 'Convertir a mayÃºsculas' >> beam.Map(lambda x: x.upper())
```

**Filter**: Mantener solo elementos que cumplan condiciÃ³n
```python
| 'Solo Ã©xitos' >> beam.Filter(lambda x: x['status_code'] == 200)
```

**GroupByKey**: Agrupar por clave
```python
| 'Agrupar por endpoint' >> beam.GroupByKey()
```

**CombinePerKey**: Agregar valores por clave
```python
| 'Promedio por endpoint' >> beam.CombinePerKey(beam.combiners.MeanCombineFn())
```

#### 3. Windowing para Streaming

Cuando procesas datos en tiempo real, necesitas **ventanas** para agregar:

```python
from apache_beam import window

(
    pipeline
    | 'Leer de Pub/Sub' >> beam.io.ReadFromPubSub(topic='projects/my-project/topics/logs')
    | 'Parsear JSON' >> beam.Map(json.loads)
    | 'Ventanas de 1 minuto' >> beam.WindowInto(window.FixedWindows(60))
    | 'Contar por endpoint' >> beam.combiners.Count.PerKey()
    | 'Escribir resultados' >> beam.io.WriteToBigQuery(...)
)
```

**Tipos de ventanas:**
- **Fixed**: Ventanas de tamaÃ±o fijo (ej: cada minuto)
- **Sliding**: Ventanas deslizantes con overlap
- **Session**: Ventanas basadas en actividad del usuario

### Batch vs Streaming

| Aspecto | Batch | Streaming |
|---------|-------|-----------|
| **Entrada** | Archivos en GCS | Pub/Sub topics |
| **Procesamiento** | Todo de una vez | Continuo (24/7) |
| **Latencia** | Minutos-horas | Segundos |
| **Costo** | MÃ¡s barato | MÃ¡s caro (workers siempre activos) |
| **Caso de uso** | Reportes diarios | Dashboards en tiempo real |

---

## 5. Pub/Sub: MensajerÃ­a para Streaming

### Â¿QuÃ© es Pub/Sub?

**Pub/Sub** (Publish/Subscribe) es el servicio de mensajerÃ­a de GCP para comunicar sistemas en tiempo real.

### AnalogÃ­a: Pub/Sub como un sistema de correo

Imagina el **sistema de correo postal**:
- **Publishers**: Personas que envÃ­an cartas (productores de datos)
- **Topics**: Buzones especÃ­ficos (ej: "buzÃ³n de logs", "buzÃ³n de ventas")
- **Subscriptions**: Suscripciones para recibir copias (varios consumidores pueden suscribirse al mismo topic)
- **Subscribers**: Personas que reciben las cartas (consumidores de datos)

### Conceptos Clave

#### 1. Topics (Temas)

```python
from google.cloud import pubsub_v1

publisher = pubsub_v1.PublisherClient()
topic_path = publisher.topic_path('mi-proyecto', 'logs-api')

# Crear topic
publisher.create_topic(request={"name": topic_path})
```

#### 2. Publicar Mensajes

```python
import json

# Publicar un log
log_data = {
    'timestamp': '2025-01-15T10:30:45Z',
    'endpoint': '/api/users',
    'status_code': 200,
    'response_time_ms': 145
}

# Convertir a bytes
data = json.dumps(log_data).encode('utf-8')

# Publicar
future = publisher.publish(topic_path, data)
message_id = future.result()  # Esperar confirmaciÃ³n

print(f"Mensaje publicado con ID: {message_id}")
```

#### 3. Subscriptions (Suscripciones)

```python
subscriber = pubsub_v1.SubscriberClient()
subscription_path = subscriber.subscription_path('mi-proyecto', 'procesar-logs')

# Crear subscription
subscriber.create_subscription(
    request={
        "name": subscription_path,
        "topic": topic_path,
        "ack_deadline_seconds": 60  # Timeout para procesar
    }
)
```

#### 4. Consumir Mensajes

```python
def callback(message):
    """FunciÃ³n que procesa cada mensaje"""
    log = json.loads(message.data.decode('utf-8'))

    # Procesar log
    if log['status_code'] >= 500:
        print(f"ERROR: {log['endpoint']} retornÃ³ {log['status_code']}")

    # Confirmar procesamiento (ACK)
    message.ack()

# Suscribirse y escuchar mensajes
streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)

try:
    streaming_pull_future.result()  # Bloquea y escucha infinitamente
except KeyboardInterrupt:
    streaming_pull_future.cancel()
```

### At-Least-Once Delivery

**Pub/Sub garantiza** que cada mensaje se entrega **al menos una vez**:
- âœ… **Ventaja**: Nunca pierdes mensajes
- âš ï¸ **Cuidado**: Puedes recibir duplicados

**SoluciÃ³n**: Hacer tu procesamiento **idempotente**:
```python
# âŒ MAL: Incrementar contador
contador += 1

# âœ… BIEN: Usar ID Ãºnico para deduplicar
if mensaje_id not in procesados:
    contador += 1
    procesados.add(mensaje_id)
```

### Casos de Uso Reales

**1. Ingesta en tiempo real**
```
API REST â†’ Pub/Sub Topic "logs-api" â†’ Dataflow â†’ BigQuery
```

**2. Arquitectura Event-Driven**
```
Usuario hace pedido â†’ Pub/Sub "pedidos" â†’ 3 subscriptions:
  - Subscription "inventario" â†’ Actualizar stock
  - Subscription "facturacion" â†’ Generar factura
  - Subscription "analytics" â†’ Registrar mÃ©trica
```

**3. Backpressure**

Pub/Sub actÃºa como **buffer** cuando el consumidor es mÃ¡s lento que el productor:
```
API produce 1000 msg/s â†’ Pub/Sub â†’ Dataflow procesa 500 msg/s
                           â†‘ Buffer (hasta 7 dÃ­as de retenciÃ³n)
```

---

## 6. Cloud Composer: OrquestaciÃ³n Managed

### Â¿QuÃ© es Cloud Composer?

**Cloud Composer** es **Apache Airflow completamente managed** por GCP.

### AnalogÃ­a: Cloud Composer como un director de orquesta

Imagina un **director de orquesta**:
- **DAG (Directed Acyclic Graph)**: La partitura musical
- **Tasks**: Cada instrumento (violÃ­n, piano, flauta)
- **Operators**: Los mÃºsicos que tocan cada instrumento
- **Scheduler**: El director que indica cuÃ¡ndo toca cada uno
- **Cloud Composer**: El teatro que proporciona todo (infraestructura)

### Conceptos Clave

#### 1. DAG BÃ¡sico

```python
from airflow import DAG
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'email_on_failure': True,
    'email': ['alerts@cloudapi.com'],
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'pipeline_diario_logs',
    default_args=default_args,
    description='Procesa logs diarios de API',
    schedule_interval='0 2 * * *',  # 2 AM diario
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['logs', 'analytics'],
) as dag:

    # Task 1: Cargar CSV de GCS a BigQuery
    load_to_bq = GCSToBigQueryOperator(
        task_id='load_csv_to_bigquery',
        bucket='cloudapi-logs-raw',
        source_objects=['{{ ds }}/logs-*.csv'],  # Fecha de ejecuciÃ³n
        destination_project_dataset_table='cloudapi_analytics.logs_raw',
        write_disposition='WRITE_TRUNCATE',
        skip_leading_rows=1,
    )

    # Task 2: Limpiar y transformar datos
    transform = BigQueryInsertJobOperator(
        task_id='transform_data',
        configuration={
            'query': {
                'query': '''
                    INSERT INTO cloudapi_analytics.logs_processed
                    SELECT
                      timestamp,
                      endpoint,
                      method,
                      status_code,
                      response_time_ms
                    FROM cloudapi_analytics.logs_raw
                    WHERE status_code IS NOT NULL
                      AND response_time_ms > 0
                ''',
                'useLegacySql': False,
            }
        },
    )

    # Task 3: Calcular mÃ©tricas agregadas
    calculate_metrics = BigQueryInsertJobOperator(
        task_id='calculate_daily_metrics',
        configuration={
            'query': {
                'query': '''
                    INSERT INTO cloudapi_analytics.metricas_diarias
                    SELECT
                      DATE(timestamp) as dia,
                      endpoint,
                      COUNT(*) as total_requests,
                      AVG(response_time_ms) as avg_response_time,
                      APPROX_QUANTILES(response_time_ms, 100)[OFFSET(95)] as p95
                    FROM cloudapi_analytics.logs_processed
                    WHERE DATE(timestamp) = '{{ ds }}'
                    GROUP BY dia, endpoint
                ''',
                'useLegacySql': False,
            }
        },
    )

    # Definir dependencias
    load_to_bq >> transform >> calculate_metrics
```

#### 2. Operators Comunes

**GCP Operators:**
- `BigQueryInsertJobOperator`: Ejecutar queries SQL
- `GCSToBigQueryOperator`: Cargar CSV/JSON a BigQuery
- `DataflowTemplatedJobStartOperator`: Ejecutar job de Dataflow
- `PubSubPublishMessageOperator`: Publicar mensajes a Pub/Sub

**Sensores:**
- `GCSObjectExistenceSensor`: Esperar a que exista un archivo
- `BigQueryTableExistenceSensor`: Esperar a que exista una tabla

#### 3. XComs para pasar datos entre tasks

```python
# Task 1: Calcular y retornar resultado
def calcular_total(**context):
    total = 12345
    context['ti'].xcom_push(key='total_registros', value=total)

# Task 2: Leer resultado de Task 1
def enviar_notificacion(**context):
    total = context['ti'].xcom_pull(key='total_registros', task_ids='calcular_total')
    print(f"Se procesaron {total} registros")
```

### Mejores PrÃ¡cticas

#### 1. Idempotencia

Tu DAG debe poder ejecutarse mÃºltiples veces sin efectos secundarios:

```sql
-- âŒ MAL: Duplica datos en cada ejecuciÃ³n
INSERT INTO tabla
SELECT * FROM otra_tabla;

-- âœ… BIEN: Reemplaza datos de la fecha especÃ­fica
DELETE FROM tabla WHERE DATE(timestamp) = '{{ ds }}';
INSERT INTO tabla
SELECT * FROM otra_tabla WHERE DATE(timestamp) = '{{ ds }}';

-- âœ… MEJOR: Usa MERGE para upsert
MERGE tabla AS target
USING otra_tabla AS source
ON target.id = source.id AND DATE(target.timestamp) = '{{ ds }}'
WHEN MATCHED THEN UPDATE SET ...
WHEN NOT MATCHED THEN INSERT ...;
```

#### 2. ParametrizaciÃ³n con Jinja

```python
query = '''
    SELECT * FROM logs
    WHERE DATE(timestamp) = '{{ ds }}'           -- Fecha de ejecuciÃ³n
      AND timestamp >= '{{ ds }} 00:00:00'
      AND timestamp < '{{ next_ds }} 00:00:00'   -- Siguiente dÃ­a
'''
```

#### 3. Alertas

```python
def alert_on_failure(context):
    """EnvÃ­a alerta cuando un task falla"""
    task_instance = context.get('task_instance')
    dag_id = context.get('dag_id')

    # Enviar a Slack, email, PagerDuty, etc.
    send_slack_message(
        f"âŒ Task {task_instance.task_id} fallÃ³ en DAG {dag_id}"
    )

default_args = {
    'on_failure_callback': alert_on_failure,
}
```

---

## ðŸ“Š ComparaciÃ³n de Servicios GCP

| Servicio | Equivalente AWS | CuÃ¡ndo Usarlo | Costo Relativo |
|----------|-----------------|---------------|----------------|
| **Cloud Storage** | S3 | Data lake, almacenamiento de objetos | $ |
| **BigQuery** | Redshift/Athena | Data warehouse, analytics SQL | $$ |
| **Dataflow** | Glue + EMR | Procesamiento batch/streaming escalable | $$$ |
| **Pub/Sub** | Kinesis/SQS | Ingesta en tiempo real, event-driven | $ |
| **Cloud Composer** | MWAA | Orquestar workflows complejos | $$$ |

---

## ðŸ’° EstimaciÃ³n de Costos - Proyecto Real

**CloudAPI Systems - Pipeline completo GCP:**

### Arquitectura
```
API REST (1000 req/s) â†’ Pub/Sub â†’ Dataflow â†’ BigQuery
                                     â†“
                               Cloud Storage (backup)
                                     â†“
                           Cloud Composer (orquestaciÃ³n)
```

### Costos Mensuales (estimado)

| Servicio | Uso | Costo |
|----------|-----|-------|
| **Cloud Storage** | 500 GB | 500 Ã— $0.02 = **$10** |
| **Pub/Sub** | 1M msg/dÃ­a Ã— 30 = 30M msg | 30M Ã— $40/millÃ³n = **$12** |
| **Dataflow** | 10 workers Ã— 24h Ã— 30d | 7,200 vCPU-hours Ã— $0.056 = **$403** |
| **BigQuery Storage** | 1 TB | 1000 Ã— $0.02 = **$20** |
| **BigQuery Queries** | 100 GB/dÃ­a escaneado | 3 TB/mes Ã— $5/TB = **$15** |
| **Cloud Composer** | 1 environment (small) | **$300** |
| **TOTAL** |  | **$760/mes** |

**Con optimizaciones:**
- Usar Dataflow solo batch (no 24/7): $403 â†’ $50
- Particionar BigQuery: $15 â†’ $3
- **TOTAL optimizado: $410/mes**

---

## âœ… Checklist de Aprendizaje

Verifica que puedes hacer lo siguiente:

- [ ] Crear buckets en Cloud Storage y subir archivos
- [ ] Configurar lifecycle policies para optimizar costos
- [ ] Crear datasets y tablas en BigQuery con particionamiento
- [ ] Escribir queries SQL optimizadas para BigQuery
- [ ] DiseÃ±ar un pipeline de Apache Beam para batch
- [ ] Implementar procesamiento streaming con ventanas
- [ ] Crear topics y subscriptions en Pub/Sub
- [ ] Publicar y consumir mensajes en tiempo real
- [ ] Escribir un DAG de Airflow con operators de GCP
- [ ] Calcular costos estimados de un pipeline GCP
- [ ] Aplicar mejores prÃ¡cticas de seguridad (IAM, encryption)

---

## ðŸŽ“ PrÃ³ximos Pasos

Has completado la teorÃ­a de GCP. Ahora:

1. **Practica con ejemplos** â†’ `02-EJEMPLOS.md`
2. **Resuelve ejercicios** â†’ `03-EJERCICIOS.md`
3. **Implementa el proyecto** â†’ `04-proyecto-practico/`

**En el proyecto prÃ¡ctico construirÃ¡s:**
- Pipeline completo de ingesta â†’ procesamiento â†’ analytics
- IntegraciÃ³n de todos los servicios GCP
- Monitoreo y alertas
- OptimizaciÃ³n de costos

---

**Â¡Ã‰xito con GCP!** ðŸš€

*Ãšltima actualizaciÃ³n: 2025-11-09*
*MÃ³dulo 7 - Tema 2: GCP para Data Engineering*
