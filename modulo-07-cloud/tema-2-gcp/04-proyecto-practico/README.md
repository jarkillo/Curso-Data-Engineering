# üîß Proyecto Pr√°ctico: Analytics de Salud con GCP

## üìã Tabla de Contenidos

1. [Introducci√≥n](#introducci√≥n)
2. [Contexto del Proyecto](#contexto-del-proyecto)
3. [Arquitectura](#arquitectura)
4. [Funciones Implementadas](#funciones-implementadas)
5. [Instalaci√≥n y Setup](#instalaci√≥n-y-setup)
6. [Ejecuci√≥n](#ejecuci√≥n)
7. [Tests](#tests)
8. [Estructura del Proyecto](#estructura-del-proyecto)

---

## Introducci√≥n

Este proyecto implementa un **sistema completo de analytics de salud** usando servicios de Google Cloud Platform (GCP). El objetivo es demostrar c√≥mo construir pipelines de datos en la nube utilizando:

- ‚úÖ **Cloud Storage**: Para almacenar archivos de pacientes
- ‚úÖ **BigQuery**: Para an√°lisis de datos y queries SQL
- ‚úÖ **Dataflow (Apache Beam)**: Para procesamiento ETL
- ‚úÖ **Pub/Sub**: Para ingesta en tiempo real
- ‚úÖ **Cloud Functions**: Para automatizaci√≥n (simulado)

El proyecto sigue **Test-Driven Development (TDD)** con >80% de cobertura de tests.

---

## Contexto del Proyecto

### Empresa Ficticia: HealthTech Analytics Inc.

**HealthTech Analytics Inc.** es una startup que ayuda a hospitales y cl√≠nicas a analizar datos de pacientes para mejorar la calidad de atenci√≥n m√©dica.

### Desaf√≠o de Negocio

Los hospitales generan miles de registros diarios:
- **Registros de pacientes**: Admisiones, diagn√≥sticos, tratamientos
- **Consultas m√©dicas**: Historial de visitas, prescripciones
- **Eventos de monitoreo**: Signos vitales, alertas m√©dicas

**Problema**: Los datos est√°n dispersos en archivos CSV, necesitan:
1. **Centralizaci√≥n**: Todos los datos en un data warehouse
2. **An√°lisis en tiempo real**: Detectar pacientes de alto riesgo
3. **Reportes automatizados**: KPIs diarios para directores m√©dicos

### Soluci√≥n T√©cnica

Construir un **pipeline de datos en GCP** que:
1. Almacene archivos CSV en Cloud Storage
2. Procese datos con Dataflow (limpiar, validar, enriquecer)
3. Cargue en BigQuery para an√°lisis
4. Detecte eventos cr√≠ticos con Pub/Sub
5. Genere alertas autom√°ticas

---

## Arquitectura

### Diagrama General

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      HEALTHTECH ANALYTICS                        ‚îÇ
‚îÇ                   Pipeline de Datos en GCP                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Hospitales      ‚îÇ
                    ‚îÇ  (CSVs diarios)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             v
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Cloud Storage   ‚îÇ ‚Üê CAPA 1: ALMACENAMIENTO
                    ‚îÇ  (Data Lake)     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             v
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Dataflow        ‚îÇ ‚Üê CAPA 2: PROCESAMIENTO
                    ‚îÇ  (ETL con Beam)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 v                       v
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   BigQuery     ‚îÇ      ‚îÇ   Pub/Sub     ‚îÇ ‚Üê CAPA 3: ANALYTICS
        ‚îÇ (Data Warehouse)‚îÇ      ‚îÇ  (Eventos)    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ                       ‚îÇ
                 v                       v
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Dashboards    ‚îÇ      ‚îÇ   Alertas     ‚îÇ ‚Üê CAPA 4: CONSUMO
        ‚îÇ  (BI Tools)    ‚îÇ      ‚îÇ  (Emails)     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de Datos Detallado

```
1. INGESTA:
   Hospital ‚Üí CSV ‚Üí Cloud Storage (gs://healthtech-data/raw/YYYY-MM-DD/pacientes_*.csv)

2. PROCESAMIENTO (Dataflow):
   - Leer CSV desde Cloud Storage
   - Validar datos (nulls, formatos)
   - Limpiar (normalizar timestamps, categorizar)
   - Enriquecer (calcular edad desde fecha de nacimiento, categor√≠a de riesgo)
   - Filtrar registros inv√°lidos

3. ALMACENAMIENTO:
   - Datos limpios ‚Üí BigQuery (tabla: healthtech.pacientes)
   - Datos inv√°lidos ‚Üí Cloud Storage (gs://healthtech-data/invalid/)

4. AN√ÅLISIS (BigQuery):
   - Queries SQL para KPIs (pacientes por diagn√≥stico, edad promedio, etc.)
   - Agregaciones por fecha, hospital, diagn√≥stico

5. EVENTOS CR√çTICOS (Pub/Sub):
   - Detectar pacientes de alto riesgo
   - Publicar evento a topic: healthtech-alerts
   - Consumir evento ‚Üí Enviar email/SMS

6. AUTOMATIZACI√ìN:
   - Schedule diario en Cloud Functions (simulated)
   - Trigger: Nuevos archivos en Cloud Storage ‚Üí Iniciar Dataflow job
```

---

## Funciones Implementadas

El proyecto est√° organizado en **6 m√≥dulos principales** con funciones peque√±as y testeables.

### M√≥dulo 1: `storage_operations.py` (Cloud Storage)

Funciones para gestionar archivos en Cloud Storage.

**Funciones**:
1. `subir_archivo_a_gcs(bucket_name: str, archivo_local: str, ruta_destino: str) -> str`
   - Sube un archivo local a Cloud Storage
   - Configura metadata (tipo, fecha_subida)
   - Retorna URI del archivo (gs://...)

2. `descargar_archivo_desde_gcs(uri: str, archivo_local: str) -> None`
   - Descarga un archivo desde GCS
   - Parsea URI para extraer bucket y path

3. `listar_archivos_por_prefijo(bucket_name: str, prefijo: str) -> list[str]`
   - Lista archivos que coincidan con un prefijo
   - Ejemplo: `prefijo="raw/pacientes/2025-01-15/"` lista todos los CSVs del 15 de enero

4. `eliminar_archivo_gcs(uri: str) -> None`
   - Elimina un archivo de Cloud Storage

**Cobertura esperada**: >90%

---

### M√≥dulo 2: `validation.py` (Validaci√≥n de Datos)

Funciones para validar registros de pacientes.

**Funciones**:
1. `validar_paciente_id(paciente_id: str) -> tuple[bool, str]`
   - Valida formato de ID (ej: "P001")
   - Retorna (es_valido, mensaje_error)

2. `validar_edad(edad: Any) -> tuple[bool, str]`
   - Valida que edad sea int >= 0

3. `validar_diagnostico(diagnostico: str) -> tuple[bool, str]`
   - Valida que diagn√≥stico no est√© vac√≠o

4. `validar_fecha_nacimiento(fecha_str: str) -> tuple[bool, str]`
   - Valida formato YYYY-MM-DD
   - Valida que fecha no sea futura

5. `validar_registro_completo(registro: dict) -> tuple[bool, str]`
   - Valida que tenga todos los campos requeridos
   - Llama a validadores espec√≠ficos
   - Retorna (es_valido, mensaje_error)

**Cobertura esperada**: >95%

---

### M√≥dulo 3: `transformations.py` (Transformaciones ETL)

Funciones para limpiar y transformar datos.

**Funciones**:
1. `limpiar_nulls(datos: list[dict]) -> list[dict]`
   - Elimina registros con nulls en campos cr√≠ticos

2. `normalizar_fechas(datos: list[dict]) -> list[dict]`
   - Normaliza fechas a formato ISO 8601 (YYYY-MM-DD)

3. `calcular_edad_desde_fecha_nacimiento(fecha_nacimiento: str) -> int`
   - Calcula edad actual desde fecha de nacimiento

4. `categorizar_por_edad(edad: int) -> str`
   - Categoriza paciente: "ni√±o" (<18), "adulto" (18-64), "mayor" (65+)

5. `categorizar_nivel_riesgo(registro: dict) -> str`
   - Categoriza riesgo: "bajo", "medio", "alto"
   - Basado en edad, diagn√≥stico, signos vitales

6. `enriquecer_registro(registro: dict) -> dict`
   - Agrega campos calculados (edad, categor√≠a, riesgo)
   - Agrega timestamp de procesamiento

**Cobertura esperada**: >90%

---

### M√≥dulo 4: `bigquery_operations.py` (BigQuery)

Funciones para interactuar con BigQuery.

**Funciones**:
1. `crear_dataset(project_id: str, dataset_id: str, location: str = "US") -> bigquery.Dataset`
   - Crea un dataset en BigQuery

2. `crear_tabla_pacientes(project_id: str, dataset_id: str, tabla_id: str) -> bigquery.Table`
   - Crea tabla particionada para pacientes
   - Schema: paciente_id, nombre, edad, diagnostico, fecha_registro, etc.

3. `cargar_datos_desde_lista(project_id: str, dataset_id: str, tabla_id: str, datos: list[dict]) -> int`
   - Carga datos desde lista de diccionarios a BigQuery
   - Retorna n√∫mero de filas insertadas

4. `ejecutar_query(project_id: str, query: str) -> list[dict]`
   - Ejecuta query SQL y retorna resultados como lista de dicts

5. `obtener_distribucion_diagnosticos(project_id: str, dataset_id: str, tabla_id: str) -> list[dict]`
   - Query pre-construida para distribuci√≥n de diagn√≥sticos
   - Retorna: [{diagnostico, total, porcentaje}, ...]

6. `obtener_estadisticas_por_edad(project_id: str, dataset_id: str, tabla_id: str) -> list[dict]`
   - Query pre-construida para estad√≠sticas por rango de edad
   - Retorna: [{rango_edad, total_pacientes, diagnostico_mas_comun}, ...]

**Cobertura esperada**: >85% (algunas funciones requieren BigQuery real)

---

### M√≥dulo 5: `pubsub_operations.py` (Pub/Sub)

Funciones para mensajer√≠a con Pub/Sub.

**Funciones**:
1. `crear_topic(project_id: str, topic_id: str) -> str`
   - Crea un topic en Pub/Sub
   - Retorna nombre completo del topic

2. `crear_subscription(project_id: str, topic_id: str, subscription_id: str) -> str`
   - Crea una subscription para consumir mensajes

3. `publicar_evento(project_id: str, topic_id: str, evento: dict) -> str`
   - Publica un evento (dict) a Pub/Sub
   - Serializa a JSON
   - Retorna message_id

4. `publicar_alerta_paciente_riesgo(project_id: str, topic_id: str, paciente: dict) -> str`
   - Funci√≥n espec√≠fica para alertas de pacientes de alto riesgo
   - Formato estandarizado: {tipo_evento, paciente_id, nivel_riesgo, timestamp}

5. `consumir_mensajes_simulado(mensajes: list[bytes]) -> list[dict]`
   - Simula consumo de mensajes (para testing sin Pub/Sub real)
   - Parsea JSON y retorna lista de eventos

**Cobertura esperada**: >85%

---

### M√≥dulo 6: `beam_pipelines.py` (Dataflow con Apache Beam)

DoFns y funciones para pipelines Dataflow.

**DoFns (Beam Transforms)**:
1. `ParsearCSV(beam.DoFn)`
   - Parsea l√≠neas CSV a diccionarios

2. `ValidarRegistro(beam.DoFn)`
   - Valida registros usando funciones de `validation.py`
   - Descarta inv√°lidos

3. `EnriquecerRegistro(beam.DoFn)`
   - Enriquece registros usando funciones de `transformations.py`

4. `DetectarPacientesAltoRiesgo(beam.DoFn)`
   - Filtra pacientes de alto riesgo
   - Para publicar a Pub/Sub

5. `CalcularMetricasPorDiagnostico(beam.DoFn)`
   - Agrega m√©tricas por diagn√≥stico (count, edad promedio, etc.)

**Funciones**:
1. `crear_pipeline_batch(input_path: str, output_table: str, project_id: str) -> beam.Pipeline`
   - Crea pipeline batch completo
   - Lee CSV ‚Üí Valida ‚Üí Enriquece ‚Üí Escribe a BigQuery

2. `crear_pipeline_streaming(subscription_id: str, output_table: str, project_id: str) -> beam.Pipeline`
   - Crea pipeline streaming
   - Lee Pub/Sub ‚Üí Procesa ‚Üí Escribe a BigQuery

**Cobertura esperada**: >80% (DoFns son altamente testeables)

---

## Instalaci√≥n y Setup

### Requisitos

- **Python**: 3.9+
- **Cuenta GCP**: Con proyecto creado
- **SDK de Google Cloud**: `gcloud` CLI instalado
- **Librer√≠as Python**:
  ```
  google-cloud-storage==2.10.0
  google-cloud-bigquery==3.11.0
  google-cloud-pubsub==2.18.0
  apache-beam[gcp]==2.50.0
  pytest==7.4.0
  pytest-cov==4.1.0
  ```

### Paso 1: Clonar Repositorio

```bash
cd "E:\Curso Data Engineering\modulo-07-cloud\tema-2-gcp\04-proyecto-practico"
```

### Paso 2: Crear Entorno Virtual

```bash
python -m venv venv

# Activar (Windows)
venv\Scripts\activate

# Activar (Linux/Mac)
source venv/bin/activate
```

### Paso 3: Instalar Dependencias

```bash
pip install -r requirements.txt
```

### Paso 4: Configurar Credenciales de GCP

**Opci√≥n 1: Usar cuenta personal**
```bash
gcloud auth login
gcloud auth application-default login
gcloud config set project YOUR_PROJECT_ID
```

**Opci√≥n 2: Usar Service Account (producci√≥n)**
```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account-key.json"
```

### Paso 5: Crear Recursos en GCP

```bash
# Crear bucket
gsutil mb -l us-central1 gs://healthtech-data-YOUR_PROJECT_ID

# Crear dataset en BigQuery
bq mk --location=US healthtech_analytics
```

### Paso 6: Configurar Variables de Entorno

Crear archivo `.env`:
```bash
PROJECT_ID=your-project-id
BUCKET_NAME=healthtech-data-your-project-id
DATASET_ID=healthtech_analytics
LOCATION=us-central1
```

---

## Ejecuci√≥n

### Ejemplo 1: Subir CSV a Cloud Storage

```bash
python -m src.examples.example_01_storage
```

**C√≥digo** (`src/examples/example_01_storage.py`):
```python
from src.storage_operations import subir_archivo_a_gcs

uri = subir_archivo_a_gcs(
    bucket_name="healthtech-data-myproject",
    archivo_local="data/pacientes_sample.csv",
    ruta_destino="raw/pacientes/2025-01-15/pacientes.csv"
)

print(f"‚úÖ Archivo subido: {uri}")
```

---

### Ejemplo 2: Procesar CSV con Validaci√≥n

```bash
python -m src.examples.example_02_validation
```

**C√≥digo**:
```python
from src.validation import validar_registro_completo

registro = {
    "paciente_id": "P001",
    "nombre": "Juan P√©rez",
    "edad": 45,
    "diagnostico": "Diabetes"
}

es_valido, error = validar_registro_completo(registro)

if es_valido:
    print("‚úÖ Registro v√°lido")
else:
    print(f"‚ùå Error: {error}")
```

---

### Ejemplo 3: Cargar Datos a BigQuery

```bash
python -m src.examples.example_03_bigquery
```

**C√≥digo**:
```python
from src.bigquery_operations import cargar_datos_desde_lista

datos = [
    {"paciente_id": "P001", "nombre": "Juan P√©rez", "edad": 45, "diagnostico": "Diabetes", "fecha_registro": "2025-01-15"},
    {"paciente_id": "P002", "nombre": "Mar√≠a Garc√≠a", "edad": 32, "diagnostico": "Hipertensi√≥n", "fecha_registro": "2025-01-15"}
]

filas_insertadas = cargar_datos_desde_lista(
    project_id="healthtech-prod",
    dataset_id="healthtech_analytics",
    tabla_id="pacientes",
    datos=datos
)

print(f"‚úÖ {filas_insertadas} filas insertadas en BigQuery")
```

---

### Ejemplo 4: Pipeline Dataflow (Batch Local)

```bash
python -m src.examples.example_04_dataflow_batch
```

**C√≥digo**:
```python
from src.beam_pipelines import crear_pipeline_batch

pipeline = crear_pipeline_batch(
    input_path="data/pacientes_*.csv",
    output_table="healthtech-prod:healthtech_analytics.pacientes",
    project_id="healthtech-prod"
)

# Ejecutar localmente (DirectRunner)
pipeline.run().wait_until_finish()
```

---

### Ejemplo 5: Publicar Alertas a Pub/Sub

```bash
python -m src.examples.example_05_pubsub
```

**C√≥digo**:
```python
from src.pubsub_operations import publicar_alerta_paciente_riesgo

paciente = {
    "paciente_id": "P001",
    "nombre": "Juan P√©rez",
    "edad": 68,
    "diagnostico": "Diabetes",
    "nivel_riesgo": "alto"
}

message_id = publicar_alerta_paciente_riesgo(
    project_id="healthtech-prod",
    topic_id="healthtech-alerts",
    paciente=paciente
)

print(f"‚úÖ Alerta publicada: {message_id}")
```

---

## Tests

### Ejecutar Todos los Tests

```bash
pytest tests/ -v
```

### Ejecutar con Cobertura

```bash
pytest tests/ --cov=src --cov-report=html
```

Luego abre `htmlcov/index.html` en tu navegador para ver el reporte detallado.

### Ejecutar Tests de un M√≥dulo Espec√≠fico

```bash
pytest tests/test_validation.py -v
pytest tests/test_bigquery_operations.py -v
```

### Estructura de Tests

```
tests/
‚îú‚îÄ‚îÄ test_storage_operations.py      (25 tests)
‚îú‚îÄ‚îÄ test_validation.py               (30 tests)
‚îú‚îÄ‚îÄ test_transformations.py          (20 tests)
‚îú‚îÄ‚îÄ test_bigquery_operations.py      (15 tests, algunos mocked)
‚îú‚îÄ‚îÄ test_pubsub_operations.py        (15 tests, algunos mocked)
‚îî‚îÄ‚îÄ test_beam_pipelines.py           (20 tests, con TestPipeline)
```

**Total**: ~125 tests

---

## Estructura del Proyecto

```
04-proyecto-practico/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ storage_operations.py       (4 funciones, ~150 l√≠neas)
‚îÇ   ‚îú‚îÄ‚îÄ validation.py                (5 funciones, ~200 l√≠neas)
‚îÇ   ‚îú‚îÄ‚îÄ transformations.py           (6 funciones, ~250 l√≠neas)
‚îÇ   ‚îú‚îÄ‚îÄ bigquery_operations.py       (6 funciones, ~300 l√≠neas)
‚îÇ   ‚îú‚îÄ‚îÄ pubsub_operations.py         (5 funciones, ~200 l√≠neas)
‚îÇ   ‚îî‚îÄ‚îÄ beam_pipelines.py            (5 DoFns + 2 funciones, ~400 l√≠neas)
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_storage_operations.py   (~400 l√≠neas)
‚îÇ   ‚îú‚îÄ‚îÄ test_validation.py           (~500 l√≠neas)
‚îÇ   ‚îú‚îÄ‚îÄ test_transformations.py      (~350 l√≠neas)
‚îÇ   ‚îú‚îÄ‚îÄ test_bigquery_operations.py  (~300 l√≠neas)
‚îÇ   ‚îú‚îÄ‚îÄ test_pubsub_operations.py    (~300 l√≠neas)
‚îÇ   ‚îî‚îÄ‚îÄ test_beam_pipelines.py       (~400 l√≠neas)
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ pacientes_sample.csv         (datos de prueba)
‚îÇ   ‚îî‚îÄ‚îÄ pacientes_invalid.csv        (datos inv√°lidos para testing)
‚îÇ
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ example_01_storage.py
‚îÇ   ‚îú‚îÄ‚îÄ example_02_validation.py
‚îÇ   ‚îú‚îÄ‚îÄ example_03_bigquery.py
‚îÇ   ‚îú‚îÄ‚îÄ example_04_dataflow_batch.py
‚îÇ   ‚îî‚îÄ‚îÄ example_05_pubsub.py
‚îÇ
‚îú‚îÄ‚îÄ .env.example                     (template de configuraci√≥n)
‚îú‚îÄ‚îÄ requirements.txt                 (dependencias Python)
‚îú‚îÄ‚îÄ pytest.ini                       (configuraci√≥n de pytest)
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md                        (este archivo)
```

---

## Conceptos Clave Aplicados

### 1. Test-Driven Development (TDD)

Todos los m√≥dulos siguen el ciclo **Red ‚Üí Green ‚Üí Refactor**:
1. Escribir test que falla
2. Implementar c√≥digo m√≠nimo para que pase
3. Refactorizar manteniendo tests verdes

### 2. Programaci√≥n Funcional

- Funciones puras (sin efectos colaterales)
- No modifican par√°metros de entrada
- Retornan nuevos objetos

### 3. Tipado Expl√≠cito

Todas las funciones tienen type hints:
```python
def validar_edad(edad: Any) -> tuple[bool, str]:
    ...
```

### 4. Docstrings con Ejemplos

Todas las funciones tienen docstrings con:
- Descripci√≥n
- Args
- Returns
- Raises
- Examples (doctests)

### 5. Manejo de Errores Expl√≠cito

Funciones de validaci√≥n retornan tuplas `(bool, str)`:
```python
es_valido, mensaje_error = validar_registro_completo(registro)
```

### 6. Testing con Mocks

Para servicios externos (BigQuery, Pub/Sub), usamos mocks:
```python
@patch('google.cloud.bigquery.Client')
def test_crear_dataset(mock_client):
    ...
```

---

## Pr√≥ximos Pasos

1. **Implementar funciones faltantes** (stubs actuales)
2. **Alcanzar >90% de cobertura** en todos los m√≥dulos
3. **Deploy a GCP**: Ejecutar pipeline en Dataflow real
4. **A√±adir Cloud Composer**: Orquestar con Airflow
5. **Monitoreo**: Logs, m√©tricas, alertas

---

## Recursos Adicionales

- [Google Cloud Storage Docs](https://cloud.google.com/storage/docs)
- [BigQuery Docs](https://cloud.google.com/bigquery/docs)
- [Apache Beam Docs](https://beam.apache.org/documentation/)
- [Pub/Sub Docs](https://cloud.google.com/pubsub/docs)
- [Testing with pytest](https://docs.pytest.org/)

---

*√öltima actualizaci√≥n: 2025-01-15*
*Proyecto construido con üíô siguiendo TDD y mejores pr√°cticas de Data Engineering*
---

## üß≠ Navegaci√≥n

‚¨ÖÔ∏è **Anterior**: [03 Ejercicios](../03-EJERCICIOS.md) | ‚û°Ô∏è **Siguiente**: [Infrastructure as Code - 01 Teoria](../../tema-3-iac/01-TEORIA.md)
