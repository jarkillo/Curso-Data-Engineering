# üöÄ Proyecto Pr√°ctico - Airflow Intermedio

**Pipeline Multi-Fuente con Conceptos Avanzados de Airflow**

---

## üìã √çndice

1. [Descripci√≥n del Proyecto](#descripci√≥n-del-proyecto)
2. [Conceptos Aplicados](#conceptos-aplicados)
3. [Estructura del Proyecto](#estructura-del-proyecto)
4. [Requisitos](#requisitos)
5. [Instalaci√≥n](#instalaci√≥n)
6. [Uso](#uso)
7. [Testing](#testing)
8. [Quality Check](#quality-check)
9. [Troubleshooting](#troubleshooting)
10. [Arquitectura](#arquitectura)

---

## Descripci√≥n del Proyecto

Sistema de an√°lisis de ventas que procesa datos de **m√∫ltiples fuentes** (CSV de ventas + JSON de clientes), aplica validaciones rigurosas, calcula m√©tricas agregadas, y genera reportes con **decisiones condicionales** basadas en el volumen de ventas.

### üéØ Objetivos Pedag√≥gicos

Al completar este proyecto, dominar√°s:

‚úÖ **TaskGroups:** Organizar pipelines complejos visualmente  
‚úÖ **XComs:** Compartir metadatos entre tasks (6 XComs diferentes)  
‚úÖ **Branching:** Implementar flujos condicionales (if-else en DAGs)  
‚úÖ **Sensors:** Esperar por recursos externos sin bloquear workers  
‚úÖ **Dynamic DAGs:** Generar tasks autom√°ticamente  
‚úÖ **Templating Jinja2:** Variables din√°micas en paths y configuraciones

---

## Conceptos Aplicados

### 1Ô∏è‚É£ Sensors (FileSensor)

Esperamos a que los archivos de entrada est√©n disponibles antes de procesar:

```python
esperar_ventas = FileSensor(
    task_id="esperar_ventas_csv",
    filepath="data/input/ventas.csv",
    poke_interval=10,
    timeout=60,
    mode="reschedule"  # No bloquea el worker
)
```

**Ventaja:** El pipeline no falla si los archivos llegan con retraso.

---

### 2Ô∏è‚É£ TaskGroups

Agrupamos tasks relacionadas para mejorar la legibilidad:

```
üìÅ grupo_ventas
  ‚îú‚îÄ‚îÄ extraer_ventas
  ‚îî‚îÄ‚îÄ validar_ventas

üìÅ grupo_clientes
  ‚îú‚îÄ‚îÄ extraer_clientes
  ‚îî‚îÄ‚îÄ validar_clientes

üìÅ grupo_exportar
  ‚îú‚îÄ‚îÄ exportar_csv
  ‚îî‚îÄ‚îÄ exportar_json
```

**Ventaja:** DAG con 20+ tasks se visualiza como 6 grupos principales.

---

### 3Ô∏è‚É£ XComs (Cross-Communication)

Compartimos 6 metadatos diferentes entre tasks:

| XCom Key | Productor | Consumidor | Tipo |
|----------|-----------|------------|------|
| `df_ventas` | extraer_ventas | validar_ventas | dict |
| `df_clientes` | extraer_clientes | validar_clientes | dict |
| `num_ventas` | extraer_ventas | calcular_metricas | int |
| `num_clientes` | extraer_clientes | calcular_metricas | int |
| `total_ventas` | calcular_metricas | decidir_ruta | float |
| `cliente_top` | calcular_metricas | notificar_gerente | str |

**Ventaja:** Tasks se comunican sin necesidad de archivos intermedios.

---

### 4Ô∏è‚É£ Branching (BranchPythonOperator)

Decisi√≥n condicional basada en el total de ventas:

```
Si total_ventas > 5000‚Ç¨:
  ‚Üí Ruta PREMIUM
    - Generar reporte detallado
    - Notificar al gerente
Sino:
  ‚Üí Ruta NORMAL
    - Generar reporte simple
    - Guardar log
```

**Ventaja:** El pipeline se adapta din√°micamente seg√∫n las m√©tricas calculadas.

---

### 5Ô∏è‚É£ Templating con Jinja2

Variables din√°micas en paths de archivos:

```python
# En el DAG
fecha = context["ds_nodash"]  # YYYYMMDD

# Archivos generados
csv_path = f"data/output/metricas_{fecha}.csv"
# ‚Üí data/output/metricas_20251029.csv

json_path = f"data/output/reporte_{fecha}.json"
# ‚Üí data/output/reporte_20251029.json
```

**Ventaja:** Cada ejecuci√≥n genera archivos √∫nicos identificados por fecha.

---

## Estructura del Proyecto

```
proyecto-practico/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ dag_pipeline_intermedio.py       # DAG principal
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ sensors.py                        # Verificaci√≥n de archivos
‚îÇ   ‚îú‚îÄ‚îÄ extraccion.py                     # Extraer CSV/JSON
‚îÇ   ‚îú‚îÄ‚îÄ validacion.py                     # Validar schemas y datos
‚îÇ   ‚îú‚îÄ‚îÄ transformacion.py                 # Calcular m√©tricas
‚îÇ   ‚îú‚îÄ‚îÄ branching.py                      # L√≥gica de decisi√≥n
‚îÇ   ‚îú‚îÄ‚îÄ reportes.py                       # Generar reportes
‚îÇ   ‚îî‚îÄ‚îÄ notificaciones.py                 # Simular emails/logs
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_sensors.py                   # 3 tests
‚îÇ   ‚îú‚îÄ‚îÄ test_extraccion.py                # 6 tests
‚îÇ   ‚îú‚îÄ‚îÄ test_validacion.py                # 8 tests
‚îÇ   ‚îú‚îÄ‚îÄ test_transformacion.py            # 5 tests
‚îÇ   ‚îú‚îÄ‚îÄ test_branching.py                 # 4 tests
‚îÇ   ‚îú‚îÄ‚îÄ test_reportes.py                  # 5 tests
‚îÇ   ‚îú‚îÄ‚îÄ test_notificaciones.py            # 3 tests
‚îÇ   ‚îî‚îÄ‚îÄ test_dag.py                       # 2 tests (skipped si no Airflow)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ventas.csv                    # Datos de entrada
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clientes.json                 # Datos de entrada
‚îÇ   ‚îî‚îÄ‚îÄ output/                           # Generado por el pipeline
‚îÇ       ‚îú‚îÄ‚îÄ metricas_YYYYMMDD.csv
‚îÇ       ‚îú‚îÄ‚îÄ reporte_YYYYMMDD.json
‚îÇ       ‚îî‚îÄ‚îÄ logs/
‚îÇ           ‚îî‚îÄ‚îÄ ejecucion_YYYY-MM-DD.log
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md                             # Este archivo
‚îú‚îÄ‚îÄ ARQUITECTURA.md                       # Dise√±o detallado
‚îî‚îÄ‚îÄ CHANGELOG.md                          # Historial de cambios
```

---

## Requisitos

### Sistema

- **Python:** 3.9+
- **Apache Airflow:** 2.5+ (para ejecutar el DAG)
- **Sistema operativo:** Windows / Linux / macOS

### Librer√≠as Python

Ver `requirements.txt`:

```
pandas>=2.0.0
apache-airflow>=2.5.0
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.0.0
isort>=5.12.0
flake8>=6.0.0
```

---

## Instalaci√≥n

### 1. Clonar el repositorio

```bash
cd modulo-06-airflow/tema-2-intermedio/proyecto-practico
```

### 2. Crear entorno virtual (recomendado)

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar Airflow (opcional, para ejecuci√≥n local)

```bash
# Inicializar base de datos de Airflow
airflow db init

# Crear usuario admin
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com

# Copiar el DAG al directorio de Airflow
cp dags/dag_pipeline_intermedio.py ~/airflow/dags/

# Iniciar webserver y scheduler
airflow webserver --port 8080 &
airflow scheduler &
```

---

## Uso

### Opci√≥n 1: Ejecutar Tests (sin Airflow)

```bash
# Ejecutar todos los tests
pytest tests/ -v

# Con cobertura
pytest tests/ -v --cov=src --cov-report=term-missing
```

**Resultado esperado:** 34 tests pasando, 1 skipped, 97% cobertura

---

### Opci√≥n 2: Ejecutar DAG en Airflow Local

1. **Iniciar Airflow** (ver secci√≥n Instalaci√≥n)

2. **Acceder a la UI:** http://localhost:8080

3. **Buscar el DAG:** `dag_pipeline_intermedio`

4. **Trigger manual:** Click en el bot√≥n "Play" ‚ñ∂Ô∏è

5. **Monitorear ejecuci√≥n:**
   - Graph View: Ver flujo visual con TaskGroups
   - XCom View: Ver datos compartidos entre tasks
   - Logs: Ver output de cada task

---

### Opci√≥n 3: Test Manual de Funciones

```python
# Probar extracci√≥n
from src.extraccion import extraer_ventas_csv, extraer_clientes_json

df_ventas = extraer_ventas_csv("data/input/ventas.csv")
print(df_ventas.head())

df_clientes = extraer_clientes_json("data/input/clientes.json")
print(df_clientes.head())

# Probar transformaci√≥n
from src.transformacion import calcular_total_ventas, enriquecer_ventas_con_clientes

total = calcular_total_ventas(df_ventas)
print(f"Total de ventas: {total:.2f}‚Ç¨")

df_enriquecido = enriquecer_ventas_con_clientes(df_ventas, df_clientes)
print(df_enriquecido.head())
```

---

## Testing

### Ejecutar Todos los Tests

```bash
pytest tests/ -v
```

### Ejecutar Tests por M√≥dulo

```bash
pytest tests/test_extraccion.py -v
pytest tests/test_validacion.py -v
pytest tests/test_transformacion.py -v
pytest tests/test_branching.py -v
pytest tests/test_reportes.py -v
pytest tests/test_notificaciones.py -v
```

### Ver Cobertura de C√≥digo

```bash
pytest tests/ --cov=src --cov-report=html
# Abre htmlcov/index.html en el navegador
```

---

## Quality Check

### Formateo Autom√°tico

```bash
# Formatear con black
black src/ tests/ dags/

# Ordenar imports con isort
isort src/ tests/ dags/
```

### Linting

```bash
# Verificar con flake8
flake8 src/ tests/ dags/ --max-line-length=88 --extend-ignore=E203
```

### Quality Check Completo

```bash
# Ejecutar todo en una sola l√≠nea
black --check src/ tests/ dags/ && \
isort --check-only src/ tests/ dags/ && \
flake8 src/ tests/ dags/ --max-line-length=88 --extend-ignore=E203 && \
pytest tests/ -v --cov=src --cov-report=term-missing
```

**Resultado esperado:**
- ‚úÖ black: Todos los archivos formateados
- ‚úÖ isort: Imports ordenados
- ‚úÖ flake8: 0 errores
- ‚úÖ pytest: 34 tests pasando, 97% cobertura

---

## Troubleshooting

### ‚ùå Problema 1: ModuleNotFoundError: No module named 'airflow'

**Causa:** Airflow no est√° instalado.

**Soluci√≥n:**
```bash
pip install apache-airflow==2.7.3
```

**Nota:** Los tests del DAG se saltar√°n autom√°ticamente si Airflow no est√° instalado.

---

### ‚ùå Problema 2: FileNotFoundError al extraer datos

**Causa:** Los archivos de entrada no existen.

**Soluci√≥n:**
```bash
# Verificar que existan
ls data/input/ventas.csv
ls data/input/clientes.json

# Si no existen, crearlos manualmente o ejecutar el script de setup
python scripts/create_sample_data.py
```

---

### ‚ùå Problema 3: Encoding errors en Windows

**Causa:** Archivos escritos con UTF-8 pero le√≠dos con cp1252.

**Soluci√≥n:** Ya corregido en el c√≥digo. Todos los archivos se abren con `encoding="utf-8"`.

---

### ‚ùå Problema 4: Tests fallan con "ModuleNotFoundError: No module named 'src'"

**Causa:** El directorio ra√≠z no est√° en el PYTHONPATH.

**Soluci√≥n:**
```bash
# Opci√≥n 1: Ejecutar desde el directorio del proyecto
cd modulo-06-airflow/tema-2-intermedio/proyecto-practico
pytest tests/ -v

# Opci√≥n 2: Configurar PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"  # Linux/macOS
set PYTHONPATH=%PYTHONPATH%;%cd%           # Windows
```

---

### ‚ùå Problema 5: DAG no aparece en Airflow UI

**Causa 1:** El DAG no est√° en el directorio correcto.

**Soluci√≥n:**
```bash
# Copiar al directorio de DAGs de Airflow
cp dags/dag_pipeline_intermedio.py ~/airflow/dags/

# O configurar AIRFLOW__CORE__DAGS_FOLDER
export AIRFLOW__CORE__DAGS_FOLDER=$(pwd)/dags
```

**Causa 2:** Errores de sintaxis en el DAG.

**Soluci√≥n:**
```bash
# Verificar errores de parseo en Airflow UI o con
airflow dags list-import-errors
```

---

### ‚ùå Problema 6: XCom no funciona entre tasks

**Causa:** La task no est√° usando `ti.xcom_push()` o `ti.xcom_pull()` correctamente.

**Soluci√≥n:**
```python
# En la task productora
def mi_task(**context):
    ti = context["ti"]
    ti.xcom_push(key="mi_dato", value=42)

# En la task consumidora
def otra_task(**context):
    ti = context["ti"]
    valor = ti.xcom_pull(task_ids="mi_task", key="mi_dato")
```

---

### ‚ùå Problema 7: Branching no toma la ruta esperada

**Causa:** La funci√≥n de branching no retorna el task_id correcto.

**Soluci√≥n:** Verificar que la funci√≥n retorne **exactamente** el task_id como string:

```python
def decidir(**context):
    if condicion:
        return "task_a"  # ‚úÖ Correcto
    else:
        return "task_b"

# ‚ùå Incorrecto
# return {"task_a": True}  # NO es un string
```

---

### ‚ùå Problema 8: Cobertura de tests no alcanza 85%

**Causa:** Hay l√≠neas de c√≥digo que no est√°n cubiertas por tests.

**Soluci√≥n:**
```bash
# Ver qu√© l√≠neas faltan
pytest tests/ --cov=src --cov-report=html
# Abre htmlcov/index.html y busca l√≠neas rojas

# Agregar tests para esas l√≠neas
```

---

### ‚ùå Problema 9: FileSensor timeout

**Causa:** El archivo no lleg√≥ en el tiempo configurado.

**Soluci√≥n:**
```python
# Aumentar timeout
esperar_archivo = FileSensor(
    task_id="esperar",
    filepath="ruta/archivo.csv",
    timeout=600,  # 10 minutos en lugar de 60 segundos
)
```

---

### ‚ùå Problema 10: Task falla con "list index out of range"

**Causa:** El DataFrame est√° vac√≠o o no tiene las columnas esperadas.

**Soluci√≥n:** Agregar validaciones:

```python
def mi_task(**context):
    ti = context["ti"]
    df = ti.xcom_pull(...)

    if df is None or len(df) == 0:
        raise ValueError("DataFrame est√° vac√≠o")

    if "columna" not in df.columns:
        raise ValueError("Falta columna 'columna'")

    # Proceder con seguridad
    valor = df.iloc[0]["columna"]
```

---

## Arquitectura

Para ver el dise√±o detallado del pipeline, consulta [`ARQUITECTURA.md`](./ARQUITECTURA.md).

**Highlights:**

- **20+ tasks** organizadas en 6 grupos principales
- **3 archivos de salida** generados por ejecuci√≥n (CSV, JSON, log)
- **2 rutas condicionales** (premium/normal) basadas en m√©tricas
- **6 XComs diferentes** para comunicaci√≥n entre tasks
- **2 sensores paralelos** para esperar por archivos de entrada

---

## Resultados del Quality Check

| Herramienta | Resultado | Estado |
|-------------|-----------|--------|
| **black** | 18 archivos formateados | ‚úÖ |
| **isort** | Imports ordenados (PEP 8) | ‚úÖ |
| **flake8** | 0 errores de linting | ‚úÖ |
| **pytest** | 34 tests pasando, 1 skipped | ‚úÖ |
| **cobertura** | 97% (objetivo: >85%) | ‚úÖ |

**Calificaci√≥n Final:** **10/10** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

## Licencia

Este proyecto es parte del **Curso de Data Engineering** y tiene fines educativos.

---

## Autor

**Curso Data Engineering - M√≥dulo 6: Apache Airflow**  
Proyecto Pr√°ctico - Tema 2: Conceptos Intermedios

---

## Pr√≥ximos Pasos

Despu√©s de completar este proyecto, est√°s listo para:

1. **Tema 3: Airflow en Producci√≥n**
   - Variables y Connections
   - Logs y Monitoreo
   - Testing de DAGs
   - Ejecutores (CeleryExecutor, KubernetesExecutor)

2. **Proyectos Reales**
   - Integrar con APIs externas
   - Procesar datos en la nube (AWS S3, GCS)
   - Orquestar pipelines de Machine Learning
   - Implementar data quality checks

---

**¬°Felicitaciones por completar el Proyecto Pr√°ctico de Airflow Intermedio!** üéâ
