# üìö Tema 2: Airflow Intermedio - Ejemplos Pr√°cticos

**M√≥dulo 6: Apache Airflow y Orquestaci√≥n**
**Nivel:** Intermedio
**Duraci√≥n estimada:** 2-3 horas
**Prerequisitos:** 01-TEORIA.md completado

---

## üìñ √çndice de Ejemplos

1. [Ejemplo 1: TaskGroup B√°sico](#ejemplo-1-taskgroup-b√°sico-‚≠ê‚≠ê)
2. [Ejemplo 2: XComs - Pasar Datos entre Tasks](#ejemplo-2-xcoms---pasar-datos-entre-tasks-‚≠ê‚≠ê)
3. [Ejemplo 3: BranchPythonOperator - Flujos Condicionales](#ejemplo-3-branchpythonoperator---flujos-condicionales-‚≠ê‚≠ê‚≠ê)
4. [Ejemplo 4: FileSensor - Esperar por Archivos](#ejemplo-4-filesensor---esperar-por-archivos-‚≠ê‚≠ê)
5. [Ejemplo 5: Dynamic DAG - Generar Tasks Din√°micamente](#ejemplo-5-dynamic-dag---generar-tasks-din√°micamente-‚≠ê‚≠ê)

---

## Ejemplo 1: TaskGroup B√°sico ‚≠ê‚≠ê

### üéØ Objetivo

Aprender a organizar tasks relacionadas usando **TaskGroups** para mejorar la legibilidad del DAG.

### üìù Descripci√≥n

Crearemos un pipeline ETL simple que:
1. **Extrae** datos de ventas
2. **Procesa** los datos en un TaskGroup con 3 sub-tasks:
   - Validar datos
   - Transformar datos
   - Calcular m√©tricas
3. **Carga** los resultados

### üíª C√≥digo Completo

```python
"""
DAG que demuestra el uso de TaskGroups para organizar un pipeline ETL.

Estructura:
    inicio ‚Üí grupo_procesamiento ‚Üí cargar_resultados ‚Üí fin
              ‚îú‚îÄ validar_datos
              ‚îú‚îÄ transformar_datos
              ‚îî‚îÄ calcular_metricas
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime


def extraer_ventas():
    """Simula la extracci√≥n de datos de ventas"""
    print("[EXTRAER] Extrayendo datos de ventas desde la base de datos...")
    ventas = [
        {"producto": "Laptop", "cantidad": 3, "precio": 1200},
        {"producto": "Mouse", "cantidad": 10, "precio": 25},
        {"producto": "Teclado", "cantidad": 5, "precio": 75},
    ]
    print(f"[EXTRAER] ‚úÖ Extra√≠das {len(ventas)} ventas")
    return ventas


def validar_datos(**context):
    """Valida que los datos extra√≠dos tienen el formato correcto"""
    print("[VALIDAR] Validando estructura de datos...")
    ti = context["ti"]
    ventas = ti.xcom_pull(task_ids="extraer_ventas")

    for venta in ventas:
        assert "producto" in venta, "Falta campo 'producto'"
        assert "cantidad" in venta, "Falta campo 'cantidad'"
        assert "precio" in venta, "Falta campo 'precio'"

    print(f"[VALIDAR] ‚úÖ Validadas {len(ventas)} ventas correctamente")


def transformar_datos(**context):
    """Calcula el total por producto"""
    print("[TRANSFORMAR] Calculando totales por producto...")
    ti = context["ti"]
    ventas = ti.xcom_pull(task_ids="extraer_ventas")

    ventas_transformadas = []
    for venta in ventas:
        total = venta["cantidad"] * venta["precio"]
        ventas_transformadas.append({
            "producto": venta["producto"],
            "total": total
        })

    print(f"[TRANSFORMAR] ‚úÖ Transformadas {len(ventas_transformadas)} ventas")
    return ventas_transformadas


def calcular_metricas(**context):
    """Calcula m√©tricas agregadas"""
    print("[METRICAS] Calculando m√©tricas globales...")
    ti = context["ti"]

    # Leer del grupo: necesitamos especificar el full task_id
    ventas = ti.xcom_pull(task_ids="grupo_procesamiento.transformar_datos")

    total_general = sum(v["total"] for v in ventas)
    num_productos = len(ventas)
    promedio = total_general / num_productos if num_productos > 0 else 0

    metricas = {
        "total_general": total_general,
        "num_productos": num_productos,
        "promedio": promedio
    }

    print(f"[METRICAS] Total General: {total_general}‚Ç¨")
    print(f"[METRICAS] N√∫mero de Productos: {num_productos}")
    print(f"[METRICAS] Promedio por Producto: {promedio:.2f}‚Ç¨")

    return metricas


def cargar_resultados(**context):
    """Simula la carga de resultados a una base de datos"""
    print("[CARGAR] Cargando resultados...")
    ti = context["ti"]

    metricas = ti.xcom_pull(task_ids="grupo_procesamiento.calcular_metricas")

    print(f"[CARGAR] ‚úÖ Guardando m√©tricas en BD:")
    print(f"  - Total: {metricas['total_general']}‚Ç¨")
    print(f"  - Productos: {metricas['num_productos']}")
    print(f"  - Promedio: {metricas['promedio']:.2f}‚Ç¨")


# Definici√≥n del DAG
with DAG(
    dag_id="ejemplo_01_taskgroup",
    description="Ejemplo de uso de TaskGroups para organizar un ETL",
    start_date=datetime(2025, 1, 1),
    schedule=None,
    catchup=False,
    tags=["ejemplo", "tema2", "taskgroup"],
) as dag:

    # Task inicial
    inicio = DummyOperator(task_id="inicio")

    # Task de extracci√≥n
    extraer = PythonOperator(
        task_id="extraer_ventas",
        python_callable=extraer_ventas
    )

    # TaskGroup para agrupar el procesamiento
    with TaskGroup("grupo_procesamiento") as grupo_procesamiento:

        validar = PythonOperator(
            task_id="validar_datos",
            python_callable=validar_datos
        )

        transformar = PythonOperator(
            task_id="transformar_datos",
            python_callable=transformar_datos
        )

        metricas = PythonOperator(
            task_id="calcular_metricas",
            python_callable=calcular_metricas
        )

        # Dependencias dentro del grupo
        validar >> transformar >> metricas

    # Task de carga
    cargar = PythonOperator(
        task_id="cargar_resultados",
        python_callable=cargar_resultados
    )

    # Task final
    fin = DummyOperator(task_id="fin")

    # Dependencias del DAG
    inicio >> extraer >> grupo_procesamiento >> cargar >> fin
```

### üìä Ejecuci√≥n y Resultados

**C√≥mo ejecutar:**
1. Guardar el c√≥digo en `dags/ejemplo_01_taskgroup.py`
2. Ir a Airflow UI: http://localhost:8080
3. Buscar el DAG `ejemplo_01_taskgroup`
4. Hacer clic en el bot√≥n ‚ñ∂Ô∏è (Trigger DAG)

**Salida esperada en logs:**
```
[EXTRAER] Extrayendo datos de ventas desde la base de datos...
[EXTRAER] ‚úÖ Extra√≠das 3 ventas
[VALIDAR] Validando estructura de datos...
[VALIDAR] ‚úÖ Validadas 3 ventas correctamente
[TRANSFORMAR] Calculando totales por producto...
[TRANSFORMAR] ‚úÖ Transformadas 3 ventas
[METRICAS] Calculando m√©tricas globales...
[METRICAS] Total General: 4175‚Ç¨
[METRICAS] N√∫mero de Productos: 3
[METRICAS] Promedio por Producto: 1391.67‚Ç¨
[CARGAR] Cargando resultados...
[CARGAR] ‚úÖ Guardando m√©tricas en BD:
  - Total: 4175‚Ç¨
  - Productos: 3
  - Promedio: 1391.67‚Ç¨
```

### üîç Vista en Airflow UI

**Graph View (colapsado):**
```
inicio ‚Üí extraer_ventas ‚Üí [üìÅ grupo_procesamiento] ‚Üí cargar_resultados ‚Üí fin
```

**Graph View (expandido):**
```
inicio ‚Üí extraer_ventas ‚Üí grupo_procesamiento.validar_datos
                         ‚Üí grupo_procesamiento.transformar_datos
                         ‚Üí grupo_procesamiento.calcular_metricas
                         ‚Üí cargar_resultados ‚Üí fin
```

### üí° Puntos Clave

1. **TaskGroup con context manager:** Usamos `with TaskGroup(...)` para agrupar tasks
2. **Namespace autom√°tico:** Las tasks dentro del grupo tienen prefijo `grupo_procesamiento.`
3. **XCom entre grupos:** Para leer XComs de tasks dentro de un grupo, usa el full task_id: `grupo_procesamiento.transformar_datos`
4. **Legibilidad:** El Graph View se ve mucho m√°s limpio con el grupo colapsado

---

## Ejemplo 2: XComs - Pasar Datos entre Tasks ‚≠ê‚≠ê

### üéØ Objetivo

Aprender a usar **XComs** para compartir datos entre tasks de forma segura y eficiente.

### üìù Descripci√≥n

Crearemos un pipeline que:
1. **Calcula** m√©tricas de ventas y las guarda en XCom
2. **Eval√∫a** si las m√©tricas superan umbrales definidos
3. **Genera** un reporte usando todos los datos compartidos

### üíª C√≥digo Completo

```python
"""
DAG que demuestra el uso de XComs para compartir datos entre tasks.

Flujo:
    calcular_ventas ‚Üí evaluar_metricas ‚Üí generar_reporte ‚Üí enviar_resumen

XComs compartidos:
    - total_ventas (float)
    - num_ventas (int)
    - promedio_venta (float)
    - supera_objetivo (bool)
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import random


def calcular_ventas(**context):
    """
    Calcula m√©tricas de ventas y las guarda en XCom.

    XComs producidos:
        - total_ventas: Suma de todas las ventas
        - num_ventas: Cantidad de ventas procesadas
        - promedio_venta: Promedio por venta
    """
    ti = context["ti"]

    # Simular c√°lculo de ventas
    num_ventas = random.randint(20, 50)
    ventas = [random.uniform(50, 500) for _ in range(num_ventas)]

    total = sum(ventas)
    promedio = total / num_ventas

    print(f"[CALCULAR] Procesadas {num_ventas} ventas")
    print(f"[CALCULAR] Total: {total:.2f}‚Ç¨")
    print(f"[CALCULAR] Promedio: {promedio:.2f}‚Ç¨")

    # Guardar en XCom con keys espec√≠ficas
    ti.xcom_push(key="total_ventas", value=total)
    ti.xcom_push(key="num_ventas", value=num_ventas)
    ti.xcom_push(key="promedio_venta", value=promedio)

    # Tambi√©n se puede retornar un valor (se guarda en key "return_value")
    return {
        "mensaje": "C√°lculo completado exitosamente",
        "timestamp": str(datetime.now())
    }


def evaluar_metricas(**context):
    """
    Lee m√©tricas de XCom y eval√∫a si superan los objetivos.

    XComs consumidos:
        - total_ventas
        - num_ventas

    XComs producidos:
        - supera_objetivo: True si se super√≥ el objetivo
        - porcentaje_cumplimiento: % de cumplimiento del objetivo
    """
    ti = context["ti"]

    # Leer XComs de la task anterior
    total = ti.xcom_pull(task_ids="calcular_ventas", key="total_ventas")
    num_ventas = ti.xcom_pull(task_ids="calcular_ventas", key="num_ventas")

    # Definir objetivos
    objetivo_total = 10000
    objetivo_cantidad = 30

    # Evaluar
    supera_total = total >= objetivo_total
    supera_cantidad = num_ventas >= objetivo_cantidad
    supera_objetivo = supera_total and supera_cantidad

    porcentaje = (total / objetivo_total) * 100

    print(f"[EVALUAR] Objetivo de Total: {objetivo_total}‚Ç¨")
    print(f"[EVALUAR] Total Alcanzado: {total:.2f}‚Ç¨ ({porcentaje:.1f}%)")
    print(f"[EVALUAR] Objetivo de Cantidad: {objetivo_cantidad}")
    print(f"[EVALUAR] Cantidad Alcanzada: {num_ventas}")

    if supera_objetivo:
        print(f"[EVALUAR] ‚úÖ ¬°OBJETIVO SUPERADO!")
    else:
        print(f"[EVALUAR] ‚ùå Objetivo no alcanzado")

    # Guardar resultado de evaluaci√≥n
    ti.xcom_push(key="supera_objetivo", value=supera_objetivo)
    ti.xcom_push(key="porcentaje_cumplimiento", value=porcentaje)


def generar_reporte(**context):
    """
    Genera un reporte consolidado con todos los datos compartidos.

    XComs consumidos:
        - total_ventas
        - num_ventas
        - promedio_venta
        - supera_objetivo
        - porcentaje_cumplimiento
    """
    ti = context["ti"]

    # Leer todos los XComs
    total = ti.xcom_pull(task_ids="calcular_ventas", key="total_ventas")
    num_ventas = ti.xcom_pull(task_ids="calcular_ventas", key="num_ventas")
    promedio = ti.xcom_pull(task_ids="calcular_ventas", key="promedio_venta")
    supera = ti.xcom_pull(task_ids="evaluar_metricas", key="supera_objetivo")
    porcentaje = ti.xcom_pull(task_ids="evaluar_metricas", key="porcentaje_cumplimiento")

    # Generar reporte
    print("="*60)
    print("          REPORTE DE VENTAS - DIARIO")
    print("="*60)
    print(f"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("")
    print("M√âTRICAS:")
    print(f"  ‚Ä¢ Total de Ventas:     {total:>10.2f}‚Ç¨")
    print(f"  ‚Ä¢ Cantidad de Ventas:  {num_ventas:>10}")
    print(f"  ‚Ä¢ Promedio por Venta:  {promedio:>10.2f}‚Ç¨")
    print("")
    print("EVALUACI√ìN:")
    print(f"  ‚Ä¢ Cumplimiento:        {porcentaje:>10.1f}%")

    if supera:
        print(f"  ‚Ä¢ Estado:              {'OBJETIVO CUMPLIDO':>20} ‚úÖ")
    else:
        print(f"  ‚Ä¢ Estado:              {'Objetivo No Cumplido':>20} ‚ùå")

    print("="*60)

    # Retornar resumen (se guarda en XCom autom√°ticamente)
    return {
        "total": total,
        "num_ventas": num_ventas,
        "cumplimiento": porcentaje,
        "exitoso": supera
    }


def enviar_resumen(**context):
    """
    Simula el env√≠o del resumen por email.
    Lee el return_value de la task anterior.
    """
    ti = context["ti"]

    # Leer el return_value de generar_reporte
    resumen = ti.xcom_pull(task_ids="generar_reporte", key="return_value")

    print("[EMAIL] Preparando email...")
    print(f"[EMAIL] Destinatario: gerente@empresa.com")
    print(f"[EMAIL] Asunto: Reporte Diario de Ventas")
    print(f"[EMAIL] Contenido:")
    print(f"  - Total: {resumen['total']:.2f}‚Ç¨")
    print(f"  - Ventas: {resumen['num_ventas']}")
    print(f"  - Cumplimiento: {resumen['cumplimiento']:.1f}%")
    print(f"  - Exitoso: {'S√≠' if resumen['exitoso'] else 'No'}")
    print(f"[EMAIL] ‚úÖ Email enviado exitosamente")


# Definici√≥n del DAG
with DAG(
    dag_id="ejemplo_02_xcoms",
    description="Ejemplo de uso de XComs para compartir datos entre tasks",
    start_date=datetime(2025, 1, 1),
    schedule=None,
    catchup=False,
    tags=["ejemplo", "tema2", "xcoms"],
) as dag:

    calcular = PythonOperator(
        task_id="calcular_ventas",
        python_callable=calcular_ventas
    )

    evaluar = PythonOperator(
        task_id="evaluar_metricas",
        python_callable=evaluar_metricas
    )

    reportar = PythonOperator(
        task_id="generar_reporte",
        python_callable=generar_reporte
    )

    enviar = PythonOperator(
        task_id="enviar_resumen",
        python_callable=enviar_resumen
    )

    calcular >> evaluar >> reportar >> enviar
```

### üìä Ejecuci√≥n y Resultados

**Salida esperada en logs:**
```
[CALCULAR] Procesadas 37 ventas
[CALCULAR] Total: 11234.56‚Ç¨
[CALCULAR] Promedio: 303.64‚Ç¨

[EVALUAR] Objetivo de Total: 10000‚Ç¨
[EVALUAR] Total Alcanzado: 11234.56‚Ç¨ (112.3%)
[EVALUAR] Objetivo de Cantidad: 30
[EVALUAR] Cantidad Alcanzada: 37
[EVALUAR] ‚úÖ ¬°OBJETIVO SUPERADO!

============================================================
          REPORTE DE VENTAS - DIARIO
============================================================
Fecha: 2025-01-27 14:30:45

M√âTRICAS:
  ‚Ä¢ Total de Ventas:       11234.56‚Ç¨
  ‚Ä¢ Cantidad de Ventas:           37
  ‚Ä¢ Promedio por Venta:       303.64‚Ç¨

EVALUACI√ìN:
  ‚Ä¢ Cumplimiento:              112.3%
  ‚Ä¢ Estado:          OBJETIVO CUMPLIDO ‚úÖ
============================================================

[EMAIL] Preparando email...
[EMAIL] ‚úÖ Email enviado exitosamente
```

### üí° Puntos Clave

1. **xcom_push con keys:** Usa keys descriptivas para m√∫ltiples valores
2. **xcom_pull espec√≠fico:** Lee XComs especificando `task_ids` y `key`
3. **return autom√°tico:** Si retornas un valor, se guarda en key `return_value`
4. **Tipos serializables:** Usa dict, list, int, float, str, bool (serializables en JSON)
5. **Documentaci√≥n:** Documenta qu√© XComs produce/consume cada funci√≥n

---

## Ejemplo 3: BranchPythonOperator - Flujos Condicionales ‚≠ê‚≠ê‚≠ê

### üéØ Objetivo

Aprender a crear **flujos condicionales** (if-else) en DAGs usando **BranchPythonOperator**.

### üìù Descripci√≥n

Crearemos un pipeline que:
1. **Calcula** el total de ventas
2. **Decide** qu√© procesamiento aplicar seg√∫n el monto:
   - Si ventas > 15,000‚Ç¨ ‚Üí Procesamiento PREMIUM
   - Si ventas ‚â§ 15,000‚Ç¨ ‚Üí Procesamiento NORMAL
3. **Consolida** los resultados (join)

### üíª C√≥digo Completo

```python
"""
DAG que demuestra el uso de BranchPythonOperator para flujos condicionales.

Flujo:
    inicio ‚Üí calcular_ventas ‚Üí decidir_procesamiento
                                    ‚îú‚îÄ‚îÄ‚Üí (ventas > 15000) procesar_premium ‚Üí alerta_gerente
                                    ‚îî‚îÄ‚îÄ‚Üí (ventas ‚â§ 15000) procesar_normal ‚Üí guardar_log
                                                                                    ‚Üì
                                                                        consolidar_resultados ‚Üí fin
"""

from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.dummy import DummyOperator
from datetime import datetime
import random


def calcular_ventas():
    """Simula el c√°lculo de ventas totales"""
    total = random.uniform(8000, 20000)
    print(f"[CALCULAR] Total de ventas calculado: {total:.2f}‚Ç¨")
    return total


def decidir_procesamiento(**context):
    """
    Funci√≥n de branching que decide qu√© camino tomar.

    Retorna:
        - "procesar_premium" si ventas > 15000
        - "procesar_normal" si ventas ‚â§ 15000
    """
    ti = context["ti"]
    total_ventas = ti.xcom_pull(task_ids="calcular_ventas")

    umbral = 15000

    print(f"[BRANCH] Total de ventas: {total_ventas:.2f}‚Ç¨")
    print(f"[BRANCH] Umbral: {umbral:.2f}‚Ç¨")

    if total_ventas > umbral:
        print(f"[BRANCH] ‚úÖ Ventas superan umbral ‚Üí Ruta PREMIUM")
        return "procesar_premium"
    else:
        print(f"[BRANCH] üìù Ventas no superan umbral ‚Üí Ruta NORMAL")
        return "procesar_normal"


def procesar_premium(**context):
    """Procesamiento para ventas PREMIUM (altas)"""
    ti = context["ti"]
    total = ti.xcom_pull(task_ids="calcular_ventas")

    print("="*60)
    print("üåü PROCESAMIENTO PREMIUM üåü")
    print("="*60)
    print(f"Total de Ventas: {total:.2f}‚Ç¨")
    print("Aplicando descuentos exclusivos...")
    print("Calculando comisiones especiales...")
    print("Actualizando ranking de clientes VIP...")
    print("‚úÖ Procesamiento PREMIUM completado")
    print("="*60)

    return {"tipo": "premium", "total": total, "descuento": 0.15}


def alerta_gerente(**context):
    """Env√≠a alerta al gerente sobre ventas altas"""
    ti = context["ti"]
    resultado = ti.xcom_pull(task_ids="procesar_premium")

    print("[ALERTA] üìß Enviando alerta al gerente...")
    print(f"[ALERTA] Asunto: ¬°Ventas excepcionales del d√≠a!")
    print(f"[ALERTA] Total: {resultado['total']:.2f}‚Ç¨")
    print(f"[ALERTA] Descuento aplicado: {resultado['descuento']*100}%")
    print("[ALERTA] ‚úÖ Alerta enviada exitosamente")


def procesar_normal(**context):
    """Procesamiento para ventas NORMALES"""
    ti = context["ti"]
    total = ti.xcom_pull(task_ids="calcular_ventas")

    print("="*60)
    print("üìã PROCESAMIENTO NORMAL")
    print("="*60)
    print(f"Total de Ventas: {total:.2f}‚Ç¨")
    print("Aplicando proceso est√°ndar...")
    print("Calculando comisiones regulares...")
    print("‚úÖ Procesamiento NORMAL completado")
    print("="*60)

    return {"tipo": "normal", "total": total, "descuento": 0.05}


def guardar_log(**context):
    """Guarda un log de las ventas normales"""
    ti = context["ti"]
    resultado = ti.xcom_pull(task_ids="procesar_normal")

    print("[LOG] üìù Guardando log de ventas...")
    print(f"[LOG] Tipo: {resultado['tipo']}")
    print(f"[LOG] Total: {resultado['total']:.2f}‚Ç¨")
    print(f"[LOG] Descuento: {resultado['descuento']*100}%")
    print("[LOG] ‚úÖ Log guardado en /logs/ventas.log")


def consolidar_resultados(**context):
    """
    Consolida los resultados de ambos caminos.

    Nota: Esta task tiene trigger_rule="none_failed_min_one_success"
    para que se ejecute incluso si una de las rutas fue saltada.
    """
    ti = context["ti"]

    print("[CONSOLIDAR] Consolidando resultados...")

    # Intentar leer de ambos caminos
    resultado_premium = ti.xcom_pull(task_ids="procesar_premium")
    resultado_normal = ti.xcom_pull(task_ids="procesar_normal")

    if resultado_premium is not None:
        print(f"[CONSOLIDAR] Ruta PREMIUM ejecutada")
        print(f"[CONSOLIDAR]   - Total: {resultado_premium['total']:.2f}‚Ç¨")
        print(f"[CONSOLIDAR]   - Descuento: {resultado_premium['descuento']*100}%")

    if resultado_normal is not None:
        print(f"[CONSOLIDAR] Ruta NORMAL ejecutada")
        print(f"[CONSOLIDAR]   - Total: {resultado_normal['total']:.2f}‚Ç¨")
        print(f"[CONSOLIDAR]   - Descuento: {resultado_normal['descuento']*100}%")

    print("[CONSOLIDAR] ‚úÖ Consolidaci√≥n completada")


# Definici√≥n del DAG
with DAG(
    dag_id="ejemplo_03_branching",
    description="Ejemplo de uso de BranchPythonOperator para flujos condicionales",
    start_date=datetime(2025, 1, 1),
    schedule=None,
    catchup=False,
    tags=["ejemplo", "tema2", "branching"],
) as dag:

    inicio = DummyOperator(task_id="inicio")

    calcular = PythonOperator(
        task_id="calcular_ventas",
        python_callable=calcular_ventas
    )

    # BranchPythonOperator - decide qu√© camino tomar
    decidir = BranchPythonOperator(
        task_id="decidir_procesamiento",
        python_callable=decidir_procesamiento
    )

    # Ruta PREMIUM
    premium = PythonOperator(
        task_id="procesar_premium",
        python_callable=procesar_premium
    )

    alerta = PythonOperator(
        task_id="alerta_gerente",
        python_callable=alerta_gerente
    )

    # Ruta NORMAL
    normal = PythonOperator(
        task_id="procesar_normal",
        python_callable=procesar_normal
    )

    log = PythonOperator(
        task_id="guardar_log",
        python_callable=guardar_log
    )

    # Consolidaci√≥n (join)
    consolidar = PythonOperator(
        task_id="consolidar_resultados",
        python_callable=consolidar_resultados,
        trigger_rule="none_failed_min_one_success"  # ‚Üê Importante!
    )

    fin = DummyOperator(task_id="fin")

    # Dependencias
    inicio >> calcular >> decidir
    decidir >> premium >> alerta >> consolidar
    decidir >> normal >> log >> consolidar
    consolidar >> fin
```

### üìä Ejecuci√≥n y Resultados

**Escenario 1: Ventas Altas (>15,000‚Ç¨)**
```
[CALCULAR] Total de ventas calculado: 17543.21‚Ç¨
[BRANCH] Total de ventas: 17543.21‚Ç¨
[BRANCH] Umbral: 15000.00‚Ç¨
[BRANCH] ‚úÖ Ventas superan umbral ‚Üí Ruta PREMIUM

============================================================
üåü PROCESAMIENTO PREMIUM üåü
============================================================
Total de Ventas: 17543.21‚Ç¨
Aplicando descuentos exclusivos...
Calculando comisiones especiales...
Actualizando ranking de clientes VIP...
‚úÖ Procesamiento PREMIUM completado
============================================================

[ALERTA] üìß Enviando alerta al gerente...
[ALERTA] Asunto: ¬°Ventas excepcionales del d√≠a!
[ALERTA] Total: 17543.21‚Ç¨
[ALERTA] Descuento aplicado: 15.0%
[ALERTA] ‚úÖ Alerta enviada exitosamente

[CONSOLIDAR] Consolidando resultados...
[CONSOLIDAR] Ruta PREMIUM ejecutada
[CONSOLIDAR]   - Total: 17543.21‚Ç¨
[CONSOLIDAR]   - Descuento: 15.0%
[CONSOLIDAR] ‚úÖ Consolidaci√≥n completada
```

**Estados de las tasks:**
- ‚úÖ inicio: success
- ‚úÖ calcular_ventas: success
- ‚úÖ decidir_procesamiento: success (retorn√≥ "procesar_premium")
- ‚úÖ procesar_premium: success
- ‚úÖ alerta_gerente: success
- ‚è≠Ô∏è procesar_normal: **skipped** (no ejecutada)
- ‚è≠Ô∏è guardar_log: **skipped** (no ejecutada)
- ‚úÖ consolidar_resultados: success
- ‚úÖ fin: success

---

**Escenario 2: Ventas Normales (‚â§15,000‚Ç¨)**
```
[CALCULAR] Total de ventas calculado: 12345.67‚Ç¨
[BRANCH] Total de ventas: 12345.67‚Ç¨
[BRANCH] Umbral: 15000.00‚Ç¨
[BRANCH] üìù Ventas no superan umbral ‚Üí Ruta NORMAL

============================================================
üìã PROCESAMIENTO NORMAL
============================================================
Total de Ventas: 12345.67‚Ç¨
Aplicando proceso est√°ndar...
Calculando comisiones regulares...
‚úÖ Procesamiento NORMAL completado
============================================================

[LOG] üìù Guardando log de ventas...
[LOG] Tipo: normal
[LOG] Total: 12345.67‚Ç¨
[LOG] Descuento: 5.0%
[LOG] ‚úÖ Log guardado en /logs/ventas.log

[CONSOLIDAR] Consolidando resultados...
[CONSOLIDAR] Ruta NORMAL ejecutada
[CONSOLIDAR]   - Total: 12345.67‚Ç¨
[CONSOLIDAR]   - Descuento: 5.0%
[CONSOLIDAR] ‚úÖ Consolidaci√≥n completada
```

**Estados de las tasks:**
- ‚úÖ inicio: success
- ‚úÖ calcular_ventas: success
- ‚úÖ decidir_procesamiento: success (retorn√≥ "procesar_normal")
- ‚è≠Ô∏è procesar_premium: **skipped**
- ‚è≠Ô∏è alerta_gerente: **skipped**
- ‚úÖ procesar_normal: success
- ‚úÖ guardar_log: success
- ‚úÖ consolidar_resultados: success
- ‚úÖ fin: success

### üí° Puntos Clave

1. **BranchPythonOperator:** La funci√≥n debe retornar el `task_id` a ejecutar
2. **Tasks saltadas:** Las rutas no elegidas se marcan como "skipped"
3. **Trigger rule:** La task de consolidaci√≥n necesita `trigger_rule="none_failed_min_one_success"` para ejecutarse aunque haya tasks saltadas
4. **M√∫ltiples caminos:** Puedes tener 2, 3 o m√°s rutas diferentes
5. **Logging:** Siempre loggea qu√© camino se tom√≥ y por qu√©

---

## Ejemplo 4: FileSensor - Esperar por Archivos ‚≠ê‚≠ê

### üéØ Objetivo

Aprender a usar **FileSensor** para esperar por la llegada de archivos antes de procesarlos.

### üìù Descripci√≥n

Crearemos un pipeline que:
1. **Espera** hasta que un archivo CSV aparezca en el sistema
2. **Procesa** el archivo una vez que est√° disponible
3. **Limpia** los archivos temporales

### üíª C√≥digo Completo

```python
"""
DAG que demuestra el uso de FileSensor para esperar por archivos.

Flujo:
    preparar_directorio ‚Üí esperar_datos ‚Üí procesar_archivo ‚Üí limpiar_temporales

Prerequisito:
    - Crear el archivo /tmp/datos_ventas.csv para que el sensor lo detecte
"""

from airflow import DAG
from airflow.sensors.filesystem import FileSensor
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime
import pandas as pd
from pathlib import Path


def preparar_directorio():
    """Prepara el directorio para recibir archivos"""
    directorio = Path("/tmp")
    directorio.mkdir(exist_ok=True)

    print(f"[PREPARAR] Directorio preparado: {directorio}")
    print(f"[PREPARAR] Esperando archivo: /tmp/datos_ventas.csv")
    print("[PREPARAR] Para simular la llegada del archivo, ejecuta:")
    print("  echo 'producto,cantidad,precio' > /tmp/datos_ventas.csv")
    print("  echo 'Laptop,3,1200' >> /tmp/datos_ventas.csv")
    print("  echo 'Mouse,10,25' >> /tmp/datos_ventas.csv")


def procesar_archivo():
    """Procesa el archivo CSV una vez que ha llegado"""
    ruta_archivo = "/tmp/datos_ventas.csv"

    print(f"[PROCESAR] ‚úÖ Archivo detectado: {ruta_archivo}")
    print(f"[PROCESAR] Leyendo archivo...")

    try:
        df = pd.read_csv(ruta_archivo)

        print(f"[PROCESAR] Filas encontradas: {len(df)}")
        print(f"[PROCESAR] Columnas: {list(df.columns)}")
        print("")
        print("[PROCESAR] Contenido:")
        print(df.to_string(index=False))
        print("")

        # Calcular total
        if 'cantidad' in df.columns and 'precio' in df.columns:
            df['total'] = df['cantidad'] * df['precio']
            total_general = df['total'].sum()
            print(f"[PROCESAR] Total General: {total_general:.2f}‚Ç¨")

        print("[PROCESAR] ‚úÖ Procesamiento completado")

    except Exception as e:
        print(f"[PROCESAR] ‚ùå Error al procesar archivo: {e}")
        raise


def limpiar_temporales():
    """Limpia los archivos temporales"""
    ruta_archivo = Path("/tmp/datos_ventas.csv")

    if ruta_archivo.exists():
        ruta_archivo.unlink()
        print(f"[LIMPIAR] ‚úÖ Archivo eliminado: {ruta_archivo}")
    else:
        print(f"[LIMPIAR] ‚ö†Ô∏è Archivo no encontrado: {ruta_archivo}")


# Definici√≥n del DAG
with DAG(
    dag_id="ejemplo_04_filesensor",
    description="Ejemplo de uso de FileSensor para esperar por archivos",
    start_date=datetime(2025, 1, 1),
    schedule=None,
    catchup=False,
    tags=["ejemplo", "tema2", "sensor"],
) as dag:

    preparar = PythonOperator(
        task_id="preparar_directorio",
        python_callable=preparar_directorio
    )

    # FileSensor - espera hasta que el archivo exista
    esperar = FileSensor(
        task_id="esperar_datos",
        filepath="/tmp/datos_ventas.csv",
        poke_interval=10,  # Verificar cada 10 segundos
        timeout=120,  # Esperar m√°ximo 2 minutos
        mode="reschedule",  # No bloquear el worker
        soft_fail=False  # Fallar el DAG si timeout
    )

    procesar = PythonOperator(
        task_id="procesar_archivo",
        python_callable=procesar_archivo
    )

    limpiar = PythonOperator(
        task_id="limpiar_temporales",
        python_callable=limpiar_temporales
    )

    preparar >> esperar >> procesar >> limpiar
```

### üìä C√≥mo Probar

**Paso 1: Ejecutar el DAG**
1. Guardar el c√≥digo en `dags/ejemplo_04_filesensor.py`
2. Ir a Airflow UI: http://localhost:8080
3. Trigger el DAG `ejemplo_04_filesensor`

**Paso 2: Crear el archivo (en otra terminal)**

Mientras el sensor est√° esperando, crea el archivo:

```bash
# En Linux/Mac:
echo 'producto,cantidad,precio' > /tmp/datos_ventas.csv
echo 'Laptop,3,1200' >> /tmp/datos_ventas.csv
echo 'Mouse,10,25' >> /tmp/datos_ventas.csv
echo 'Teclado,5,75' >> /tmp/datos_ventas.csv

# En Windows (PowerShell):
"producto,cantidad,precio" | Out-File -FilePath C:\Temp\datos_ventas.csv
"Laptop,3,1200" | Out-File -Append -FilePath C:\Temp\datos_ventas.csv
"Mouse,10,25" | Out-File -Append -FilePath C:\Temp\datos_ventas.csv
"Teclado,5,75" | Out-File -Append -FilePath C:\Temp\datos_ventas.csv
```

**Paso 3: Ver los Resultados**

El sensor detectar√° el archivo y continuar√° con el procesamiento:

```
[PREPARAR] Directorio preparado: /tmp
[PREPARAR] Esperando archivo: /tmp/datos_ventas.csv

[SENSOR] Esperando archivo... (intento 1)
[SENSOR] Esperando archivo... (intento 2)
[SENSOR] ‚úÖ Archivo detectado!

[PROCESAR] ‚úÖ Archivo detectado: /tmp/datos_ventas.csv
[PROCESAR] Leyendo archivo...
[PROCESAR] Filas encontradas: 3
[PROCESAR] Columnas: ['producto', 'cantidad', 'precio']

[PROCESAR] Contenido:
 producto  cantidad  precio
   Laptop         3    1200
    Mouse        10      25
  Teclado         5      75

[PROCESAR] Total General: 4175.00‚Ç¨
[PROCESAR] ‚úÖ Procesamiento completado

[LIMPIAR] ‚úÖ Archivo eliminado: /tmp/datos_ventas.csv
```

### üí° Puntos Clave

1. **Poke interval:** `poke_interval=10` verifica cada 10 segundos
2. **Timeout:** `timeout=120` espera m√°ximo 2 minutos antes de fallar
3. **Mode reschedule:** `mode="reschedule"` libera el worker entre verificaciones
4. **Soft fail:** `soft_fail=False` hace que el DAG falle si el archivo no llega (ajusta seg√∫n necesidad)
5. **Uso real:** En producci√≥n, archivos externos (SFTP, S3, etc.) llegan peri√≥dicamente

---

## Ejemplo 5: Dynamic DAG - Generar Tasks Din√°micamente ‚≠ê‚≠ê

### üéØ Objetivo

Aprender a **generar tasks din√°micamente** usando bucles para procesar m√∫ltiples items sin c√≥digo repetitivo.

### üìù Descripci√≥n

Crearemos un pipeline que:
1. **Procesa** 5 regiones de Espa√±a en paralelo
2. **Consolida** los resultados de todas las regiones
3. **Genera** un reporte final

### üíª C√≥digo Completo

```python
"""
DAG que demuestra la generaci√≥n din√°mica de tasks.

Flujo:
    inicio ‚Üí [procesar_madrid, procesar_barcelona, ..., procesar_valencia] ‚Üí consolidar ‚Üí generar_reporte ‚Üí fin

    5 tasks de procesamiento se crean din√°micamente en paralelo
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from datetime import datetime
import random


def procesar_region(region: str, **context):
    """
    Procesa los datos de una regi√≥n espec√≠fica.

    Args:
        region: Nombre de la regi√≥n a procesar
    """
    ti = context["ti"]

    # Simular procesamiento de datos
    num_ventas = random.randint(50, 200)
    total_ventas = random.uniform(5000, 20000)
    promedio = total_ventas / num_ventas

    print(f"[{region.upper()}] Procesando datos de la regi√≥n {region}")
    print(f"[{region.upper()}] N√∫mero de ventas: {num_ventas}")
    print(f"[{region.upper()}] Total de ventas: {total_ventas:.2f}‚Ç¨")
    print(f"[{region.upper()}] Promedio por venta: {promedio:.2f}‚Ç¨")
    print(f"[{region.upper()}] ‚úÖ Procesamiento completado")

    # Retornar resultado para consolidaci√≥n
    return {
        "region": region,
        "num_ventas": num_ventas,
        "total": total_ventas,
        "promedio": promedio
    }


def consolidar_resultados(**context):
    """Consolida los resultados de todas las regiones"""
    ti = context["ti"]

    regiones = ["Madrid", "Barcelona", "Valencia", "Sevilla", "Bilbao"]

    print("[CONSOLIDAR] Consolidando resultados de todas las regiones...")
    print("="*70)

    resultados = []
    total_general = 0
    ventas_totales = 0

    for region in regiones:
        task_id = f"procesar_{region.lower()}"
        resultado = ti.xcom_pull(task_ids=task_id)

        if resultado:
            resultados.append(resultado)
            total_general += resultado["total"]
            ventas_totales += resultado["num_ventas"]

            print(f"{region:15} | Ventas: {resultado['num_ventas']:4} | "
                  f"Total: {resultado['total']:>10.2f}‚Ç¨ | "
                  f"Promedio: {resultado['promedio']:>7.2f}‚Ç¨")

    print("="*70)
    print(f"{'TOTAL':15} | Ventas: {ventas_totales:4} | "
          f"Total: {total_general:>10.2f}‚Ç¨ | "
          f"Promedio: {(total_general/ventas_totales):>7.2f}‚Ç¨")
    print("="*70)

    print(f"[CONSOLIDAR] ‚úÖ Consolidadas {len(resultados)} regiones")

    return {
        "regiones": len(resultados),
        "total_ventas": ventas_totales,
        "total_general": total_general,
        "promedio_general": total_general / ventas_totales if ventas_totales > 0 else 0
    }


def generar_reporte(**context):
    """Genera un reporte final con los datos consolidados"""
    ti = context["ti"]
    consolidado = ti.xcom_pull(task_ids="consolidar")

    print("")
    print("‚ïî"+"‚ïê"*68+"‚ïó")
    print("‚ïë" + " "*20 + "REPORTE NACIONAL DE VENTAS" + " "*22 + "‚ïë")
    print("‚ï†"+"‚ïê"*68+"‚ï£")
    print(f"‚ïë Fecha:            {datetime.now().strftime('%Y-%m-%d %H:%M:%S'):45} ‚ïë")
    print(f"‚ïë Regiones:         {consolidado['regiones']:45} ‚ïë")
    print(f"‚ïë Ventas Totales:   {consolidado['total_ventas']:45} ‚ïë")
    print(f"‚ïë Facturaci√≥n:      {consolidado['total_general']:>43.2f}‚Ç¨ ‚ïë")
    print(f"‚ïë Promedio:         {consolidado['promedio_general']:>43.2f}‚Ç¨ ‚ïë")
    print("‚ïö"+"‚ïê"*68+"‚ïù")
    print("")
    print("[REPORTE] ‚úÖ Reporte generado exitosamente")


# Definici√≥n del DAG
with DAG(
    dag_id="ejemplo_05_dynamic_dag",
    description="Ejemplo de generaci√≥n din√°mica de tasks",
    start_date=datetime(2025, 1, 1),
    schedule=None,
    catchup=False,
    tags=["ejemplo", "tema2", "dynamic"],
) as dag:

    inicio = DummyOperator(task_id="inicio")

    # Lista de regiones a procesar
    regiones = ["Madrid", "Barcelona", "Valencia", "Sevilla", "Bilbao"]

    # Generar tasks din√°micamente
    tasks_procesamiento = []

    for region in regiones:
        task = PythonOperator(
            task_id=f"procesar_{region.lower()}",
            python_callable=procesar_region,
            op_kwargs={"region": region}
        )
        tasks_procesamiento.append(task)

        # Conectar inicio ‚Üí task
        inicio >> task

    # Tasks de consolidaci√≥n y reporte
    consolidar = PythonOperator(
        task_id="consolidar",
        python_callable=consolidar_resultados
    )

    reportar = PythonOperator(
        task_id="generar_reporte",
        python_callable=generar_reporte
    )

    fin = DummyOperator(task_id="fin")

    # Conectar todas las tasks de procesamiento ‚Üí consolidar
    tasks_procesamiento >> consolidar >> reportar >> fin
```

### üìä Ejecuci√≥n y Resultados

**Salida esperada:**
```
[MADRID] Procesando datos de la regi√≥n Madrid
[MADRID] N√∫mero de ventas: 142
[MADRID] Total de ventas: 15234.56‚Ç¨
[MADRID] Promedio por venta: 107.28‚Ç¨
[MADRID] ‚úÖ Procesamiento completado

[BARCELONA] Procesando datos de la regi√≥n Barcelona
[BARCELONA] N√∫mero de ventas: 178
[BARCELONA] Total de ventas: 18765.43‚Ç¨
[BARCELONA] Promedio por venta: 105.42‚Ç¨
[BARCELONA] ‚úÖ Procesamiento completado

[VALENCIA] Procesando datos de la regi√≥n Valencia
[VALENCIA] N√∫mero de ventas: 95
[VALENCIA] Total de ventas: 9876.54‚Ç¨
[VALENCIA] Promedio por venta: 103.96‚Ç¨
[VALENCIA] ‚úÖ Procesamiento completado

[SEVILLA] Procesando datos de la regi√≥n Sevilla
[SEVILLA] N√∫mero de ventas: 123
[SEVILLA] Total de ventas: 12345.67‚Ç¨
[SEVILLA] Promedio por venta: 100.37‚Ç¨
[SEVILLA] ‚úÖ Procesamiento completado

[BILBAO] Procesando datos de la regi√≥n Bilbao
[BILBAO] N√∫mero de ventas: 87
[BILBAO] Total de ventas: 8765.43‚Ç¨
[BILBAO] Promedio por venta: 100.75‚Ç¨
[BILBAO] ‚úÖ Procesamiento completado

[CONSOLIDAR] Consolidando resultados de todas las regiones...
======================================================================
Madrid          | Ventas:  142 | Total:   15234.56‚Ç¨ | Promedio:  107.28‚Ç¨
Barcelona       | Ventas:  178 | Total:   18765.43‚Ç¨ | Promedio:  105.42‚Ç¨
Valencia        | Ventas:   95 | Total:    9876.54‚Ç¨ | Promedio:  103.96‚Ç¨
Sevilla         | Ventas:  123 | Total:   12345.67‚Ç¨ | Promedio:  100.37‚Ç¨
Bilbao          | Ventas:   87 | Total:    8765.43‚Ç¨ | Promedio:  100.75‚Ç¨
======================================================================
TOTAL           | Ventas:  625 | Total:   64987.63‚Ç¨ | Promedio:  103.98‚Ç¨
======================================================================
[CONSOLIDAR] ‚úÖ Consolidadas 5 regiones

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    REPORTE NACIONAL DE VENTAS                      ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Fecha:            2025-01-27 15:45:30                              ‚ïë
‚ïë Regiones:         5                                                ‚ïë
‚ïë Ventas Totales:   625                                              ‚ïë
‚ïë Facturaci√≥n:                                           64987.63‚Ç¨   ‚ïë
‚ïë Promedio:                                                103.98‚Ç¨   ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

[REPORTE] ‚úÖ Reporte generado exitosamente
```

### üîç Vista en Airflow UI (Graph View)

```
inicio ‚Üí [procesar_madrid]
      ‚Üí [procesar_barcelona]
      ‚Üí [procesar_valencia]     ‚Üí consolidar ‚Üí generar_reporte ‚Üí fin
      ‚Üí [procesar_sevilla]
      ‚Üí [procesar_bilbao]
```

Las 5 tasks de procesamiento se ejecutan **en paralelo**, y solo cuando todas terminan, se ejecuta la consolidaci√≥n.

### üí° Puntos Clave

1. **Lista de items:** Define una lista clara de los items a procesar
2. **Bucle for:** Genera tasks din√°micamente con un bucle
3. **op_kwargs:** Pasa par√°metros espec√≠ficos a cada task con `op_kwargs`
4. **Task IDs √∫nicos:** Usa f-strings para crear IDs √∫nicos: `f"procesar_{region}"`
5. **Dependencias din√°micas:** Conecta las tasks generadas con `>>` dentro del bucle
6. **Consolidaci√≥n:** La task de consolidaci√≥n lee los XComs de todas las tasks din√°micas
7. **Escalabilidad:** Si ma√±ana son 10 regiones, solo cambias la lista

---

## üéØ Resumen de Ejemplos

### Conceptos Cubiertos

| Ejemplo | Concepto Principal | Dificultad | Tiempo Estimado |
|---------|-------------------|------------|-----------------|
| Ejemplo 1 | TaskGroups | ‚≠ê‚≠ê | 20 min |
| Ejemplo 2 | XComs | ‚≠ê‚≠ê | 20 min |
| Ejemplo 3 | BranchPythonOperator | ‚≠ê‚≠ê‚≠ê | 25 min |
| Ejemplo 4 | FileSensor | ‚≠ê‚≠ê | 20 min |
| Ejemplo 5 | Dynamic DAG Generation | ‚≠ê‚≠ê | 15 min |

---

### üöÄ Pr√≥ximos Pasos

Ahora que has visto los ejemplos pr√°cticos, es momento de **practicar por tu cuenta**:

1. **Ejecuta todos los ejemplos** en tu entorno de Airflow local
2. **Modifica los ejemplos:** Cambia par√°metros, a√±ade funcionalidades
3. **Combina conceptos:** Crea un DAG que use TaskGroups + XComs + Branching
4. **Contin√∫a con 03-EJERCICIOS.md:** Practica con 15 ejercicios graduados

---

**¬°Felicitaciones por completar los ejemplos del Tema 2!** üéâ
