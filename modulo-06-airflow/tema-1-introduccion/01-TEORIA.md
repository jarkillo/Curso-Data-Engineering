# Introducci√≥n a Apache Airflow

**Tiempo de lectura:** 35-40 minutos
**Nivel:** Principiante absoluto
**Prerequisitos:** Conocimientos b√°sicos de Python (variables, funciones, if/else)

> **‚ö†Ô∏è IMPORTANTE - Prerequisito de Docker:**
> Este tema requiere **Docker y Docker Compose** instalados y funcionando en tu sistema.
> Airflow se ejecuta como un conjunto de servicios (webserver, scheduler, base de datos)
> que se gestionan f√°cilmente con Docker Compose.
>
> **Si a√∫n no tienes Docker:**
> - **Windows/Mac**: Instala [Docker Desktop](https://www.docker.com/products/docker-desktop)
> - **Linux**: Sigue la [gu√≠a oficial de Docker](https://docs.docker.com/engine/install/)
>
> **Verifica tu instalaci√≥n** ejecutando:
> ```bash
> docker --version
> docker-compose --version
> ```
>
> Si necesitas ayuda, consulta `documentacion/GUIA_INSTALACION.md` en este repositorio.

---

## üìã √çndice

1. [¬øQu√© es Apache Airflow y por qu√© existe?](#qu√©-es-apache-airflow-y-por-qu√©-existe)
2. [Conceptos Fundamentales](#conceptos-fundamentales)
3. [Instalaci√≥n y Configuraci√≥n con Docker](#instalaci√≥n-y-configuraci√≥n-con-docker)
4. [Tu Primer DAG: "Hola Mundo"](#tu-primer-dag-hola-mundo)
5. [Operators B√°sicos](#operators-b√°sicos)
6. [Dependencias entre Tasks](#dependencias-entre-tasks)
7. [Schedule Intervals: Cu√°ndo Ejecutar](#schedule-intervals-cu√°ndo-ejecutar)
8. [Mejores Pr√°cticas](#mejores-pr√°cticas)
9. [Errores Comunes](#errores-comunes)
10. [Checklist de Aprendizaje](#checklist-de-aprendizaje)

---

## ¬øQu√© es Apache Airflow y por qu√© existe?

### El Problema Real

Imagina que trabajas en una empresa de e-commerce y cada ma√±ana a las 6 AM necesitas:

1. **Descargar** las ventas del d√≠a anterior desde la base de datos
2. **Limpiar** los datos (eliminar duplicados, corregir errores)
3. **Calcular** m√©tricas (total de ventas, productos m√°s vendidos)
4. **Generar** un reporte en Excel
5. **Enviar** el reporte por email al equipo de marketing

¬øC√≥mo lo har√≠as?

**Opci√≥n 1: Manual** üòì
Todos los d√≠as a las 6 AM, ejecutar 5 scripts de Python uno por uno. ¬øQu√© pasa si uno falla? ¬øQu√© pasa si te vas de vacaciones? ¬øQu√© pasa si el paso 3 depende del paso 2?

**Opci√≥n 2: Cron jobs** ü§î
Usar cron (programador de tareas de Linux) para ejecutar cada script. Mejor, pero:
- ¬øC√≥mo manejas dependencias? (El paso 3 debe esperar al paso 2)
- ¬øC√≥mo ves si algo fall√≥?
- ¬øC√≥mo reintentas si algo sale mal?
- ¬øC√≥mo lo monitoreas?

**Opci√≥n 3: Apache Airflow** ‚ú®
Una herramienta dise√±ada espec√≠ficamente para este problema.

---

### La Analog√≠a del Director de Orquesta

Piensa en Airflow como el **director de una orquesta**:

- **T√∫ eres el compositor**: Defines qu√© instrumentos tocan, en qu√© orden y cu√°ndo
- **Airflow es el director**: Se asegura de que cada m√∫sico (script) toque en el momento correcto
- **Los m√∫sicos son tus scripts de Python**: Cada uno hace una tarea espec√≠fica
- **La partitura es el DAG**: El plan maestro que define todo

Si un m√∫sico comete un error, el director puede pedirle que repita esa parte (reintentos autom√°ticos). Si un m√∫sico termina su solo, el director se√±ala al siguiente (dependencias). Todo est√° coordinado y bajo control.

---

### ¬øPor Qu√© Airflow en Data Engineering?

En Data Engineering, constantemente necesitas:

**Pipelines ETL (Extract, Transform, Load)**:
- Extraer datos de APIs, bases de datos, archivos
- Transformar esos datos (limpiar, agregar, calcular)
- Cargar resultados en un data warehouse

**Workflows complejos**:
- Entrenar modelos de Machine Learning cada semana
- Generar reportes diarios para diferentes equipos
- Sincronizar datos entre sistemas cada hora
- Hacer backups de bases de datos cada noche

**Problemas que Airflow resuelve**:
- ‚úÖ **Orquestaci√≥n**: Ejecutar tareas en el orden correcto
- ‚úÖ **Programaci√≥n**: Ejecutar autom√°ticamente a horas espec√≠ficas
- ‚úÖ **Monitoreo**: Ver qu√© est√° ejecut√°ndose y qu√© fall√≥
- ‚úÖ **Reintentos**: Intentar de nuevo si algo falla
- ‚úÖ **Alertas**: Notificarte si algo sale mal
- ‚úÖ **Paralelizaci√≥n**: Ejecutar tareas simult√°neamente cuando es posible
- ‚úÖ **Visualizaci√≥n**: Ver tu workflow como un gr√°fico

---

## Conceptos Fundamentales

Antes de escribir c√≥digo, necesitas entender 6 conceptos clave. Vamos a explicar cada uno con analog√≠as simples.

### 1. DAG (Directed Acyclic Graph)

**Definici√≥n t√©cnica**: Un gr√°fico dirigido ac√≠clico que representa un flujo de trabajo.

**Definici√≥n simple**: Una receta de cocina con pasos ordenados.

**Analog√≠a: Hacer un Pastel** üéÇ

```
Comprar ingredientes
    ‚Üì
Mezclar harina, huevos, az√∫car
    ‚Üì
Hornear
    ‚Üì
Decorar
```

**Caracter√≠sticas clave**:

1. **Directed (Dirigido)**: Las flechas tienen direcci√≥n. No puedes hornear antes de mezclar.
2. **Acyclic (Ac√≠clico)**: No hay ciclos infinitos. No puedes decorar, luego hornear, luego decorar otra vez en el mismo pastel.
3. **Graph (Gr√°fico)**: Es un diagrama visual de pasos conectados.

**En Airflow**:
- Cada receta es un DAG
- Cada paso (comprar, mezclar, hornear) es una **Task**
- Las flechas son las **dependencias**

**Por qu√© "ac√≠clico" importa**:
Imagina si pudieras hacer esto:
```
Hornear ‚Üí Decorar ‚Üí Hornear ‚Üí Decorar ‚Üí Hornear ‚Üí ...
```
¬°Nunca terminar√≠as! Airflow no permite esto.

---

### 2. Task (Tarea)

**Definici√≥n**: Un solo paso ejecutable en tu workflow.

**Analog√≠a**: Un solo paso de la receta de cocina.

**Ejemplos de Tasks en Data Engineering**:
- "Descargar archivo CSV de un servidor FTP"
- "Ejecutar una consulta SQL en PostgreSQL"
- "Entrenar un modelo de Machine Learning"
- "Enviar un email con el reporte generado"

**Caracter√≠sticas de una buena Task**:
1. **Hace una sola cosa**: "Descargar archivo" (no "descargar y procesar y enviar")
2. **Es idempotente**: Si la ejecutas 10 veces con los mismos datos, obtienes el mismo resultado
3. **Tiene entrada y salida claras**: Sabes qu√© recibe y qu√© produce

**Ejemplo visual**:
```
Task: "Calcular total de ventas"
Entrada: ventas.csv
Salida: total = 25000
```

---

### 3. Operator (Operador)

**Definici√≥n**: El tipo de tarea que quieres ejecutar.

**Analog√≠a**: El tipo de herramienta de cocina que usas.

En la cocina:
- **Batidora**: Para mezclar ingredientes
- **Horno**: Para hornear
- **Cuchillo**: Para cortar
- **Sart√©n**: Para fre√≠r

En Airflow:
- **PythonOperator**: Ejecuta una funci√≥n de Python
- **BashOperator**: Ejecuta un comando de shell (bash)
- **PostgresOperator**: Ejecuta una consulta SQL
- **EmailOperator**: Env√≠a un email

**Ejemplo pr√°ctico**:

Si quieres ejecutar esta funci√≥n Python:
```python
def calcular_ventas():
    return 1000 + 2000
```

Usas un **PythonOperator**:
```python
task_calcular = PythonOperator(
    task_id="calcular_ventas",
    python_callable=calcular_ventas
)
```

Si quieres ejecutar este comando shell:
```bash
ls -la
```

Usas un **BashOperator**:
```python
task_listar = BashOperator(
    task_id="listar_archivos",
    bash_command="ls -la"
)
```

---

### 4. Scheduler (Programador)

**Definici√≥n**: El componente de Airflow que decide cu√°ndo ejecutar cada tarea.

**Analog√≠a**: El reloj despertador de tu smartphone.

Imagina que tienes 20 alarmas configuradas:
- Lunes a viernes: 7 AM (ir al gym)
- Todos los d√≠as: 8 AM (desayunar)
- Cada domingo: 10 AM (lavar el coche)

Tu smartphone:
1. **Revisa constantemente** la hora
2. **Compara** con las alarmas configuradas
3. **Activa** la alarma cuando es el momento
4. **Te notifica** si algo no funciona

El **Scheduler de Airflow** hace exactamente lo mismo:
1. Revisa cada 5-10 segundos qu√© DAGs deben ejecutarse
2. Verifica si es el momento (seg√∫n el `schedule_interval`)
3. Env√≠a las tasks al Executor para que las ejecute
4. Registra todo en logs

**Sin Scheduler**:
Tendr√≠as que ejecutar manualmente cada DAG con `python mi_dag.py` cada vez.

**Con Scheduler**:
Configuras `schedule="@daily"` y Airflow se encarga de ejecutarlo todos los d√≠as a medianoche.

---

### 5. Executor (Ejecutor)

**Definici√≥n**: El componente que ejecuta las tareas.

**Analog√≠a**: Los chefs en una cocina.

En un restaurante:
- **Un solo chef** (SequentialExecutor): Cocina un plato a la vez. Lento pero simple.
- **Varios chefs** (LocalExecutor): Varios chefs en la misma cocina. M√°s r√°pido.
- **Muchos chefs en varias cocinas** (CeleryExecutor): Chefs distribuidos en diferentes ubicaciones. Muy r√°pido, para producci√≥n.

**Tipos de Executors**:

1. **SequentialExecutor** (1 tarea a la vez):
   - Solo para pruebas
   - No permite paralelizaci√≥n
   - No recomendado

2. **LocalExecutor** (varias tareas en paralelo, misma m√°quina):
   - Bueno para desarrollo
   - Usa el poder de tu computadora/servidor
   - El que usaremos en este curso

3. **CeleryExecutor** (varias tareas en varias m√°quinas):
   - Para producci√≥n a gran escala
   - Requiere Redis o RabbitMQ
   - Distribuci√≥n real

4. **KubernetesExecutor** (tareas en contenedores Kubernetes):
   - Para producci√≥n cloud-native
   - Escalado din√°mico

**Por ahora, usamos LocalExecutor**: Suficiente para aprender y para proyectos medianos.

---

### 6. Webserver (Servidor Web)

**Definici√≥n**: La interfaz visual de Airflow donde ves todo lo que pasa.

**Analog√≠a**: El tablero de un coche.

En un coche:
- **Veloc√≠metro**: Cu√°nto vas
- **Combustible**: Cu√°nto te queda
- **Temperatura**: Si el motor est√° caliente
- **Luces de advertencia**: Si algo va mal

En Airflow Web UI:
- **DAGs page**: Lista de todos tus workflows
- **Graph View**: Visualizaci√≥n gr√°fica del DAG
- **Tree View**: Historial de ejecuciones
- **Logs**: Detalles de qu√© pas√≥ en cada task
- **Admin panel**: Configuraci√≥n de variables, conexiones

**Acceso**: Normalmente en `http://localhost:8080`

**Qu√© puedes hacer**:
- ‚úÖ Ver qu√© DAGs est√°n ejecut√°ndose
- ‚úÖ Ejecutar un DAG manualmente (trigger)
- ‚úÖ Ver logs de una task que fall√≥
- ‚úÖ Pausar/activar DAGs
- ‚úÖ Ver m√©tricas y estad√≠sticas

---

## Instalaci√≥n y Configuraci√≥n con Docker

### ¬øPor qu√© Docker?

Instalar Airflow "a pelo" es complejo porque requiere:
- PostgreSQL (base de datos para metadata)
- Redis (opcional, para cache)
- Configuraci√≥n de usuario admin
- M√∫ltiples variables de entorno
- Inicializaci√≥n de base de datos

**Docker simplifica todo esto** en un solo comando.

---

### Prerequisitos

1. **Docker Desktop** instalado y ejecut√°ndose
   - Windows/Mac: Descargar de [docker.com](https://www.docker.com)
   - Linux: `sudo apt install docker.io docker-compose`

2. **Git** instalado (para clonar el repositorio del curso)

3. **Editor de c√≥digo** (VS Code recomendado)

---

### Paso 1: Verificar Docker

Abre tu terminal y ejecuta:

```bash
docker --version
docker-compose --version
```

Deber√≠as ver algo como:
```
Docker version 24.0.5
docker-compose version 1.29.2
```

Si ves errores, instala Docker Desktop primero.

---

### Paso 2: Navegar a la Carpeta del Curso

```bash
cd "Curso Data Engineering"
```

---

### Paso 3: Verificar el Archivo docker-compose.yml

El archivo `docker-compose.yml` en la ra√≠z del proyecto ya tiene Airflow configurado. Puedes verlo con:

```bash
cat docker-compose.yml | grep -A 20 "airflow"
```

**Servicios incluidos**:
- `postgres-airflow`: Base de datos para Airflow
- `airflow-init`: Inicializa la base de datos y crea el usuario admin
- `airflow-webserver`: Interfaz web en puerto 8080
- `airflow-scheduler`: Programador que ejecuta los DAGs

---

### Paso 4: Iniciar Airflow

Ejecuta:

```bash
docker-compose up -d postgres-airflow airflow-init airflow-webserver airflow-scheduler
```

**Explicaci√≥n del comando**:
- `docker-compose`: Herramienta para manejar m√∫ltiples contenedores
- `up`: Iniciar servicios
- `-d`: Modo "detached" (en segundo plano)
- `postgres-airflow airflow-init airflow-webserver airflow-scheduler`: Los 4 servicios necesarios

**Tiempo**: Primera vez tarda 2-5 minutos (descarga im√°genes de Docker)

**Progreso**:
```
[+] Running 4/4
 ‚úî Container master-postgres-airflow     Started
 ‚úî Container master-airflow-init         Started
 ‚úî Container master-airflow-webserver    Started
 ‚úî Container master-airflow-scheduler    Started
```

---

### Paso 5: Verificar que Todo Funciona

```bash
docker-compose ps
```

Deber√≠as ver:
```
NAME                        STATUS              PORTS
master-postgres-airflow     Up                  5432/tcp
master-airflow-webserver    Up (healthy)        0.0.0.0:8080->8080/tcp
master-airflow-scheduler    Up
```

**Si ves "Up (healthy)"** ‚Üí Todo bien ‚úÖ
**Si ves "Restarting" o "Unhealthy"** ‚Üí Hay un problema ‚ùå

---

### Paso 6: Acceder a la Interfaz Web

1. Abre tu navegador
2. Ve a: **http://localhost:8080**
3. Ver√°s la pantalla de login de Airflow

**Credenciales**:
- **Usuario**: `admin`
- **Contrase√±a**: `Airflow2025!Admin`

(Estas credenciales est√°n en el archivo `docker-compose.yml`)

---

### Tour de la Interfaz Web

Una vez dentro, ver√°s:

**1. DAGs Page (P√°gina principal)**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Airflow                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ DAGs  Browse  Admin  Docs           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Search DAGs: [___________] üîç       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ DAG                  Runs   Status  ‚îÇ
‚îÇ example_bash_operator  5     On    ‚îÇ
‚îÇ example_python         3     Off   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**2. DAG Detail (Al hacer clic en un DAG)**:
- **Graph View**: Visualizaci√≥n gr√°fica del workflow
- **Tree View**: Historial de ejecuciones
- **Code**: Ver el c√≥digo Python del DAG
- **Logs**: Ver logs de cada task

**3. Admin Panel**:
- **Variables**: Configuraci√≥n global (por ejemplo, API keys)
- **Connections**: Credenciales de bases de datos/APIs

---

### Comandos √ötiles

**Ver logs de un servicio**:
```bash
docker-compose logs -f airflow-webserver
docker-compose logs -f airflow-scheduler
```

**Detener Airflow**:
```bash
docker-compose stop
```

**Reiniciar Airflow**:
```bash
docker-compose restart airflow-webserver airflow-scheduler
```

**Detener y eliminar todo (‚ö†Ô∏è cuidado, borra datos)**:
```bash
docker-compose down -v
```

---

## Tu Primer DAG: "Hola Mundo"

Ahora que Airflow est√° corriendo, vamos a crear nuestro primer workflow.

### Objetivo

Crear un DAG que ejecute una funci√≥n Python simple que imprima "¬°Hola desde Airflow!".

---

### Paso 1: Crear el Archivo del DAG

Los DAGs van en la carpeta `airflow/dags/`. Crea un archivo llamado `hola_mundo.py`:

**Ubicaci√≥n**: `airflow/dags/hola_mundo.py`

```python
# Importar librer√≠as necesarias
from datetime import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator

# Definir la funci√≥n que queremos ejecutar
def saludar():
    """
    Esta funci√≥n simplemente imprime un saludo.
    En un caso real, aqu√≠ har√≠as algo √∫til como
    procesar datos, hacer c√°lculos, etc.
    """
    print("¬°Hola desde Airflow!")
    print("Este es mi primer DAG y funciona correctamente.")
    return "Saludo completado"

# Definir el DAG
with DAG(
    dag_id="hola_mundo",                    # Nombre √∫nico del DAG
    description="Mi primer DAG en Airflow",  # Descripci√≥n para la UI
    start_date=datetime(2025, 1, 1),        # Fecha de inicio (retroactiva)
    schedule="@daily",                       # Ejecutar una vez al d√≠a
    catchup=False,                           # No ejecutar fechas pasadas
    tags=["tutorial", "principiante"],      # Etiquetas para organizar
) as dag:

    # Crear la tarea
    task_saludar = PythonOperator(
        task_id="saludar",           # Nombre √∫nico de la tarea
        python_callable=saludar,     # La funci√≥n a ejecutar
    )
```

---

### Explicaci√≥n L√≠nea por L√≠nea

#### Imports

```python
from datetime import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator
```

- `datetime`: Para definir fechas de inicio
- `DAG`: La clase principal para crear un workflow
- `PythonOperator`: El operador para ejecutar funciones Python

---

#### La Funci√≥n Python

```python
def saludar():
    print("¬°Hola desde Airflow!")
    return "Saludo completado"
```

**¬øQu√© puede ir aqu√≠?**
- Procesar un archivo CSV
- Hacer una consulta a una base de datos
- Llamar a una API
- Entrenar un modelo de ML
- Cualquier c√≥digo Python v√°lido

**Buenas pr√°cticas**:
1. La funci√≥n debe ser **autocontenida** (no depender de variables globales)
2. Debe retornar algo (√∫til para debugging)
3. Debe manejar errores con try/except si es posible

---

#### El DAG

```python
with DAG(
    dag_id="hola_mundo",
    ...
) as dag:
    ...
```

**Par√°metros clave**:

1. **dag_id** (obligatorio):
   - Nombre √∫nico del DAG
   - Solo letras, n√∫meros, guiones bajos
   - Ejemplo: `"hola_mundo"`, `"etl_ventas_diario"`

2. **description**:
   - Descripci√≥n que aparece en la UI
   - Ayuda a recordar qu√© hace este DAG

3. **start_date** (obligatorio):
   - Fecha desde la cual este DAG puede ejecutarse
   - Usa `datetime(2025, 1, 1)` (a√±o, mes, d√≠a)
   - Puede ser en el pasado (Airflow no ejecutar√° fechas pasadas si `catchup=False`)

4. **schedule**:
   - Cu√°ndo ejecutar el DAG
   - Opciones:
     - `"@daily"`: Una vez al d√≠a a medianoche
     - `"@hourly"`: Una vez por hora
     - `"@weekly"`: Una vez por semana
     - `None`: Solo ejecuci√≥n manual
     - Cron expression: `"0 8 * * *"` (a las 8 AM todos los d√≠as)

5. **catchup**:
   - `False`: No ejecutar fechas pasadas (recomendado)
   - `True`: Ejecutar todas las fechas desde `start_date` hasta ahora

6. **tags**:
   - Lista de etiquetas para organizar
   - Ejemplo: `["ventas", "producci√≥n"]`

---

#### La Tarea

```python
task_saludar = PythonOperator(
    task_id="saludar",
    python_callable=saludar,
)
```

**Par√°metros clave**:

1. **task_id** (obligatorio):
   - Nombre √∫nico de la tarea dentro del DAG
   - Aparece en la UI

2. **python_callable**:
   - La funci√≥n Python a ejecutar
   - **OJO**: Sin par√©ntesis (`saludar`, NO `saludar()`)

---

### Paso 2: Detectar el DAG en Airflow

Airflow revisa la carpeta `dags/` cada 30 segundos por defecto.

**Verificar en la terminal**:
```bash
docker-compose logs -f airflow-scheduler | grep "hola_mundo"
```

Deber√≠as ver algo como:
```
[INFO] - Added DAG: hola_mundo
```

**Verificar en la UI**:
1. Ve a http://localhost:8080
2. En la lista de DAGs, busca "hola_mundo"
3. Si no aparece, espera 30-60 segundos y recarga la p√°gina

---

### Paso 3: Ejecutar el DAG Manualmente

1. En la lista de DAGs, encuentra "hola_mundo"
2. Haz clic en el **bot√≥n de play ‚ñ∂Ô∏è** a la derecha
3. Confirma "Trigger DAG"
4. Ve a la columna "Runs" ‚Üí ver√°s un c√≠rculo verde (√©xito) o rojo (fallo)

---

### Paso 4: Ver los Logs

1. Haz clic en el nombre del DAG "hola_mundo"
2. Ver√°s el **Graph View**: Un gr√°fico con la tarea "saludar"
3. Haz clic en el cuadro de la tarea "saludar"
4. Haz clic en "Logs"
5. Ver√°s el output:
   ```
   [INFO] - ¬°Hola desde Airflow!
   [INFO] - Este es mi primer DAG y funciona correctamente.
   ```

---

### Paso 5: Pausar/Activar el DAG

En la lista de DAGs:
- **Toggle a la izquierda del nombre**: Activar/Pausar
- **Verde** = Activo (se ejecutar√° seg√∫n schedule)
- **Gris** = Pausado (solo ejecuci√≥n manual)

---

## Operators B√°sicos

Ya usaste `PythonOperator`. Ahora veremos otros operators comunes.

### 1. PythonOperator (Ejecutar Funci√≥n Python)

**Cu√°ndo usarlo**: Cuando necesitas ejecutar c√≥digo Python personalizado.

**Ejemplo**: Procesar un archivo, hacer c√°lculos, llamar a una API.

```python
def procesar_datos():
    import pandas as pd
    df = pd.read_csv("ventas.csv")
    total = df["precio"].sum()
    print(f"Total de ventas: {total}")
    return total

task_procesar = PythonOperator(
    task_id="procesar_datos",
    python_callable=procesar_datos
)
```

---

### 2. BashOperator (Ejecutar Comando Shell)

**Cu√°ndo usarlo**: Cuando necesitas ejecutar comandos de sistema operativo.

**Ejemplo**: Listar archivos, mover archivos, comprimir archivos.

```python
from airflow.operators.bash import BashOperator

task_listar = BashOperator(
    task_id="listar_archivos",
    bash_command="ls -la /path/to/directory"
)

task_comprimir = BashOperator(
    task_id="comprimir_logs",
    bash_command="tar -czf logs.tar.gz /var/log/*.log"
)
```

---

### 3. DummyOperator (Placeholder sin Ejecuci√≥n)

**Cu√°ndo usarlo**: Para organizar visualmente tu DAG, especialmente en puntos de uni√≥n o inicio/fin.

**Ejemplo**: Punto de inicio antes de tareas paralelas, punto de fin despu√©s de convergencia.

```python
from airflow.operators.dummy import DummyOperator

inicio = DummyOperator(task_id="inicio")
fin = DummyOperator(task_id="fin")

# inicio >> [tarea1, tarea2, tarea3] >> fin
```

**Visualizaci√≥n**:
```
      inicio
     /  |  \
task1 task2 task3
     \  |  /
       fin
```

---

### Comparaci√≥n R√°pida

| Operator         | Usa Para                    | Ejemplo                             |
| ---------------- | --------------------------- | ----------------------------------- |
| PythonOperator   | C√≥digo Python personalizado | Procesar CSV, llamar API            |
| BashOperator     | Comandos shell              | ls, cp, tar, curl                   |
| DummyOperator    | Organizaci√≥n visual         | Puntos de inicio/fin, convergencias |
| PostgresOperator | Queries SQL en PostgreSQL   | INSERT, UPDATE, SELECT              |
| EmailOperator    | Enviar emails               | Notificaciones de √©xito/fallo       |

---

## Dependencias entre Tasks

En un DAG real, las tareas tienen orden. "Primero descarga, luego procesa, luego env√≠a reporte".

### Operador `>>`  (Downstream)

**Significado**: "Esta tarea debe ejecutarse DESPU√âS de la otra"

```python
A >> B  # B se ejecuta DESPU√âS de A
```

**Analog√≠a**: "Hornear >> Decorar" (decorar despu√©s de hornear)

---

### Operador `<<` (Upstream)

**Significado**: "Esta tarea debe ejecutarse ANTES de la otra"

```python
B << A  # B se ejecuta DESPU√âS de A (igual que A >> B)
```

**Uso**: Es menos com√∫n, `>>` es m√°s intuitivo.

---

### Dependencias Secuenciales

```python
descargar >> limpiar >> analizar >> reportar
```

**Visualizaci√≥n**:
```
descargar ‚Üí limpiar ‚Üí analizar ‚Üí reportar
```

**Ejecuci√≥n**:
1. Primero: `descargar`
2. Cuando termine: `limpiar`
3. Cuando termine: `analizar`
4. Cuando termine: `reportar`

---

### Dependencias Paralelas (Fan-out)

```python
descargar >> [procesar_ventas, procesar_inventario, procesar_clientes]
```

**Visualizaci√≥n**:
```
         descargar
            |
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚Üì        ‚Üì        ‚Üì
ventas  inventario  clientes
```

**Ejecuci√≥n**:
1. Primero: `descargar`
2. Cuando termine: Las 3 tareas en paralelo (si el Executor lo permite)

---

### Convergencia (Fan-in)

```python
[procesar_ventas, procesar_inventario, procesar_clientes] >> generar_reporte
```

**Visualizaci√≥n**:
```
ventas  inventario  clientes
   ‚Üì        ‚Üì        ‚Üì
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            |
      generar_reporte
```

**Ejecuci√≥n**:
1. Las 3 tareas en paralelo
2. `generar_reporte` espera a que las 3 terminen
3. Solo cuando las 3 est√°n completas: ejecuta `generar_reporte`

---

### Ejemplo Completo

```python
from datetime import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator

def descargar():
    print("Descargando datos...")

def procesar_ventas():
    print("Procesando ventas...")

def procesar_inventario():
    print("Procesando inventario...")

def generar_reporte():
    print("Generando reporte final...")

with DAG(
    dag_id="pipeline_completo",
    start_date=datetime(2025, 1, 1),
    schedule=None,
    catchup=False,
) as dag:

    inicio = DummyOperator(task_id="inicio")

    task_descargar = PythonOperator(
        task_id="descargar",
        python_callable=descargar
    )

    task_ventas = PythonOperator(
        task_id="procesar_ventas",
        python_callable=procesar_ventas
    )

    task_inventario = PythonOperator(
        task_id="procesar_inventario",
        python_callable=procesar_inventario
    )

    task_reporte = PythonOperator(
        task_id="generar_reporte",
        python_callable=generar_reporte
    )

    fin = DummyOperator(task_id="fin")

    # Definir dependencias
    inicio >> task_descargar >> [task_ventas, task_inventario] >> task_reporte >> fin
```

**Visualizaci√≥n en Airflow**:
```
        inicio
          |
      descargar
       /     \
   ventas  inventario
       \     /
       reporte
          |
         fin
```

---

## Schedule Intervals: Cu√°ndo Ejecutar

El `schedule` define cu√°ndo Airflow debe ejecutar autom√°ticamente tu DAG.

### Opciones Comunes (Presets)

```python
schedule="@once"     # Una sola vez
schedule="@hourly"   # Cada hora (minuto 0)
schedule="@daily"    # Cada d√≠a a medianoche (00:00)
schedule="@weekly"   # Cada lunes a medianoche
schedule="@monthly"  # El d√≠a 1 de cada mes a medianoche
schedule="@yearly"   # El 1 de enero a medianoche
schedule=None        # Solo ejecuci√≥n manual
```

---

### Cron Expressions (Avanzado)

**Formato**: `"minuto hora d√≠a_mes mes d√≠a_semana"`

**Ejemplos**:

```python
# Todos los d√≠as a las 8:30 AM
schedule="30 8 * * *"

# Cada lunes a las 3 PM
schedule="0 15 * * 1"

# Cada 15 minutos
schedule="*/15 * * * *"

# El d√≠a 1 y 15 de cada mes a las 6 AM
schedule="0 6 1,15 * *"
```

**Herramienta √∫til**: [crontab.guru](https://crontab.guru/) para generar cron expressions.

---

### El Par√°metro `catchup`

**Problema**: Si configuras `start_date=datetime(2024, 1, 1)` y hoy es 2025-10-25, ¬øAirflow ejecutar√° 300 d√≠as de historial?

**Soluci√≥n**:
- `catchup=True`: S√≠, ejecutar√° todas las fechas pasadas (√∫til para backfill)
- `catchup=False`: No, solo desde ahora (recomendado para la mayor√≠a de casos)

**Ejemplo**:

```python
# Ejecutar solo desde hoy en adelante
with DAG(
    dag_id="reporte_diario",
    start_date=datetime(2024, 1, 1),  # Fecha en el pasado
    schedule="@daily",
    catchup=False,  # ‚Üê Importante
) as dag:
    ...
```

---

### Ejecuci√≥n Manual

Si `schedule=None`, el DAG solo se ejecuta manualmente:
- Haces clic en el bot√≥n "Trigger DAG" en la UI
- O usas el CLI: `airflow dags trigger mi_dag`

**√ötil para**:
- Pipelines on-demand
- Testing
- Procesos espor√°dicos

---

## Mejores Pr√°cticas

### 1. Un DAG por Archivo

‚ùå **MAL**:
```
mi_script.py
‚îú‚îÄ‚îÄ dag1
‚îú‚îÄ‚îÄ dag2
‚îî‚îÄ‚îÄ dag3
```

‚úÖ **BIEN**:
```
dags/
‚îú‚îÄ‚îÄ dag_ventas.py
‚îú‚îÄ‚îÄ dag_inventario.py
‚îî‚îÄ‚îÄ dag_reportes.py
```

**Por qu√©**: M√°s f√°cil de mantener, encontrar y debuggear.

---

### 2. Nombres Descriptivos

‚ùå **MAL**:
```python
dag_id="dag1"
task_id="t1"
```

‚úÖ **BIEN**:
```python
dag_id="etl_ventas_diario"
task_id="descargar_ventas_api"
```

**Por qu√©**: En 6 meses no recordar√°s qu√© hace "dag1".

---

### 3. Usa `catchup=False` por Defecto

A menos que espec√≠ficamente necesites ejecutar fechas pasadas.

```python
catchup=False  # ‚Üê A√±ade esto siempre
```

---

### 4. Docstrings en DAGs

```python
with DAG(
    dag_id="etl_ventas",
    description="Pipeline ETL para procesar ventas diarias desde la API",  # ‚Üê Esto
    ...
) as dag:
    ...
```

---

### 5. Tasks Idempotentes

**Idempotente**: Ejecutar la misma task 10 veces con los mismos inputs produce el mismo output.

‚ùå **NO idempotente**:
```python
def agregar_venta():
    # Siempre a√±ade una fila, aunque ya exista
    ejecutar_sql("INSERT INTO ventas VALUES (1, 100)")
```

‚úÖ **Idempotente**:
```python
def agregar_venta():
    # Primero elimina si existe, luego inserta
    ejecutar_sql("DELETE FROM ventas WHERE id = 1")
    ejecutar_sql("INSERT INTO ventas VALUES (1, 100)")
```

**Por qu√©**: Si Airflow reintenta una task, no quieres duplicados ni efectos secundarios.

---

### 6. Usar Tags para Organizar

```python
tags=["producci√≥n", "cr√≠tico", "ventas"]
```

Luego puedes filtrar en la UI por tags.

---

### 7. Variables de Entorno para Configuraci√≥n

No hardcodees credenciales o paths:

‚ùå **MAL**:
```python
db_password = "MiPasswordSecreta123"
```

‚úÖ **BIEN**:
```python
from airflow.models import Variable
db_password = Variable.get("db_password")
```

(Configurar Variables en Admin ‚Üí Variables en la UI)

---

## Errores Comunes

### Error 1: DAG No Aparece en la UI

**S√≠ntomas**: Creaste el archivo, pero no ves el DAG.

**Causas posibles**:
1. **Sintaxis error en Python**: Revisa logs del scheduler
   ```bash
   docker-compose logs airflow-scheduler | grep "ERROR"
   ```

2. **Archivo no est√° en `dags/`**: Debe estar en `airflow/dags/mi_dag.py`

3. **Airflow no ha refrescado**: Espera 30-60 segundos

**Soluci√≥n**: Revisa logs y sintaxis.

---

### Error 2: Task Falla con "ModuleNotFoundError"

**S√≠ntomas**: Task falla, error dice "No module named 'pandas'"

**Causa**: La librer√≠a no est√° instalada en el contenedor de Airflow.

**Soluci√≥n**:
1. A√±adir al `requirements.txt` del proyecto
2. Reconstruir el contenedor:
   ```bash
   docker-compose down
   docker-compose build
   docker-compose up -d
   ```

---

### Error 3: "DAG Seems to be Missing"

**S√≠ntomas**: DAG aparec√≠a, ahora dice "missing".

**Causa**: Error de sintaxis introducido recientemente.

**Soluci√≥n**: Revisa el archivo `.py` del DAG, probablemente hay un error.

---

### Error 4: Task en Estado "Running" Indefinidamente

**S√≠ntomas**: Task ejecut√°ndose durante horas sin terminar.

**Causas**:
1. **Loop infinito** en tu c√≥digo Python
2. **Esperando input del usuario** (Airflow no puede interactuar)

**Soluci√≥n**:
1. Ve a la UI ‚Üí Logs de la task
2. Identifica d√≥nde se atasc√≥
3. Corrige el c√≥digo

---

### Error 5: "Broken DAG" (DAG Roto)

**S√≠ntomas**: Icono rojo en la lista de DAGs.

**Causa**: Error de sintaxis o importaci√≥n.

**Soluci√≥n**:
1. Haz clic en el DAG roto
2. Ver√°s el mensaje de error espec√≠fico
3. Corrige el error en el archivo `.py`

---

## Checklist de Aprendizaje

Al terminar este tema, deber√≠as poder:

- [ ] **Explicar qu√© es Airflow** con tus propias palabras
- [ ] **Describir qu√© es un DAG** usando una analog√≠a (receta de cocina)
- [ ] **Diferenciar entre Task, Operator, Scheduler y Executor**
- [ ] **Iniciar Airflow con Docker Compose** sin ayuda
- [ ] **Acceder a la UI de Airflow** en localhost:8080
- [ ] **Crear un DAG simple** con PythonOperator
- [ ] **Definir dependencias** usando `>>` entre tasks
- [ ] **Configurar un schedule** (por ejemplo, `@daily`)
- [ ] **Ejecutar un DAG manualmente** desde la UI
- [ ] **Ver los logs** de una task que se ejecut√≥
- [ ] **Identificar 3 operators b√°sicos**: PythonOperator, BashOperator, DummyOperator
- [ ] **Explicar qu√© significa `catchup=False`**
- [ ] **Listar 3 mejores pr√°cticas** de Airflow

---

## Recursos Adicionales

### Documentaci√≥n Oficial

- **Airflow Docs**: https://airflow.apache.org/docs/
- **Concepts**: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.html
- **Tutorial**: https://airflow.apache.org/docs/apache-airflow/stable/tutorial/index.html

### Videos Recomendados

- **Airflow in 5 Minutes** (YouTube): B√∫squeda recomendada
- **Apache Airflow Tutorial** (freeCodeCamp): Curso completo gratuito

### Art√≠culos √ötiles

- **Best Practices**: https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html
- **Common Pitfalls**: B√∫squeda en Google "Airflow common mistakes"

### Comunidad

- **Slack de Airflow**: https://apache-airflow.slack.com/
- **Stack Overflow**: Tag `apache-airflow`
- **GitHub Issues**: https://github.com/apache/airflow/issues

---

## Pr√≥ximos Pasos

En el siguiente tema (Airflow Intermedio), aprender√°s:

1. **TaskGroups**: Organizar DAGs complejos
2. **XComs**: Pasar datos entre tasks
3. **Branching**: Condicionales en workflows
4. **Sensors**: Esperar por condiciones externas
5. **Dynamic DAG Generation**: Generar tasks en loops

Pero antes, aseg√∫rate de:
- ‚úÖ Completar todos los ejercicios del Tema 1
- ‚úÖ Tener Airflow corriendo en tu m√°quina
- ‚úÖ Haber creado al menos 2-3 DAGs simples

---

**¬°Felicitaciones por completar la introducci√≥n a Airflow!** üéâ

Ahora tienes las bases para orquestar workflows de Data Engineering. En los ejemplos y ejercicios pr√°cticos, consolidar√°s estos conceptos con casos reales.

---

**Fecha de creaci√≥n**: 2025-10-25
**Autor**: @teaching [pedagogo]
**M√≥dulo**: 6 - Apache Airflow y Orquestaci√≥n
**Tema**: 1 - Introducci√≥n a Airflow
